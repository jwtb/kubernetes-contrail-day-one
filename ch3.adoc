= Kubernetes in practice
// vim:set ft=asciidoc syntax=ON tw=80:
:toc:
:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline

This chapter introduces some of the fundamental and powerful features of
kubernetes.


== Labels

Imagine you have a POD that’s need to be host on a machine with certain
specifications ( SSD HD, physical location , processing power , ..,etc ) 
OR imagine you want to search or group your PODs for easier administration 
what would you do ?
then label is your way to go, in Kubernetes Label are Key/value pairs attached to an object  
let’s see how can we use label with node selector to make a POD is lunched on a certain machine 

[TIP]
====
* You can assign multiple labels per object but avoid using too much label or
  too little, too much would get you confused and too little won’t give the real
  benefits of grouping, selecting and searching 
* Best practice is to assign labels to indicate
    - application/program ID use this POD
    - owner (who manage this POD/application)
    - stage (the POD/application in development/testing/ production as well version)
    - resource requirements (SSD, CPU, storage)
    - location (preferred location/zone/ Datacenter to run this POD/application) 
====

Let’s assign label stage: testing & zone: production to these two nodes
respectively then try to lunch a POD with label zone: testing 
 
    kubectl get nodes --show-labels

    NAME                                         STATUS     ROLES     AGE       VERSION   LABELS
    ip-172-25-1-216.us-west-1.compute.internal   Ready      <none>    2h        v1.9.2    <none>
    ip-172-25-1-56.us-west-1.compute.internal    NotReady   <none>    2h        v1.9.2    <none>
    ip-172-25-1-83.us-west-1.compute.internal    Ready      <none>    2h        v1.9.2    <none>


    kubectl label nodes ip-172-25-1-83.us-west-1.compute.internal stage=testing
    kubectl label nodes ip-172-25-1-216.us-west-1.compute.internal stage=production

    kubectl get nodes --show-labels

    NAME                                        STATUS    ROLES   AGE  VERSION  LABELS
    ip-172-25-1-216.us-west-1.compute.internal  Ready     <none>  2h   v1.9.2   stage=production
    ip-172-25-1-56.us-west-1.compute.internal   NotReady  <none>  2h   v1.9.2   <none>
    ip-172-25-1-83.us-west-1.compute.internal   Ready     <none>  2h   v1.9.2   stage=testing

now let’s lunch a basic Nginx POD tagged with stage=testing and confirm it
will land on a node tagged with stage=testing

    [root@ip-172-25-1-56 /]# cat > web-server.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
      labels:
        app: webserver
    spec:
      containers:
      - name: nginx
        image: nginx
      nodeSelector:
        stage: testing

    [root@ip-172-25-1-56 /]# kubectl create -f web-server.yaml
    pod "wordpress" created

    [root@ip-172-25-1-56 /]# kubectl get pods --output=wide
    NAME        READY     STATUS    RESTARTS   AGE       IP              NODE
    wordpress   1/1       Running   0          48s       10.47.255.250   ip-172-25-1-83.us-west-1.compute.internal


[Tip]
====
You can assign POD to certain node without label by adding the argument
nodeName: nodeX under spec in the YAML file where nodeX is the name of the node  

Another very important use of labels is linking PODs to Replica-set ,
Deployment , services …,etc  and that’s what we will see next with services and
label selector 

====

== Namespace

As in many other platforms, normally there is more than one users (or teams) working on a
kubernetes cluster. suppose a pod named 'webserver1' has been built by 'dev'
department, when 'sales' department attempts to launch a pod with the same name,
the system will give an error:

----
Error from server (AlreadyExists): error when creating "webserver1.yaml": pods "webserver1" already exists
----

this is a very common scenario in many today's network platforms - an object
name has to be unique in the same "scope":

* IP prefix in a router
* virtual network in openstack cluster
* a variable name in a program

and the solution is to support multiple scopes or namespaces, respectively:
* VRF
* project/tenant
* function

the solution in k8s is just called 'Namespaces', or 'NS' for short, which
provide a scope for names. Names of resources need to be unique within a
namespace, but not across namespaces. it is a nature way to divide cluster
resources between multiple users. 

to create a NS is pretty simple, you can avoid the need to give a yaml file by
using kubectl with '-f' option, followed by '-' and hit enter:

    root@test3:~# kubectl create -f -

now the kubectl will wait for you to manually input the definition of NS from
'stdin', you can now input these 4 lines to create a VN:

    apiVersion: v1
    kind: Namespace
    metadata:
        name: dev

when done, press ctr-d to submit the stdin buffer content into kubectl.

    namespace/development created

new namespace is now created

    root@test3:~# kubectl get ns
    NAME          STATUS    AGE
    default       Active    15d
    dev           Active    5s  #<-----

you may notice there is a NS named 'default' in any k8s setup. that is, as the
name indicates, the "default" NS that will be created when you install the
setup, same as the "default routing table" in a router, "default tenant" in
openstack setup - there needs to be a scope providing you an initial working
environment so from there you can create all other objects.

now 'dev' 'webserver1' in 'dev' NS won't conflict with 'webserver1' in 'sales'
NS.

----
$ kubectl get pod --all-namespaces -o wide
NAMESPACE  NAME  READY  STATUS   RESTARTS  AGE   IP             NODE     NOMINATED  NODE
......
dev        csrx  1/1    Running  4         2d4h  10.47.255.249  cent222  <none>
......
sales      csrx  1/1    Running  4         2d4h  10.47.255.244  cent222  <none>
......
----

== Scalling 
=== Replication Controller

you have learned how to launch a pod that representing your containers from its
yaml file in chapter 2. one question will rise in your mind: what if I need 5
exactly the same pods (each runs a apache container) to make sure the web
service appears more robust? shall I change the name in yaml file then repeat the
same commands to create 5 more pods? or maybe with a shell script? kubernetes
already has the objects to address this exact demand and the right answer are RC -
replication controller or ReplicaSet

> A ReplicationController ensures that a specified number of pod replicas are
> running at any one time. In other words, a ReplicationController makes sure
> that a pod or a homogeneous set of pods is always up and available.

let's look at how it works with an example. first create a yaml file for a RC
object named `myweb`.

    #myweb-rc.yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: myweb
    spec:
      replicas: 5
      selector:
        app: myweb
      template:
        metadata:
          labels:
            app: myweb
        spec:
          containers:
          - name: myweb
            image: kubeguide/tomcat-app:v1
            ports:
            - containerPort: 8080

again, `kind` indicates the object type that this yaml file is to define, here
it is a RC instead of a pod. in `metadata` it is showing the RC's `name` as
`myweb`.  in `spec` is the detail specification of this RC object. `replicas` 5
indicates a same pod will be cloned 4 times to make sure the total number of
pods created by the RC is always 5. `template` gives information about
the containers that will run in the pod, same as what you saw in a `pod` yaml
file.

now use this yaml file to create the RC object:

    kubectl create -f myweb-rc.yaml
    replicationcontroller "myweb" created

    $ kubectl get rc
    NAME            DESIRED   CURRENT   READY   AGE
    myweb           5         5         5       3m29s

eventually you will see 5 pods launched:

////
    $ kubectl get pod
    NAME          READY     STATUS              RESTARTS   AGE
    myweb-5ggv6   1/1       Running             0          9s
    myweb-5ww92   1/1       Running             0          50s
    myweb-lbj89   0/1       ContainerCreating   0          9s
    myweb-m6nrx   0/1       ContainerCreating   0          9s
    myweb-q5gv4   1/1       Running             0          50s
////

    $ kubectl get pod
    NAME          READY     STATUS    RESTARTS   AGE
    myweb-5ggv6   1/1       Running   0          21m
    myweb-5ww92   1/1       Running   0          22m
    myweb-lbj89   1/1       Running   0          21m
    myweb-m6nrx   1/1       Running   0          21m
    myweb-q5gv4   1/1       Running   0          22m

with `replicas` parameter specified in RC object yaml file, the kubernetes
replication controller, running as part of `kube-controller-manager` process in
the `master node`, will keep monitoring the number of running pods spawned by
the RC, and automatically launch new ones should any of them runs into failures. 

you can test this out by deleting one of the pod:

    $ kubectl delete pod myweb-5ggv6
    pod "myweb-5ggv6" deleted
    $ kubectl get pod
    NAME          READY     STATUS        RESTARTS   AGE
    myweb-5ggv6   0/1       Terminating   0          22m        #<---
    myweb-5v9w6   1/1       Running       0          2s
    myweb-5ww92   1/1       Running       0          23m
    myweb-lbj89   1/1       Running       0          22m
    myweb-m6nrx   1/1       Running       0          22m
    myweb-q5gv4   1/1       Running       0          23m

    $ kubectl get pod
    NAME          READY     STATUS        RESTARTS   AGE
    myweb-5v9w6   1/1       Running       0          5s
    myweb-5ww92   1/1       Running       0          23m
    myweb-lbj89   1/1       Running       0          22m
    myweb-m6nrx   1/1       Running       0          22m
    myweb-q5gv4   1/1       Running       0          23m

there are other benefits with RC. actually since this abstraction is so popular
and heavily used in practice that, two very similar new objects have been
designed with more powerful features introduced. The original RC object and its
abstraction has been split into 2 new objects named `RS(ReplicaSet)` and
`deployment`.  roughly, you can call them "next generation of RC". let's stop
exploring more RC features for now and move our focus to these 2 objects.

=== ReplicaSet 

`ReplicaSet`, or `RS` object, is pretty much the same thing as a `RC` object,
with just one major exception - the looks of `selector`.

    $ cat myweb-rs.yaml
    apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      name: myweb
    spec:
      replicas: 1
      selector:
        matchLabels:                                    #<---
          app: myweb                                    #<---
        matchExpressions:                               #<---
          - {key: app, operator: In, values: [myweb]}   #<---
      template:
        metadata:
          labels:
            app: myweb
        spec:
          containers:
          - name: myweb
            image: kubeguide/tomcat-app:v1
            ports:
            - containerPort: 8080
            env:
            - name: MYSQL_SERVICE_HOST
              value: 'mysql'
            - name: MYSQL_SERVICE_PORT
              value: '3306'
            - name: MYSQL_ROOT_PASSWORD
              value: "123456"

RC uses "Equality-based" selector while RS uses "set-based". function-wise the
two forms of selector do the same job - to "select" the pod with a matching
"label".

      #RS:
      selector:
        matchLabels:                                 
          app: myweb                                 
        matchExpressions:                            
          - {key: app, operator: In, values: [myweb]}

      #RC:
      selector:
        app: myweb

    $ kubectl create -f myweb-rs.yaml
    replicaset.extensions/myweb created

    $ kubectl get pod
    NAME                         READY   STATUS    RESTARTS   AGE
    myweb-lkwvt                  1/1     Running   0          8s

a RS is created and it launchs a pod, just same as what a RC would do.
if you compare the `kubectl describe` on the 2 objects:

    $ kubectl describe rs myweb                                        
    ......
    Selector:     app=myweb,app in (myweb)                             
    ......
      Type    Reason            Age   From                   Message   
      ----    ------            ----  ----                   -------   
      Normal  SuccessfulCreate  15s   replicaset-controller  Created pod: myweb-kt9zx

    $ kubectl describe rc myweb
    ......
    Selector:     app=myweb
    ......
      Type    Reason            Age   From                    Message
      ----    ------            ----  ----                    -------
      Normal  SuccessfulCreate  19s   replication-controller  Created pod: myweb-tbbhc

as you see, most part of the output are the same, with only exception of
selector format. you can also sale the RS same way as you do with RC:

    $ kubectl scale rs myweb --replicas=5
    replicaset.extensions/myweb scaled

    $ kubectl get pod
    NAME                         READY   STATUS    RESTARTS   AGE
    myweb-4jvvx                  1/1     Running   0          3m30s
    myweb-722pf                  1/1     Running   0          3m30s
    myweb-8z8f8                  1/1     Running   0          3m30s
    myweb-lkwvt                  1/1     Running   0          4m28s
    myweb-ww9tn                  1/1     Running   0          3m30s

== Deployment
now you may start to wonder why kubernetes has two different objects to do the
almost same job. as mentioned earlier the features of RC has been extended
through the 2 new objects. we've seen `RS` has done the same job of `RC` just
with a different selector format, now we'll check out the other new object
`deployment` and explore the features coming from it. simply changing `kind`
attribute from `ReplicaSet` to `deployment` we get the yaml file of a deployment
object:

    $ cat myweb-deployment.yaml
    apiVersion: extensions/v1beta1
    kind: Deployment    #<---
    metadata:
      name: myweb
    ...(everything else remains the same as replicaset)...

    $ kubectl create -f myweb-deployment.yaml
    deployment.extensions/myweb created

    $ kubectl get all | grep myweb
    deployment.apps/myweb            1    1        1  1    21s
    replicaset.apps/myweb-c586fd645  1    1        1  21s
    pod/myweb-c586fd645-b2ft8        1/1  Running  0  21s

the deployment is a relatively higher level of abstraction than RC and RS.
deployment does not create a pod directly, the `describe` command reveals this:

    $ kubectl describe deployments myweb
    Name:                   myweb
    Namespace:              default
    CreationTimestamp:      Sat, 25 May 2019 16:00:26 -0400
    Labels:                 app=myweb
    Annotations:            deployment.kubernetes.io/revision: 1
    Selector:               app=myweb,app in (myweb)
    Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  1 max unavailable, 1 max surge
    Pod Template:
      Labels:  app=myweb
      Containers:
       myweb:
        Image:      kubeguide/tomcat-app:v1
        Port:       8080/TCP
        Host Port:  0/TCP
        Environment:
          MYSQL_SERVICE_HOST:   mysql
          MYSQL_SERVICE_PORT:   3306
          MYSQL_ROOT_PASSWORD:  123456
        Mounts:                 <none>
      Volumes:                  <none>
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
    OldReplicaSets:  <none>
    NewReplicaSet:   myweb-c586fd645 (1/1 replicas created)     #<---
    Events:          <none>

what happens is when you create a Deployment, a replica set is created
underneath. The pods defined in a Deployment object are created and supervised
by the Deployment's replicaset. RC on the other hand, works with pod directly.
the workflows differences are shown in this diagram:

                             |=> pod
                             |
    RC =============>========|=> pod
                             |
                             |=> pod

                             |=> pod
                             |
    deployment =====> RS ====|=> pod
                             |
                             |=> pod

You might still be wondering why you need RS as one more layer sitting in
between deployment and pod, after all with RC's magic it seems sufficient to
keep a set of pods running.

the reason is about another important usage scenario in pratice: pod update.
"rolling update" feature is one of the "more powerful feature" coming with
deployment object. this is how it works:

when you update the pod with new software, the `deployment` object introduces a
new RS that will start the pod update process. the idea is NOT to "login" to the
existing pod and do the image update in there, instead, the new RC just creates
a new pod equiped with the new software release in it. once this new pod is up
and running, the original RS will be "scaled down" by one, making the total
number of running pod remaining unchanged. new RS will continue to scale up by
one and original RS scales down the same number of pod. this process repeats
until number of pods created by new RS reaches the original replica number
defined in the deployment, and that is the time when all of the original RS's
pods are terminated. this process is depicted in this diagram:

                             |=> pod
                 |==> RS ====|=> pod
                 |   (v1)    |=> pod
                 |
                 |           |=> pod
                 |           |
    deployment ==|==> RS ====|=> pod
                 |   (v2)    |
                 |           |=> pod
                 |
                 |           |=> pod
                 |==> RS ====|=> pod
                     (v3)    |=> pod

now coming back to the question in your mind. This whole process of creating a
new RS, scaling up the new RS and scaling down the old one simultaneously, is
fully automated and taken care of by the deployment object. it is `deployment`
who is `deploying` and driving `ReplicaSet` object, which, in this sense working
as merely a backend of it. this is why `deployment` is considered a higher layer
object in kubernetes, also the reason why it is officially recommended to never
use `ReplicaSet` along without `deployment`. in contrast, RC alone, without this
additional higher layer abstraction, is not able to coordinate this process.

deployment also has the ability to "record" the whole process, so in case
needed, you can display the update history after the update job is done:

    $ kubectl describe deployment/nginx-deployment
    Name:                   nginx-deployment
    ......
    NewReplicaSet:   nginx-deployment-6fdbb596db (3/3 replicas created)
    Events:
      Type    Reason             Age   From                   Message
      ----    ------             ----  ----                   -------
      Normal  ScalingReplicaSet  28m   deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 3    #<------
      Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 1    #<------
      Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 2  #<------
      Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 2    #<------
      Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 1  #<------
      Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 3    #<------
      Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 0  #<------

similarly, you can also pause/resume the update process to verify the changes
before proceeding:

    $ kubectl rollout pause deployment/nginx-deployment
    $ kubectl rollout resume deployment/nginx-deployment

you can even "undo" the update when things are going wrong during the
maintenance window

    $ kubectl rollout undo deployment/nginx-deployment

    $ kubectl describe deployment/nginx-deployment
    Name:                   nginx-deployment
    ......
    NewReplicaSet:   nginx-deployment-6fdbb596db (3/3 replicas created)
    NewReplicaSet:   nginx-deployment-67594d6bf6 (3/3 replicas created)
    Events:
      Type    Reason              Age              From                   Message
      ----    ------              ----             ----                   -------
      Normal  DeploymentRollback  8m               deployment-controller  Rolled back deployment "nginx-deployment" to revision 1       #<------
      Normal  ScalingReplicaSet   8m               deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 1        #<------
      Normal  ScalingReplicaSet   8m               deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 2      #<------
      Normal  ScalingReplicaSet   8m               deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 2        #<------
      Normal  ScalingReplicaSet   8m (x2 over 1h)  deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 3        #<------
      Normal  ScalingReplicaSet   8m               deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 1      #<------
      Normal  ScalingReplicaSet   8m               deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 0      #<------

This is pretty much similar as the junos's `rollback` magic command that you
probably use everyday when you need to quickly revert the changes you make to
your router. Typically you do this when something is broken in your router
deployment. comparing with how much work it takes to prepare for the software
upgrade during maintenance window in the old days, this is going to be a killing
feature to have!

//image::https://user-images.githubusercontent.com/2038044/58375577-1f2b6a80-7f24-11e9-84d7-24dc2c40db32.png[]


== Quota

similiar to openstack 'tenant', you can now apply constraints that limits
resource consumption per namespace. for example, you can limit the quantity of
objects that can be created in a namespace, total amount of compute resources
that may be consumed by resources, etc. the constraint in k8s is called 'quota'.
here is an example:

    kubectl -n ns-user-2 create quota foobar --hard pods=1

we just created a quota 'foobar', and the constraint we gave is 'pods=1' - only
one pod is allowed to be created in this NS.

----
$ kubectl get quota -n ns-user-2
NAME     CREATED AT
foobar   2019-06-14T04:25:37Z

$ kubectl get quota -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: 2019-06-14T04:25:37Z
    name: foobar
    namespace: ns-user-2
    resourceVersion: "823606"
    selfLink: /api/v1/namespaces/ns-user-2/resourcequotas/foobar
    uid: 76052368-8e5c-11e9-87fb-0050569e6cfc
  spec:
    hard:
      pods: "1"
  status:
    hard:
      pods: "1"
    used:
      pods: "1"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

now create a rc with replica=2

----
$ cat rc-ubuntu.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: rc-ubuntuapp
spec:
 replicas: 2
 template:
   metadata:
     labels:
       run: ubuntuapp
   spec:
     containers:
     - name: ubuntuapp
       image: ubuntu-upstart

$ kubectl apply -f rc-ubuntu.yaml
replicationcontroller/rc-ubuntuapp created

$ kubectl get pod
NAME                 READY   STATUS    RESTARTS   AGE
rc-ubuntuapp-2j84g   1/1     Running   0          10s
----

what we desired is 2 pods, but only 1 will be created:

----
$ kubectl get rc
NAME        DESIRED   CURRENT   READY   AGE
ubuntuapp   2         1         1       3m19s
----

and the reason is that the 2nd pod creation is "forbidden" due to quota
exceeded:

    ..."rc-ubuntuapp-88cxk" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1

----
$ kubectl describe rc
Name:         rc-ubuntuapp
Namespace:    ns-user-2
Selector:     run=ubuntuapp
......
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate         #<---
Events:
  Type     Reason            Age                 From                    Message
  ----     ------            ----                ----                    -------
  Normal   SuccessfulCreate  2m8s                replication-controller  Created pod: rc-ubuntuapp-2j84g
  Warning  FailedCreate      2m8s                replication-controller  Error creating: pods "rc-ubuntuapp-88cxk" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
  Warning  FailedCreate      2m8s                replication-controller  Error creating: pods "rc-ubuntuapp-tztv4" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
  ......
  Warning  FailedCreate      77s (x6 over 2m6s)  replication-controller  (combined from similar events): Error creating: pods "rc-ubuntuapp-rtb56" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
----

new pod will can be create after the quota is removed:

----
root@test1:~# kubectl delete quota foobar
resourcequota "foobar" deleted

$ kubectl scale rc rc-ubuntuapp --replicas=3
replicationcontroller/rc-ubuntuapp scaled

$ kubectl get pod
NAME                 READY   STATUS    RESTARTS   AGE
rc-ubuntuapp-2j84g   1/1     Running   0          8m4s
rc-ubuntuapp-rssl9   1/1     Running   0          16s
rc-ubuntuapp-z6cmn   1/1     Running   0          16s
----


== Service

POD get instantiated, terminated and moved from one Node to another, in doing
so POD changes IP address so how would we keep track of that?  Even if the POD
isn’t moving how traffic from outside reach a certain POD 

the answer for both questions is Kubernetes services.  

Services is an abstraction that defines a logical set of Pods and a policy by
which you can access them, you may think of Services as your waiter in a big
restaurant, this waiter isn’t cooking nor preparing the food but he just
abstract everything happing at the kitchen for you as you deal only with this
waiter 

so let’s cover three different type of services Node port , ClusterIP and load
balancer and we begin with Node port by creating this  ( I need to create a
diagram ) 

=== Cluster IP

=== NodePort 

NodePort service is an easy way to expose an application running on POD by
mapping a port in the node that host this POD with a port of the application
the POD as shown in the diagram 
 
image::https://user-images.githubusercontent.com/2038044/57959367-7badda80-78d1-11e9-835f-ccce71ffe38a.png[]

there is 2 very important parts in this services YAML file ports and selector.

targetPort is the actual port used by the application in here its port 80 as we
are planning to run a web server and nodeport is port on the node hosting that
PODs 

selector is the label selector which determine which set of pods targeted by
this services, in here any POD with label app: FRONT-END will be serviced by
this services

    apiVersion: v1
    kind: Service
    metadata:
      name: web-app
    spec:
      selector:
        app: webserver
      type: NodePort
      ports:
      - targetPort: 80
        port: 80
        nodePort: 32001

[TIP]
====
* Kubernetes by default allocate node port from (3000-32767) range it could be
  change using the flag --service-node-port-range
* The default service type is ClusterIP 
* Be aware with the change of the Node ip address as it could effect your services 
====

now let’s expose the pod we created before with services shown after putting in
web-app.yaml file

    [root@ip-172-25-1-56 /]# kubectl create -f web-app.yaml
    service "web-app" created

    [root@ip-172-25-1-56 /]# kubectl describe services web-app
    Name:                     web-app
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=webserver
    Type:                     NodePort
    IP:                       10.98.21.191
    Port:                     <unset>  80/TCP
    TargetPort:               80/TCP
    NodePort:                 <unset>  32001/TCP
    Endpoints:                10.47.255.250:80
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Events:                   <none>

Now we can test that by just send CURL -i to sent http request using the CLI 

    [root@computeee centos]# curl -i 10.98.21.191:80
    HTTP/1.1 200 OK
    Server: nginx/1.15.12
    Date: Tue, 14 May 2019 18:33:07 GMT
    Content-Type: text/html
    Content-Length: 612
    Last-Modified: Tue, 16 Apr 2019 13:08:19 GMT
    Connection: keep-alive
    ETag: "5cb5d3c3-264"
    Accept-Ranges: bytes

    <!DOCTYPE html>
    <html>
    <head>
    <title>Welcome to nginx!</title>
    <style>
        body {
            width: 35em;
            margin: 0 auto;
            font-family: Tahoma, Verdana, Arial, sans-serif;
        }
    </style>
    </head>
    <body>
    <h1>Welcome to nginx!</h1>
    <p>If you see this page, the nginx web server is successfully installed and
    working. Further configuration is required.</p>

    <p>For online documentation and support please refer to
    <a href="http://nginx.org/">nginx.org</a>.<br/>
    Commercial support is available at
    <a href="http://nginx.com/">nginx.com</a>.</p>

    <p><em>Thank you for using nginx.</em></p>
    </body>
    </html>  

=== load balancing

== Ingress 
== Annotation 
== Network policy




