// vim:set ft=asciidoc syntax=ON tw=80:
= Kubernetes in practice
:toc: right
//:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline
:numbered:

This chapter introduces some of the fundamental objects and features of
kubernetes.

== Labels

Imagine you have a POD that’s need to be host on a machine with certain
specifications ( SSD HD, physical location , processing power , ..,etc ) 
OR imagine you want to search or group your PODs for easier administration 
what would you do ?
then label is your way to go, in Kubernetes Label is a Key/value pairs attached to an object  
let’s see how can we use label with node selector to make a POD is lunched on a certain machine 

[NOTE]
====
* You can assign multiple labels per object but avoid using too much label or
  too little, too much would get you confused and too little won’t give the real
  benefits of grouping, selecting and searching 
* Best practice is to assign labels to indicate
    - application/program ID use this POD
    - owner (who manage this POD/application)
    - stage (the POD/application in development/testing/ production as well version)
    - resource requirements (SSD, CPU, storage)
    - location (preferred location/zone/ Datacenter to run this POD/application) 
====

Let’s assign label (stage: testing) & (zone: production) to two nodes
respectively then try to lunch a POD with label (zone: testing) 
 
    kubectl get nodes --show-labels

    NAME      STATUS     ROLES     AGE       VERSION   LABELS
    cent222   Ready      <none>    2h        v1.9.2    <none>
    cent111   NotReady   <none>    2h        v1.9.2    <none>
    cent333   Ready      <none>    2h        v1.9.2    <none>


    kubectl label nodes cent333 stage=testing
    kubectl label nodes cent222 stage=production

    kubectl get nodes --show-labels

    NAME         STATUS    ROLES   AGE  VERSION  LABELS
    cent222  Ready     <none>  2h   v1.9.2   stage=production
    cent111  NotReady  <none>  2h   v1.9.2   <none>
    cent333  Ready     <none>  2h   v1.9.2   stage=testing

now let’s lunch a basic Nginx POD tagged with stage=testing in the nodeSelector
and confirm it will land on a node tagged with stage=testing

    [root@cent111]# cat > web-server.yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx
      labels:
        app: webserver
    spec:
      containers:
      - name: nginx
        image: nginx
      nodeSelector:
        stage: testing

    [root@cent111]# kubectl create -f web-server.yaml
    pod "wordpress" created

    [root@cent111]# kubectl get pods --output=wide
    NAME        READY     STATUS    RESTARTS   AGE       IP              NODE
    wordpress   1/1       Running   0          48s       10.47.255.238   cent333


[NOTE]
====
You can assign POD to certain node without label by adding the argument
nodeName: nodeX under spec in the YAML file where nodeX is the name of the node  

In kubernetes, any objects can be identified using a label. You will see how
labels are linking are PODs to Replica-set ,Deployment , services …,etc  and
that’s what we will see next with services and label selector 

====

== NameSpace

=== what is NameSpace

As in many other platforms, normally there is more than one users (or teams) working on a
kubernetes cluster. suppose a pod named 'webserver1' has been built by 'dev'
department, when 'sales' department attempts to launch a pod with the same name,
the system will give an error:

----
Error from server (AlreadyExists): error when creating "webserver1.yaml": pods "webserver1" already exists
----

this is a very common scenario in many today's network platforms - an object
name has to be unique in the same "scope":

* IP prefix in a router
* virtual network in openstack cluster
* a variable name in a program

and the solution is to support multiple scopes or namespaces, respectively:

* VRF
* project/tenant
* function/class

the solution in k8s is just called 'Namespaces', or 'NS' for short, which
provides a scope for names. Names of resources need to be unique within a
namespace, but not across namespaces. it is a nature way to divide cluster
resources between multiple users. 

=== create a NS
to create a NS is pretty simple, you can avoid the need to give a yaml file by
using kubectl with '-f' option, followed by '-' and hit enter:

    root@test3:~# kubectl create -f -

now the kubectl will wait for you to manually input the definition of NS from
'stdin', you can now input these 4 lines to create a VN:

    apiVersion: v1
    kind: Namespace
    metadata:
        name: dev

when done, press ctr-d to submit the stdin buffer content into kubectl.

    namespace/development created

new namespace dev is now created

    root@test3:~# kubectl get ns
    NAME          STATUS    AGE
    default       Active    15d
    dev           Active    5s  #<-----

=== default NS

you may notice there is a NS named 'default' in any k8s setup. that is, as the
name indicates, a "default" NS that is instantiated when the cluster is
provisioned. default NS is required to hold the default set of Pods, Services,
and Deployments used by the cluster.  same as the "default routing table" in a
router, "default tenant" in openstack setup - there needs to be a scope
providing you an initial working environment so from there you can create all
other objects.

now 'dev' 'webserver1' in 'dev' NS won't conflict with 'webserver1' in 'sales'
NS.

----
$ kubectl get pod --all-namespaces -o wide
NAMESPACE  NAME  READY  STATUS   RESTARTS  AGE   IP             NODE     NOMINATED  NODE
......
dev        csrx  1/1    Running  4         2d4h  10.47.255.249  cent222  <none>
......
sales      csrx  1/1    Running  4         2d4h  10.47.255.244  cent222  <none>
......
----

=== quota

similiar to openstack 'tenant', you can now apply constraints that limits
resource consumption per namespace. for example, you can limit the quantity of
objects that can be created in a namespace, total amount of compute resources
that may be consumed by resources, etc. the constraint in k8s is called 'quota'.
here is an example:

    kubectl -n ns-user-2 create quota foobar --hard pods=1

we just created a quota 'foobar', and the constraint we gave is 'pods=1' - only
one pod is allowed to be created in this NS.

----
$ kubectl get quota -n ns-user-2
NAME     CREATED AT
foobar   2019-06-14T04:25:37Z

$ kubectl get quota -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    creationTimestamp: 2019-06-14T04:25:37Z
    name: foobar
    namespace: ns-user-2
    resourceVersion: "823606"
    selfLink: /api/v1/namespaces/ns-user-2/resourcequotas/foobar
    uid: 76052368-8e5c-11e9-87fb-0050569e6cfc
  spec:
    hard:
      pods: "1"
  status:
    hard:
      pods: "1"
    used:
      pods: "1"
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

now create a rc with replica=2

----
$ cat rc-ubuntu.yaml
apiVersion: v1
kind: ReplicationController
metadata:
name: rc-ubuntuapp
spec:
 replicas: 2
 template:
   metadata:
     labels:
       run: ubuntuapp
   spec:
     containers:
     - name: ubuntuapp
       image: ubuntu-upstart

$ kubectl apply -f rc-ubuntu.yaml
replicationcontroller/rc-ubuntuapp created

$ kubectl get pod
NAME                 READY   STATUS    RESTARTS   AGE
rc-ubuntuapp-2j84g   1/1     Running   0          10s
----

what we "desired" is 2 pods, but only 1 is "ready"

----
$ kubectl get rc
NAME        DESIRED   CURRENT   READY   AGE
ubuntuapp   2         1         1       3m19s
----

the reason is that the 2nd pod creation is "forbidden" due to quota
exceeded:

----
..."rc-ubuntuapp-88cxk" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
----

this error message is seen from the pod details given by `kubectl describe` command

----
$ kubectl describe rc
Name:         rc-ubuntuapp
Namespace:    ns-user-2
Selector:     run=ubuntuapp
......
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate         #<---
Events:
  Type     Reason            Age                 From                    Message
  ----     ------            ----                ----                    -------
  Normal   SuccessfulCreate  2m8s                replication-controller  Created pod: rc-ubuntuapp-2j84g
  Warning  FailedCreate      2m8s                replication-controller  Error creating: pods "rc-ubuntuapp-88cxk" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
  Warning  FailedCreate      2m8s                replication-controller  Error creating: pods "rc-ubuntuapp-tztv4" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
  ......
  Warning  FailedCreate      77s (x6 over 2m6s)  replication-controller  (combined from similar events): Error creating: pods "rc-ubuntuapp-rtb56" is forbidden: exceeded quota: foobar, requested: pods=1, used: pods=1, limited: pods=1
----

new pod will can be created after the quota is removed:

----
root@test1:~# kubectl delete quota foobar
resourcequota "foobar" deleted

$ kubectl scale rc rc-ubuntuapp --replicas=3
replicationcontroller/rc-ubuntuapp scaled

$ kubectl get pod
NAME                 READY   STATUS    RESTARTS   AGE
rc-ubuntuapp-2j84g   1/1     Running   0          8m4s
rc-ubuntuapp-rssl9   1/1     Running   0          16s
rc-ubuntuapp-z6cmn   1/1     Running   0          16s
----

== Replication Controller

you have learned how to launch a pod that representing your containers from its
yaml file in chapter 2. one question will rise in your mind: what if I need 5
exactly the same pods (each runs a apache container) to make sure the web
service appears more robust? shall I change the name in yaml file then repeat the
same commands to create 5 more pods? or maybe with a shell script? kubernetes
already has the objects to address this exact demand and the right answer are RC -
replication controller or RS - ReplicaSet

> A ReplicationController ensures that a specified number of pod replicas are
> running at any one time. In other words, a ReplicationController makes sure
> that a pod or a homogeneous set of pods is always up and available.

=== create RC

let's look at how it works with an example. first create a yaml file for a RC
object named `myweb`.

    #myweb-rc.yaml
    apiVersion: v1
    kind: ReplicationController
    metadata:
      name: myweb
    spec:
      replicas: 3
      selector:
        app: myweb
      template:
        metadata:
          labels:
            app: myweb
        spec:
          containers:
          - name: myweb
            image: kubeguide/tomcat-app:v1
            ports:
            - containerPort: 8080

again, `kind` indicates the object type that this yaml file is to define, here
it is a RC instead of a pod. in `metadata` it is showing the RC's `name` as
`myweb`.  in `spec` is the detail specification of this RC object. `replicas` 5
indicates a same pod will be cloned 4 times to make sure the total number of
pods created by the RC is always 5. `template` gives information about
the containers that will run in the pod, same as what you saw in a `pod` yaml
file.

now use this yaml file to create the RC object:

    kubectl create -f myweb-rc.yaml
    replicationcontroller "myweb" created

if you are quick enough, you may capture the intermediate status when the new
pods are being created:

    $ kubectl get pod
    NAME          READY     STATUS              RESTARTS   AGE
    myweb-5ggv6   1/1       Running             0          9s
    myweb-5ww92   1/1       Running             0          50s
    myweb-lbj89   0/1       ContainerCreating   0          9s
    myweb-m6nrx   0/1       ContainerCreating   0          9s
    myweb-q5gv4   1/1       Running             0          50s

eventually you will see 5 pods launched:

    $ kubectl get rc
    NAME            DESIRED   CURRENT   READY   AGE
    myweb           5         5         5       3m29s

    $ kubectl get pod
    NAME          READY     STATUS    RESTARTS   AGE
    myweb-5ggv6   1/1       Running   0          21m
    myweb-5ww92   1/1       Running   0          22m
    myweb-lbj89   1/1       Running   0          21m
    myweb-m6nrx   1/1       Running   0          21m
    myweb-q5gv4   1/1       Running   0          22m

=== test RC

with `replicas` parameter specified in RC object yaml file, the kubernetes
`replication controller`, running as part of `kube-controller-manager` process in
the `master node`, will keep monitoring the number of running pods spawned by
the RC, and automatically launch new ones should any of them runs into failures. 
the key to learn is, individual pod may die any time, but the "pool" as a whole
is always up and running, making a robust service. you will understand this
better when you learn kubernetes `service`.

you can test this out by deleting one of the pod:

    $ kubectl delete pod myweb-5ggv6
    pod "myweb-5ggv6" deleted

    $ kubectl get pod
    NAME          READY     STATUS        RESTARTS   AGE
    myweb-5ggv6   0/1       Terminating   0          22m        #<---
    myweb-5v9w6   1/1       Running       0          2s
    myweb-5ww92   1/1       Running       0          23m
    myweb-lbj89   1/1       Running       0          22m

    $ kubectl get pod
    NAME          READY     STATUS        RESTARTS   AGE
    myweb-5v9w6   1/1       Running       0          5s
    myweb-5ww92   1/1       Running       0          23m
    myweb-lbj89   1/1       Running       0          22m

you can scale up/down replicas in rc.

    $ kubectl scale rc myweb --replica=5
    replicationcontroller/myweb scaled
    
    $ kubectl get pod
    NAME          READY     STATUS              RESTARTS   AGE
    myweb-5v9w6   1/1       Running             0          8s
    myweb-5ww92   1/1       Running             0          23m
    myweb-lbj89   1/1       Running             0          22m
    myweb-m6nrx   0/1       ContainerCreating   0          2s
    myweb-q5gv4   1/1       ContainerCreating   0          2s
    
    $ kubectl get pod
    NAME          READY     STATUS        RESTARTS   AGE
    myweb-5v9w6   1/1       Running       0          10s
    myweb-5ww92   1/1       Running       0          23m
    myweb-lbj89   1/1       Running       0          22m
    myweb-m6nrx   1/1       Running       0          5s
    myweb-q5gv4   1/1       Running       0          5s

there are other benefits with RC. actually since this abstraction is so popular
and heavily used in practice that, two very similar objects `RS` - `ReplicaSet`
and `Deploy` - `Deployment` have been designed with more powerful features
introduced.  roughly, you can call them "next generation of RC". let's stop
exploring more RC features for now and move our focus to these 2 objects.

== ReplicaSet

`ReplicaSet`, or `RS` object, is pretty much the same thing as a `RC` object,
with just one major exception - the looks of `selector`.

    $ cat myweb-rs.yaml
    apiVersion: extensions/v1beta1
    kind: ReplicaSet
    metadata:
      name: myweb
    spec:
      replicas: 1
      selector:
        matchLabels:                                    #<---
          app: myweb                                    #<---
        matchExpressions:                               #<---
          - {key: app, operator: In, values: [myweb]}   #<---
      template:
        metadata:
          labels:
            app: myweb
        spec:
          containers:
          - name: myweb
            image: kubeguide/tomcat-app:v1
            ports:
            - containerPort: 8080
            env:
            - name: MYSQL_SERVICE_HOST
              value: 'mysql'
            - name: MYSQL_SERVICE_PORT
              value: '3306'
            - name: MYSQL_ROOT_PASSWORD
              value: "123456"

RC uses "Equality-based" selector while RS uses "set-based". function-wise the
two forms of selector do the same job - to "select" the pod with a matching
"label".

    #RS:
    selector:
      matchLabels:                                 
        app: myweb                                 
      matchExpressions:                            
        - {key: app, operator: In, values: [myweb]}

    #RC:
    selector:
      app: myweb

    $ kubectl create -f myweb-rs.yaml
    replicaset.extensions/myweb created

    $ kubectl get pod
    NAME                         READY   STATUS    RESTARTS   AGE
    myweb-lkwvt                  1/1     Running   0          8s

a RS is created and it launchs a pod, just same as what a RC would do.
if you compare the `kubectl describe` on the 2 objects:

    $ kubectl describe rs myweb                                        
    ......
    Selector:     app=myweb,app in (myweb)                             
    ......
      Type    Reason            Age   From                   Message   
      ----    ------            ----  ----                   -------   
      Normal  SuccessfulCreate  15s   replicaset-controller  Created pod: myweb-kt9zx

    $ kubectl describe rc myweb
    ......
    Selector:     app=myweb
    ......
      Type    Reason            Age   From                    Message
      ----    ------            ----  ----                    -------
      Normal  SuccessfulCreate  19s   replication-controller  Created pod: myweb-tbbhc

as you see, most part of the output are the same, with only exception of
selector format. you can also scale the RS same way as you do with RC:

    $ kubectl scale rs myweb --replicas=5
    replicaset.extensions/myweb scaled

    $ kubectl get pod
    NAME                         READY   STATUS    RESTARTS   AGE
    myweb-4jvvx                  1/1     Running   0          3m30s
    myweb-722pf                  1/1     Running   0          3m30s
    myweb-8z8f8                  1/1     Running   0          3m30s
    myweb-lkwvt                  1/1     Running   0          4m28s
    myweb-ww9tn                  1/1     Running   0          3m30s

== Deployment 

now you may start to wonder why kubernetes has two different objects to do the
almost same job. as mentioned earlier the features of RC has been extended
through the 2 new objects. we've seen the first new object `RS` , which has done
the same job of `RC` only with a different selector format, now we'll check out
the other new object `DEPLOY - deployment` and explore the features coming from it.

=== create a deployment

simply changing `kind` attribute from `ReplicaSet` to `deployment` we get the
yaml file of a deployment object:

    $ cat myweb-deployment.yaml
    apiVersion: extensions/v1beta1
    kind: Deployment    #<---
    metadata:
      name: myweb
    ...(everything else remains the same as replicaset)...

    $ kubectl create -f myweb-deployment.yaml
    deployment.extensions/myweb created

    $ kubectl get deployment
    NAME                   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
    deployment.apps/myweb  1        1        1           1          21s

the deployment is a relatively higher level of abstraction than RC and RS.
deployment does not create a pod directly, the `describe` command reveals this:

    $ kubectl describe deployments myweb
    Name:                   myweb
    Namespace:              default
    CreationTimestamp:      Sat, 25 May 2019 16:00:26 -0400
    Labels:                 app=myweb
    Annotations:            deployment.kubernetes.io/revision: 1
    Selector:               app=myweb,app in (myweb)
    Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  1 max unavailable, 1 max surge
    Pod Template:
      Labels:  app=myweb
      Containers:
       myweb:
        Image:      kubeguide/tomcat-app:v1
        Port:       8080/TCP
        Host Port:  0/TCP
        Environment:
          MYSQL_SERVICE_HOST:   mysql
          MYSQL_SERVICE_PORT:   3306
          MYSQL_ROOT_PASSWORD:  123456
        Mounts:                 <none>
      Volumes:                  <none>
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
    OldReplicaSets:  <none>
    NewReplicaSet:   myweb-c586fd645 (1/1 replicas created)     #<---
    Events:          <none>

////
    $ kubectl get all | grep myweb
    deployment.apps/myweb            1    1        1  1    21s
    replicaset.apps/myweb-c586fd645  1    1        1  21s
    pod/myweb-c586fd645-b2ft8        1/1  Running  0  21s
////

=== deployment work flow

what happens is when you create a Deployment, a replica set is created
automatically. The pods defined in a Deployment object are created and supervised
by the Deployment's replicaset. RC on the other hand, works with pod directly.
the workflows differences are shown in this diagram:

                             |=> pod
                             |
    RC =============>========|=> pod
                             |
                             |=> pod

                             |=> pod
                             |
    deployment =====> RS ====|=> pod
                             |
                             |=> pod

You might still be wondering why you need RS as one more layer sitting in
between deployment and pod, after all with RC's magic it seems sufficient to
keep a set of pods running.

=== rolling update

the reason is about another important usage scenario in pratice: pod update.
"rolling update" feature is one of the "more powerful feature" coming with
deployment object. in this section we'll demonstrate the feature with a test
case, then we'll explain how it works.

==== test rolling update

suppose we have a nginx-deployment, with `replica=3` an pod image `1.7.9`.
later we want to upgrade the image from version `1.7.9` to new image version
`1.9.1`. with `kuberctl` we can use `set image` option and specify the new
version number to trigger the update:

    $ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
    deployment.extensions/nginx-deployment image updated

now chck the deployment information again:

    $ kubectl describe deployment/nginx-deployment
    Name:                   nginx-deployment
    Namespace:              default
    CreationTimestamp:      Tue, 11 Sep 2018 20:49:45 -0400
    Labels:                 app=nginx
    Annotations:            deployment.kubernetes.io/revision=2
    Selector:               app=nginx
    Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
    StrategyType:           RollingUpdate
    MinReadySeconds:        0
    RollingUpdateStrategy:  25% max unavailable, 25% max surge
    Pod Template:
      Labels:  app=nginx
      Containers:
       nginx:
        Image:        nginx:1.9.1       #<------
        Port:         80/TCP
        Host Port:    0/TCP
        Environment:  <none>
        Mounts:       <none>
      Volumes:        <none>
    Conditions:
      Type           Status  Reason
      ----           ------  ------
      Available      True    MinimumReplicasAvailable
      Progressing    True    ReplicaSetUpdated
    OldReplicaSets:  nginx-deployment-67594d6bf6 (3/3 replicas created)
    NewReplicaSet:   nginx-deployment-6fdbb596db (1/1 replicas created)
    Events:
      Type    Reason             Age   From                   Message
      ----    ------             ----  ----                   -------
      Normal  ScalingReplicaSet  4m    deployment-controller  Scaled up replica
      set nginx-deployment-67594d6bf6 to 3  #<---
      Normal  ScalingReplicaSet  7s    deployment-controller  Scaled up replica
      set nginx-deployment-6fdbb596db to 1  #<---

two changes we can observe here:

* image version in deployment is updated
* a new RS `nginx-deployment-6fdbb596db` is created, with a `replica` set to 1

and with the new RS with `replica` being 1, a new pod ("the fourth one") is now generated

    $ kubectl get pods
    NAME                                READY     STATUS              RESTARTS   AGE
    frontend-6cfdb4d686-2fvvm           1/1       Running             3          3d
    mysql-z8k42                         1/1       Running             2          3d
    myweb-csbvj                         1/1       Running             3          3d
    myweb-s9tx8                         1/1       Running             3          3d
    nginx                               1/1       Running             0          9h
    nginx-deployment-67594d6bf6-88wqk   1/1       Running             0          4m
    nginx-deployment-67594d6bf6-m4fbj   1/1       Running             0          4m
    nginx-deployment-67594d6bf6-td2xn   1/1       Running             0          4m
    nginx-deployment-6fdbb596db-4b8z7   0/1       ContainerCreating   0          17s        #<------
    redis-php                           2/2       Running             4          2d
    volume-pod                          2/2       Running             4          2d

the new pod is with new image:

    $ kubectl describe pod/nginx-deployment-6fdbb596db-4b8z7 | grep Image:
    ...(snipped)...
        Image:          nginx:1.9.1     #<---
    ...(snipped)...

while the old pod is still with old image

    $ kubectl describe pod/nginx-deployment-67594d6bf6-td2xn | grep Image:
    ...(snipped)...
        Image:          nginx:1.7.9     #<------
    ...(snipped)...

wait and keep checking the pods status, eventually all old pods are terminated
and 3 new pods are running - the pod name confirms they are new ones:

    $ kubectl get pods
    NAME                                READY     STATUS    RESTARTS   AGE
    nginx-deployment-6fdbb596db-4b8z7   1/1       Running   0          1m
    nginx-deployment-6fdbb596db-bsw25   1/1       Running   0          18s
    nginx-deployment-6fdbb596db-n9tpg   1/1       Running   0          21s

so the "update" is done and all pods are now running with new version of the
image. 

==== how it works

after you see our update process, you may argue that: hold on... this is now
"update", this should be called "replacement" - kubernetes use 3 new pods
running with new image to replace the old pods! precisely speaking, yes that is
true. but that is how it works kubernetes's "philosophy" - pod is cheap and
replacement is easier. imaging how much work it will be when you have to "login"
each pod, uninstall old images, cleaning up the environment and only to install
a new image. let's look at more details about this process and understand why it
is called a "rolling" update.

when you update the pod with new software, the `deployment` object introduces a
new RS that will start the pod update process. the idea is NOT to "login" to the
existing pod and do the image update in there, instead, the new RC just creates
a new pod equiped with the new software release in it. once this new (and
"additional") pod is up and running, the original RS will be "scaled down" by
one, making the total number of running pod remaining unchanged. new RS will
continue to scale up by one and original RS scales down by same number.  this
process repeats until number of pods created by new RS reaches the original
replica number defined in the deployment, and that is the time when all of the
original RS's pods are terminated. this process is depicted in this diagram:

                  
                 |           |=> pod-v1
    deployment ==|==> RS ====|=> pod-v1
                 |   (v1)    |=> pod-v1

                 |           |=> pod-v1
                 |==> RS ====|=> pod-v1
                 |   (v1)    |
    deployment ==|
                 |           |=> pod-v2
                 |==> RS ====|
                 |   (v2)    |


                 |           |=> pod-v1
                 |==> RS ====|
                 |   (v1)    |
    deployment ==|
                 |           |=> pod-v2
                 |==> RS ====|=> pod-v2
                 |   (v2)    |

                 |           |
                 |==> RS ====|
                 |   (v1)    |
    deployment ==|
                 |           |=> pod-v2
                 |==> RS ====|=> pod-v2
                 |   (v2)    |=> pod-v2


                 |           |=> pod-v2
    deployment ==|==> RS ====|=> pod-v2
                 |   (v2)    |=> pod-v2
                  
now coming back to the question that why deployment is invented given that RC is
already designed and works fine. This whole process of creating a new RS,
scaling up the new RS and scaling down the old one simultaneously, is fully
automated and taken care of by the deployment object. it is `deployment` who is
`deploying` and driving `ReplicaSet` object, which, in this sense working as
merely a backend of it. 

this is why `deployment` is considered a higher layer object in kubernetes, also
the reason why it is officially recommended to never use `ReplicaSet` along
without `deployment`. in contrast, RC alone, without this additional higher
layer abstraction, is not able to coordinate this process.

==== record

deployment also has the ability to "record" the whole process, so in case
needed, you can review the update history after the update job is done:

----
$ kubectl describe deployment/nginx-deployment
Name:                   nginx-deployment
...(snipped)...
NewReplicaSet:   nginx-deployment-6fdbb596db (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  28m   deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 3    #<------
  Normal  ScalingReplicaSet  24m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 1    #<------
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 2  #<------
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 2    #<------
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 1  #<------
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled up replica set nginx-deployment-6fdbb596db to 3    #<------
  Normal  ScalingReplicaSet  23m   deployment-controller  Scaled down replica set nginx-deployment-67594d6bf6 to 0  #<------
----

==== pause/resume/undo

additionally, you can also pause/resume the update process to verify the changes
before proceeding:

    $ kubectl rollout pause deployment/nginx-deployment
    $ kubectl rollout resume deployment/nginx-deployment

you can even "undo" the update when things are going wrong during the
maintenance window

    $ kubectl rollout undo deployment/nginx-deployment

----
$ kubectl describe deployment/nginx-deployment
Name:                   nginx-deployment
...(snipped)...
NewReplicaSet:   nginx-deployment-6fdbb596db (3/3 replicas created)
NewReplicaSet:   nginx-deployment-67594d6bf6 (3/3 replicas created)
Events:
  Type    Reason              Age From                  Message
  ----    ------              --- ----                  -------
  Normal  DeploymentRollback  8m  deployment-controller  Rolled back deployment "nginx-deployment" to revision 1  #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 1   #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 2 #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 2   #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled up replica set nginx-deployment-67594d6bf6 to 3   #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 1 #<------
  Normal  ScalingReplicaSet   8m  deployment-controller  Scaled down replica set nginx-deployment-6fdbb596db to 0 #<------
----

This is pretty much similar as the junos's `rollback` magic command that you
probably use everyday when you need to quickly revert the changes you make to
your router. Typically you do this when something is broken in your router
deployment. comparing with how much work it takes to prepare for the software
upgrade during maintenance window in the old days, this is going to be a killing
feature to have! later throughout this book we may still use pod/RC to
demonstrate different usage case with labs, however, keep in mind that RC is
going to be deprecated, and it is rather unlikely that you will ever need to
create Pods directly in production environment, so deployment is the
future.   

== Service

POD gets instantiated, terminated and moved from one Node to another, in doing
so POD changes IP address so how would we keep track of that to get uninteruppted
functonalites from pod?  Even if the POD isn’t moving, how traffic reach group of PODs
via single entity?

the answer for both questions is Kubernetes 'SVC - services'.  

Services is an abstraction that defines a logical set of Pods and a policy by
which you can access them, you may think of Services as your waiter in a big
restaurant, this waiter isn’t cooking nor preparing the food but he just
abstract everything happing at the kitchen for you as you deal only with this
waiter.

Simply Service is a layer 4 loadbalancer exposes pods functionalities via specific
ip and port. The service and pods are linked via labels like RS. 

so let’s understand different type of services:

* ClusterIP
* NodePort
* LoadBalancer

=== clusterIP service

the `ClusterIP` type of service is the simplest one. it is the default mode if
the `ServiceType` is not given. the kubernetes official website gives this
diagram to illustrate how clusterIP service works:

image::https://user-images.githubusercontent.com/2038044/60740886-56cefe80-9f35-11e9-8b16-a61108660d6e.png[]

ClusterIP service is exposed on a `clusterIP` and a service port. when client
pods need to access the service it sends request toward this `clusterIP` and
service port. This model works great if all requests are coming from inside
of the same cluster. The nature of the ClusterIP limits the scope of this service
to be only within the cluster. overall by default the ClusterIP is not reachable
from external. 

==== create clusterIP service

let's create our first service in contrail environment, with service type
`clusterIP`. 

----
$ cat service-web-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
----

the yaml file looks pretty simple and self-explanatory. it defined a service
`service-web-clusterip` with the "service port" `8888`, mapping to `targetPort`
which means "container port" `80` in some pod. the `selector` indicates that
whichever pod with a label `app: webserver` will be choosen to be the backend
pod responding service request. 

now generate the service object by `apply` the yaml file:

----
$ kubectl apply -f service-web-clusterip.yaml
service/service-web-clusterip created
----

following kubectl commands are commonly used to quickly verify the service 
and backend pod objects.

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE    SELECTOR
service-web-clusterip  ClusterIP  10.101.150.135  <none>       8888/TCP  9m10s  app=webserver

$ kubectl get pod -o wide -l 'app=webserver'
No resources found.
----

the service is created successfully, there is no doubt about it. but there is no
pods for the service. the reason is there is no pod with the label matching to the
`selector` in the service. now we just need to create the pod with a proper
label.

we can define a pod directly, but given the benefits of RC and deployment over
pod as we've introduced earlier, use RC or deployment is more pratical. later on
you will understand this is the right choice. in our example we define a RC
object named `rc-webserver`.

----
$ cat rc-webserver.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver
  labels:
    app: webserver
spec:
  replicas: 1           #<---
  selector:
    app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver  #<---
    spec:
      containers:
      - name: webserver
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

the RC `rc-webserver` has a label `app: webserver`, matching the SELECTOR in
defined in our service. `replicas: 1` instruct RC controller to launch only 1
pod at the moment.

----
$ kubectl apply -f rc-webserver.yaml
replicationcontroller/rc-webserver created

$ kubectl get pod -o wide -l 'app=webserver'
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
rc-webserver-vl6zs  1/1    Running  0         24s  10.47.255.238  cent333  <none>
----

immediately the pod is choosen to be the backend. 
here are some brief summaries about the output:

* the service got a "ClusterIP" or "service IP" of `10.106.176.17` allocated
  from the service IP pool. 
* service port is `8888` as what is defined in yaml. 
* by default the protocol type is `TCP` if not declared in yaml file. you can
  use `protocol: UDP` to declare a UDP service.
* the backend pod can be located with the label selector

TIP: the example shown use a "equality-based" selector (`-l`) to locate the
backend pod, you can also use a "set-based" syntax to archive the same effect.
for example: `kubectl get pod -o wide -l 'app in (webserver)'`

==== verify cluserIP service

Now to verify if the service actually works, let's start another pod as a client
to initiate a http request toward the service. for this test we'll login to a
`cirros` pod and use `curl` command to send a http request toward the service.
you'll see the cirros pod being used as a client to send request throughout of
this book.

----
$ kubectl exec -it cirros -- curl 10.101.150.135:8888
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.238<br>Hostname =
    rc-webserver-vl6zs</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
----

the http request toward the service reaches a backend pod running the web server
application, which responds with a HTML page.

to better demonstrate which pod is providing the service, we are running a
customized pod image that runs a simple web server. the web server is configured
in such a way that whenever receiving a request, it will return a simple HTML
page with local pod IP and hostname embeded. This way the curl returns something
more meaningful in our test. 

the returned HTML looks relatively "OK" to read, but there is a way to make it
more "eye-friendly":

----
$ kubectl exec -it cirros -- curl 10.101.150.135:8888 | w3m -T text/html | head
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.238
                         Hostname = rc-webserver-vl6zs
----

the `w3m` tool is a "lightweight" console based web browser installed in the
host. with `w3m` we can render a html webpage into text, which is more readable
than the HTML page.

now we are convinced our service works. requests to service has been
redirected to the correct backend pod, with a pod IP `10.47.255.238`, pod name
`rc-webserver-vl6zs`.

if you want to have a specific 'clusterIP', you can mention it in the spec.
Ip address should be in service ip pool.

Sample yaml with specific 'clusterIP'

----
$ cat service-web-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  clusterIP: 10.101.150.150
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
----


=== NodePort 

NodePort service exposes a service on each node's ip at a static port. It maps
a static port on each node with a port of the application the POD as shown in 
the diagram 
 
image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/node%20port%20chapter%203.png[]
there is 2 very important parts in this services YAML file ports and selector.

targetPort is the actual port used by the application in here its port 80 as we
are planning to run a web server and nodeport is port on each node. 

selector is the label selector which determine which set of pods targeted by
this services, in here any POD with label app: FRONT-END will be serviced by
this services

    apiVersion: v1
    kind: Service
    metadata:
      name: web-app
    spec:
      selector:
        app: webserver
      type: NodePort
      ports:
      - targetPort: 80
        port: 80
        nodePort: 32001 <--(optional)

[NOTE]
====
* Kubernetes by default allocate node port from (30000-32767) range if it is not
  mentioned in the spec and the specified port should be in the configured range.
  it could be changed using the flag --service-node-port-range.
* The default service type is ClusterIP 
* Be aware with the change of the Node ip address as it could effect your services 
====

now let’s expose a nginx pod with the services shown above

[root@cent11]# cat nginx.yaml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: nginx-pod
      labels:
        app: webserver
    spec:
      containers:
        - name: nginx-c
          image: nginx

    [root@cent11]# kubectl create -f web-app.yaml
    service "web-app" created

    [root@cent11]# kubectl describe service web-app
    Name:                     web-app
    Namespace:                default
    Labels:                   <none>
    Annotations:              <none>
    Selector:                 app=webserver
    Type:                     NodePort
    IP:                       10.98.108.168
    Port:                     <unset>  80/TCP
    TargetPort:               80/TCP
    NodePort:                 <unset>  32001/TCP
    Endpoints:                10.47.255.252:80
    Session Affinity:         None
    External Traffic Policy:  Cluster
    Events:                   <none>

Now we can test that by send CURL -i which is a http request using the CLI toward the (any)node IP address

    [root@cent11 ~]# kubectl get pod -o wide
    
    NAME        READY   STATUS    RESTARTS   AGE   IP              NODE      NOMINATED NODE
    nginx-pod   1/1     Running   0          20m   10.47.255.252   cent222   <none>

    [root@cent11 ~]# kubectl describe node cent22 | grep InternalIP
  InternalIP:  10.85.188.17

    [root@cent11 ]#curl 10.85.188.17:32001
    <!DOCTYPE html>
    <html>
    <head>
    <title>Welcome to nginx!</title>
    <style>
        body {
            width: 35em;
            margin: 0 auto;
            font-family: Tahoma, Verdana, Arial, sans-serif;
        }
    </style>
    </head>
    <body>
    <h1>Welcome to nginx!</h1>
    <p>If you see this page, the nginx web server is successfully installed and
    working. Further configuration is required.</p>
    
    <p>For online documentation and support please refer to
    <a href="http://nginx.org/">nginx.org</a>.<br/>
    Commercial support is available at
    <a href="http://nginx.com/">nginx.com</a>.</p>

    <p><em>Thank you for using nginx.</em></p>
    </body>
    </html>

=== loadbalancer service

essentially, a loadBalancer service goes one more step beyond what the NodePort
service does. it exposes the Service externally using a cloud provider’s
loadbalancer. loadbalancer by its nature automatically includes all features and
functions of NodePort and ClusterIP Services. 

//the external load balancer routes the traffic 

Kubernetes clusters running on cloud providers support the automatic provision
of a load balancer. the only difference between the 3 type of services are the
`type` value. to reuse the same NodePort service yaml file and create a
loadbalancer service, just change the `type` to `LoadBalancer`:

----
type: LoadBalancer
----

the cloud will see this keyword and a load balancer will be created, with a
public IP serving as the frontend virtual IP. traffic coming to this virtual IP
will be redirect to the service backend pod. because the loadbalancer VIP is
publicly reachable, any client whoever has access to the VIP and port can access
the service provided by kubernetes cluster.

how is a loadbalancer implemented in loadbalancer service is "vendor-specific".
a GCE loadbalancer may work in a totally different way with a AWS loadbalancer.
we'll have a detail demonstration about how loadbalancer service works in
contrail kubernetes environment in chapter 4.

=== externalIP

Exposing service outside of the cluster can also be achieved via 'externalIP'
In the Service spec, externalIPs can be specified along with any of the
ServiceTypes. externalIPs are not managed by Kubernetes and are the responsibility
of the cluster administrator. 

=== kube-proxy

By default kubernetes uses kube-proxy for services. kube-proxy can be deployed in
user-space proxy-mode, iptables proxy-mode and ipvs. when the traffic hits the node,
it would be forwarded to one of the back end pod via a depolyed kube-proxy forwarding
plane. Also cni providers can have there own implementaions for services.


== Endpoints

=== Service with SELECTOT
in our 'service' introduction, there is one object that is involved but we
haven't explored is 'EP - endpoint'. we've learned it is through label selector
that a particular pod or group of pods with matching labels are choosen to be
the backend, so that the service request traffic will be redirected to them.
The IP and port information of the "matching" pods are maintained in the 'endpoint'
object.  The pods may die and spawn anytime, the "mortal" nature of the pod will
most possibly make the new pods be respawned with new IP address. during this dynamic
process the 'endpoints' will always be updated accordingly to reflect the current
backend pod IPs, so the service traffic redirection will act properly. 

here is an example to demonstrate some quick steps to verify the service,
corresponding endpoint and the pod with matching labels

create a service:

----
$ cat svc/service-web-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
----

list the endpoint:

----
$ kubectl get ep
NAME             ENDPOINTS          AGE
service-web-lb   10.47.255.252:80   5d17h
----

locate pod with the label that is used by selector in service:

----
$ kubectl get pod -o wide -l 'app=webserver'
NAME                READY  STATUS   RESTARTS  AGE    IP             NODE     NOMINATED NODE LABELS
rc-webserver-rjlgr  1/1    Running  4         5d17h  10.47.255.252  cent333  <none>         app=webserver
----

scale the backend pods

----
$ kubectl scale rc webserver --replicas=3
----

----
$ kubectl get pod -o wide -l 'app=webserver'
NAME                READY  STATUS   RESTARTS  AGE    IP             NODE     NOMINATED NODE LABELS
rc-webserver-rjlgr  1/1    Running  4         5d17h  10.47.255.252  cent333  <none>         app=webserver
rc-webserver-45skv  1/1    Running  0         5s     10.47.255.251  cent222  <none>         app=webserver
rc-webserver-m2cp5  1/1    Running  0         5s     10.47.255.250  cent333  <none>         app=webserver
----

----
$ kubectl get ep
NAME             ENDPOINTS                                            AGE
service-web-lb   10.47.255.250:80,10.47.255.251:80,10.47.255.252:80   5d17h
----

=== Service without SELECTOR

in the preceding example, the `Endpoints` object is generated automatically by
the kubernetes system whenever a service is created, and at least one pod with
matching label exists. Another use case of endpoint, is for a service that has
no label selector defined. in that case you can manually map the service to the
network address and port where it's running, by adding an endpoint object
manually and you can connect the endpoint with the service. this can be very
useful in some scenarios. for example, in your setup you have a backend web
server running in a physical server, you still want to integrate it into a
kubernetes `Service`.  you just create the service as usual, and then create an
endpoint with an "address" and "port" pointing to the web server. that's it! the
`Service` does not care about the backend type, it just redirect the service
request traffic exactly the same way as if all backend is pod.

== Ingress 

You’ve now seen two ways of exposing a service to clients outside the cluster:
`NodePort` service and `LoadBalancer service`. another method is `Ingress`

=== Ingress object

before we talk about it, the best way to get a feel of Ingress object is to look
at the yaml definition:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-simple-fanout
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: service-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: service-2
          servicePort: 9999
----

it looks pretty simple. the `spec` defines only one item that is the `rules`.
the rules says a `host`, which is "juniper" URL here, may have 2 possible `path`
in the URL string, after the `host` part of it: `/dev` and `/qa`. each path is
then associated to a different service. that's it.

if you now think kubernetes Ingress is nothing but to define some rules, and the
rules are just to instruct the system to direct incoming request to different
services, based on the URLs, you are basically right, but in the high level
only.

in practice there are other things you need to understand.

Ingress is another "core" concept of Kubernetes, it allows simple, rule-based
HTTP routing that does not exist in service. however, in reality to "implement"
these rules, you need at least another component called `ingress controller`


=== ingress controller

ingress is tyically implemented by a third party proxy. These implementations
are known as `ingress controllers`. An ingress controller is responsible for
reading the Ingress rules and program the rules into the proxy which does the
real work - dispatching traffic based on `host` / URL.

Different Kubernetes environments have different implementations of the
controller, and each ingress controllers have their own implementations to
program the ingress rules. bottom line is, there has to be an Ingress controller
running in the cluster. 

=== ingress vs service

there are similiarities between loadbalancer service and ingress. both can
expose service to outside of the cluster. but there are some main differences.

`Ingress` operates at the application layer of the OSI network model, while
`service` operates at layer 4 only. `Ingress` understand the HTTP/HTTPS protocol,
service only does forwarding based on IP and port, which means it does not care
about the protocol details.

each LoadBalancer service requires its own public IP address, whereas an Ingress
only requires one. When a client sends an HTTP request to the Ingress, the host
and path in the request determine which service the request is forwarded to.
even when providing access to multiple URLs by multiple services, overall
ingress only requires one public IP.

=== ingress examples

there are basically 3 types of ingresses:

* Single Service Ingress
* Simple fanout Ingress
* Name based virtual hosting Ingress

we'll look at each ingress's yaml file examples. 

==== single service ingress

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-single-service
spec:
  backend:
    serviceName: webservice
    servicePort: 80
----

this is the simplest form of ingress. the ingress will get an external IP so the
service will be exposed to the public, however, since there is no `host` defined
so it does not differentiate different URLs. all requests will goes to one same
service.

==== simple fanout ingress

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-simple-fanout
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 80
      - path: /qa 
        backend:
          serviceName: webservice-2
          servicePort: 80
----

comparing with `single service` ingress, `simple fanout` ingress is more
practical. we've checked this out in the beginning of this section. after the
ingress is created it will be allocated an external IP and do URL routing, based
on the path.

==== virtual host ingress

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-virutal-host
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - backend:
          serviceName: webservice-1
          servicePort: 80
  - host: www.cisco.com
    http:
      paths:
      - backend:
          serviceName: webservice-2
          servicePort: 80
----

`name based virtual host` supports routing HTTP traffic to multiple host names
at the same IP address. the example above may not be practical (unless one day
the two domains merge!) but it is good enough to showcase the idea. in the yaml
file 2 `host` are defined, which are the "juniper" and "cisco" URL. but remember
we said the ingress will be allocated with one public IP, so request toward that
one IP, will be routed, based on the `host` in URL, to different backend
services. we'll have a very detail case study in chapter 4 about this example.

== Network policy

In Kubernetes pods can reach any pods by default. Then how pods can be secured?
The answer is network policy. `Networkpolicy` is a Kubernetes resource like pod,
service, ingress and etc. It defines who are all can reach the pod(ingress) and
whom the pod can reach(egress). 

=== Prerequisites

Network polices are implemented by the network plugin, so you must be using a
network solution which supports Network Policy. Simply creating the resource
without a controller to implement it will have no effect.

Network policy logically can be divided into two sections. The first section
will identify the pod(s) where the Network policy would be applied. The second
section will define the ingress and egress rules for the selected pod(s). 

=== Pod(s) selection

How the pods would be selected? Yes. You are right. Pod(s) are identified using
labels.

    podSelector:
       matchLabels:
         role: db

In the above example the network policy would be applied to the pods which has
the label "role: db".

=== Ingress and egress rules for group of pod(s)

The second section defines the policy types for the selected pod(s). Policy type
can be `ingress` or `egress` or both. `Ingress` is the default policy type.
policy identifies the network endpoint where the selected pod(s) can
communicate. Network endpoint can be ip address block or pod(s) (all pods or
group of pods) in a namespace or selected pods in the same namespace.  Ingress
network-endpoint has to be defined in the "from" section. Egress
network-endpoint has to be defined in the "to" section. 

----
policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
----
	
In the above example:

. The ingress network points are 
.. 172.17.0.0/16 and port except 172.17.1.0/24 
.. All the pods in namespaces which has the label “project: myproject”. 
.. Pods which has the label "role: frontend"

. The egress network points are 10.0.0.0/24

Is there any way to select few pods from namespaces instead of all pods in the
namespaces? Yes. It can be specified in the namespaceSelector.
`namespaceSelector` can have podSelector. When `namespaceSelector` has
`podSelector`, network endpoint would be pods with matching labels in the selected
namespaces.

The below example shows that allowing connections from pods with label
`role=client` in namespaces with the label `user=alice`. Please be aware to use
correct yaml syntax.

  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...

So far it is fine. Still there is a security concern. Is there any way to
specify ports for ingress and egress? Yes. As part of the policy it can be
mentioned. If it is not mentioned it applies to all ports. Ports in ingress says
that selected pod(s) can allow traffic for the specified ports. Ports in egress
says that selected pod(s) can send traffic to specified ports.

Previous example along with port specifications

----
policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
----

The above network policy says that all ingress network endpoint can reach
selected pod(s) tcp port 6379 and selected pod(s) can reach all egress network
endpoint's tcp port 5978.
The rest of the traffic would be blocked.

Sample network-policy

----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: mydb
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
----

----
kubectl create -f mydb-netpol.yaml

kubectl get netpol
NAME   POD-SELECTOR   AGE
mydb    role=db        	     3m5s

kubectl describe netpol mydb
Name:         mydb
Namespace:    default
Created on:   2019-06-30 07:41:18 -0700 PDT
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     role=db
  Allowing ingress traffic:
    To Port: 6379/TCP
    From:
      IPBlock:
        CIDR: 172.17.0.0/16
        Except: 172.17.1.0/24
    From:
      NamespaceSelector: project=myproject
    From:
      PodSelector: role=frontend
  Allowing egress traffic:
    To Port: 5978/TCP
    To:
      IPBlock:
        CIDR: 10.0.0.0/24
        Except:
  Policy Types: Ingress, Egress
----


== livenessProbe and readinessProbe

=== Liveness Probe

What happen if the application in the POD is running but it can’t serve its main
purpose for whatever reason? also applications that runs for long time might
transition to broken states. In all cases the last thing you want have is a call
reporting a problem in an application that could be easily fixed with restarting
the POD. liveness probes is a Kubernetes features made specially for that.
liveness probes sent a pre-defined request to the POD on a regular basis then
restart the POD if this request failed. The most commonly used liveness probe is
HTTP GET request, but it could also be opening TCP socket or issuing a command 

this is a TCP socket probe example where the “initialDelaySeconds” is the
waiting time before the first try to open a TCP socket to port 80 then it will
run the probe every 20 second as specified in “periodSeconds” If that failed the
POD would be restarted automatically

----
apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
  labels:
    app: tcpsocket-test
spec:
  containers:
    - name: liveness-pod
      image: virtualhops/ato-ubuntu:latest
      ports:
        containerPort: 80
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN
    livenessProbe:
      tcpSocket:
        port: 80
      initialDelaySeconds: 15
      periodSeconds: 20
----
 
HTTP GET request probe is similar to the TCP socket probes, but it will sent
HTTP GET request, and you have the option to specify the path which in here just
the main website. also you can send the probe with customized header 

----
apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
  labels:
    app: tcpsocket-test
spec:
  containers:
    - name: liveness-pod
      image: virtualhops/ato-ubuntu:latest
      ports:
       - containerPort: 80
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN
      livenessProbe:
        httpGet:
           path: /
           port: 80
           httpHeaders:
           - name: some-header
             value: Running
        initialDelaySeconds: 15
        periodSeconds: 20
----

let's launch this POD then login to it to terminate the proccess that handel the
httpGet 

----
[root@cent11 ~]# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
liveness-pod   1/1     Running   0          114s


[root@cent11 ~]# kubectl exec -it liveness-pod bash
root@liveness-pod:/# sudo netstat -tulpn

Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      111/apache2     
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      45/sshd         
tcp6       0      0 :::22                   :::*                    LISTEN      45/sshd         

root@liveness-pod:/# service apache2 stop
 * Stopping web server apache2                                                   * 

root@liveness-pod:/# sudo netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      45/sshd         
tcp6       0      0 :::22                   :::*                    LISTEN      45/sshd         

[root@cent11 ~]# kubectl get pod
NAME           READY   STATUS    RESTARTS   AGE
liveness-pod   1/1     Running   1          5m33s
----

you can see the POD got restarted automatically and in the event it stated the
reason for that restart :

    Killing container with id docker://liveness-pod:Container failed liveness probe.. Container will be killed and recreated. 

----
[root@cent11 ~]# kubectl describe pod liveness-pod
Name:               liveness-pod
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Fri, 05 Jul 2019 16:39:12 -0400
Labels:             app=tcpsocket-test
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.47.255.249",
                              "mac": "02:c2:59:4a:82:9f",
                              "name": "cluster-wide-default"
                          }
                      ]
Status:             Running
IP:                 10.47.255.249
Containers:
  liveness-pod:
    Container ID:   docker://01969f51d32f38a15baab18487b85c54cee4125f55c8c7667236722084e4df06
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 05 Jul 2019 16:41:35 -0400
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Fri, 05 Jul 2019 16:39:20 -0400
      Finished:     Fri, 05 Jul 2019 16:41:34 -0400
    Ready:          True
    Restart Count:  1
    Liveness:       http-get http://:80/ delay=15s timeout=1s period=20s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-m75c5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-m75c5:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-m75c5
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  7m19s                  default-scheduler  Successfully assigned default/liveness-pod to cent22
  Warning  Unhealthy  4m6s (x3 over 4m46s)   kubelet, cent22    Liveness probe failed: Get http://10.47.255.249:80/: dial tcp 10.47.255.249:80: connect: connection refused
  Normal   Pulling    3m36s (x2 over 5m53s)  kubelet, cent22    pulling image "virtualhops/ato-ubuntu:latest"
  Normal   Killing    3m36s                  kubelet, cent22    Killing container with id docker://liveness-pod:Container failed liveness probe.. Container will be killed and recreated.
  Normal   Pulled     3m35s (x2 over 5m50s)  kubelet, cent22    Successfully pulled image "virtualhops/ato-ubuntu:latest"
  Normal   Created    3m35s (x2 over 5m50s)  kubelet, cent22    Created container
  Normal   Started    3m35s (x2 over 5m50s)  kubelet, cent22    Started container
----

=== Readiness Probe

Liveness probe make sure that your POD is in good health, but for some
application Liveness alone isn’t enough. some application need to load large
files before it start. you might think if we set a higher “initialDelaySeconds”
value then problem solve. but this not an efficient way. Readiness probe is
solution in here specially with Kubernetes services, as the POD will not receive
a traffic until it report ready. Readiness Probe is configured the same way as
liveness prob 

----
apiVersion: v1
kind: Pod
metadata:
  name: liveness-pod
  labels:
    app: tcpsocket-test
spec:
  containers:
    - name: liveness-pod
      image: virtualhops/ato-ubuntu:latest
      ports:
        containerPort: 80
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN
    livenessProbe:
      tcpSocket:
        port: 80
      initialDelaySeconds: 15
      periodSeconds: 20
    readinessProbe:
      tcpSocket:
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 10
----

NOTE: its recommended to use both Readiness Probe and Liveness Probe where
Liveness probe restart the POD if it failed and Readiness Probe make sure the
POD is ready before it gets the traffic 

== Annotation 

We have seen before how labels in Kubernetes are used for identifying, selecting
and organizing objects, labels are just one way to attach metadata to Kubernetes
objects.

Another way is Annotations which is a key/value maps that attach non-identifying
metadata to objects, Annotation has a lot of use cases such as attaching

- pointers for logging and analytics
- phone number, directory entries and web site 
- timestamps, image hashes and registry address 
- network, namespaces 

In this book we will focus on using Annotations to assign network information to
POD and we will see later on in chapter 4 how Kubernetes annotation can instruct
contrail to attach an interface to certain network 

Before seeing Annotations in action lets first create a network with minimum
configuration based on the De-facto Kubernetes Network custom resource
definition.  Network Attachment Definition is used to indicate the CNI as well
the paraments of the network where we will attached interface POD to

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: net-a
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "awesome-plugin"
  }'
----

The type in the example “awesome-plugin” is the name of the CNI which and could
be Flannel, Calico, Contrail-K8s-cni , …,etc 

Creating a POD and using annotations to attach its interface to a network called
net-a

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: net-a
----

Note: According to De-facto Kubernetes Network custom resource definition 
the annotation "k8s.v1.cni.cncf.io/networks” is used to represent “
NetworkAttachmentDefinition” and has two format

----
Network  
   k8s.v1.cni.cncf.io/networks: net-a
----

----
Namespace/network name
   k8s.v1.cni.cncf.io/networks: ns/net-a
----

NOTE: To maintain compatibility with existing Kubernetes deployments, All pods
must still be attached to the cluster-wide default network. which means even if
we attached one POD interface to a specific network, this POD would have two
interfaces one attached to the cluster-wide default network and the other
interface is attached to the network specified in the annotation argument (net-a
in this case) 

