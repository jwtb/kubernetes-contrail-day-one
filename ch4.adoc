// vim:set ft=asciidoc cc=80 tw=80: GistID: 1dd268ba5900111d035111ab77201a5a
= Chapter 4: Kubernetes and Contrail Integration 
:toc: right
:toclevels: 3
//:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline
:numbered:
:imagesdir: diagrams
ifdef::env-github[]
//:imagesdir: https://gist.githubusercontent.com/pinggit/ba7b7e6314cff5176d668509b516d50c/raw/e8351c10e3209c7a612eae3609e4e65d45dbdd59/
//:imagesdir: https://gist.githubusercontent.com/pinggit/ba7b7e6314cff5176d668509b516d50c/raw/83318e78c1785b7fa39046c2d818b6f3387cc1cd/
:imagesdir: https://gist.githubusercontent.com/pinggit/ba7b7e6314cff5176d668509b516d50c/raw/c5262793f12f21ef8ccb8010c618004c6cf1dd77/
endif::[]

This chapter expalins contrail role in kubernetes world. we'll start with a
section about contrail kubernetes integration architecture, where you will learn
how kubernetes objects are handled in contrail. those objects are NS, pod,
service, ingress, network policy and etc. then we'll look into implementation of
each of them in detail. we also introduce some contrail objects whenever needed.
multiple interface pod is a highlight of contrail's advantages as one of
kubernetes network CNI over other implementations, so we cover that also. in the
end, we will demonstate service chain using Juniper CSRX container.

== Contrail-Kubernetes Architecture 
=== Why Contrail with Kubernetes ?

Now after we have seen the main concepts of Kubernetes in chapter 2 and 3, what
could be the gain in adding Contrail to standard Kubernetes deployment ?

in brief, Contrail offers common deployment for multiple environments
(OpenStack, Kubernetes, etc) as well it enriches Kubernetes networking and
security capabilities.

When it comes to deployment for multiple environments, Yes containers is the
current trend to build applications, but don’t expect everyone to migrate
everything from VM to containers that fast (This is not to mention the nested
approach where containers are hosted in VM). if we add to the picture
workload fully or partially run in the public cloud, we end up feeling the
misery for network and security administrators where Kubernetes becomes just
one thing to manage Network and security. 

administrator in many organization manage individual orchestrator/manager for
each environment. OpenStack or VMware NSX for VM, Kubernetes or Mesos for
Containers, AWS console.  and here what contrail could put the network and
security administrators out of their misery is it provides dynamic end-to-end
networking policy and control for any cloud, any workload, and any deployment.

from a single user interface contrail translates abstract workflows into
specific policies, simplifying the orchestration of virtual overlay connectivity
across all environments by building and securing virtual networks that connect
BMS, VM and Containers located in private or public cloud. 

A very common way to deploy Kubernetes is to launch its POD in VMs orchestrated
by OpenStack. this is one of the many use cases of contrail doing its magic.

In this book we won’t cover contrail integration with other environments as we
focus only in Kubernetes. But any feature that we explain in here could be
extended for other environments.

what we mean by contrail enriching standard Kubernetes deployment?

Eventhough kubernetes does not provide the networking, it imposes fundamental
requirements on the networking implementation and it is taken care by all CNI
("Container Network Interface") providers. contrail is one of the cni providers. you
can refer to https://kubernetes.io/docs/concepts/cluster-administration/networking/
for more available CNI providers.

kubernets has defined some fundamental requirements in networking implementation:

. pods on a node can communicate with all pods on all nodes without NAT
. agents on a node (e.g. system daemons, kubelet) can communicate with all
  pods on that node
. pods in the host network of a node can communicate with all pods on all
  nodes without NAT
  
with these requirements to all CNI plugins implementations, Kubernetes offers
flat network connectivity with some security feature confined in a cluster, but
Contrail could offer on top of that:

. namespaces and services customized isolations for segmentations and
  multi-tenancy
. distributed loadbalancing and firewall with extensive centralized flow and
  logs insight
. rich security policy using tags that can extend to other environment
  (OpenStack, VMWare, BMS, AWS ,..,etc)
. service chaining

In this chapter we will cover some of these aspects, but first let’s talk about
Kubernetes/contrail architecture and the object mapping 

=== Contrail-Kube-Manager

A new components of contrail has been added called `contrail-kube-manager`,
abbreviated as `KM`. what it does basically is to watch kubernetes apiserver for
interested kubernetes resources, and translates into Contrail controller object.
the following figure illustratesthe basic work flow:

.contrail kubernetes architecture

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/kubemanager.png[]
//image::https://raw.githubusercontent.com/pinggit/kubernetes-contrail-day-one/master/diagrams/kubemanager.png?token=AAPRSHE5SF522ETPA6NAUDK5D7PHS[]
//image::https://github.com/aymanaborabh/kubernetes-contrail-day-one/blob/master/diagrams/kube-manager-chapter%204.png[]
image::https://user-images.githubusercontent.com/2038044/63705791-e9846f00-c7fb-11e9-8ba7-0638aee5d16f.png[]

////
ping: 

=== contrail-kube-manager

.contrail

image::https://user-images.githubusercontent.com/2038044/59642949-fb2f0380-9134-11e9-86d2-1035e5b901b7.png[]

.kubernetes
image::https://user-images.githubusercontent.com/2038044/59642835-94a9e580-9134-11e9-9053-80505cb1ba75.png[]

.contrail kubernetes
image::https://user-images.githubusercontent.com/2038044/59642699-1a796100-9134-11e9-8a58-fb529b329cba.png[]

////

=== Kubernetes to Contrail Object Mapping

So not much of change of the regular contrail that we have seen before and all
of that is happening behind the scene.
what we have to be aware of it before dealing with Kubernetes/contrail is the
object mapping. because contrail is single interface managing multiple
environments - as explained before – each environment has its own acronym and
terms hence the need for this mapping, which will be done by a plugin. in
kubernetes `contrail-kube-manager` does this. 

NOTE: contrail has specific plugins for each environments/orchestrator.
 
For example, Namespace in Kubernetes are intended for segmentation between
multiple teams, or projects as if we are creating virtual cluster. In contrail
the similar concept would be named as project so when you create a namespace in
Kubernetes it will automatically create an equivalent project in contrail. more
on that will come later on for now kindly make yourself familiar with this list
of object mapping 

.contrail kubernetes object mapping

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/chapter%204%20contrail%20-%20k8s%20mapping.png[]
//image::https://user-images.githubusercontent.com/2038044/60748774-6bc08780-9f5f-11e9-91ae-2ec496cab987.png[]
image::https://user-images.githubusercontent.com/2038044/63705887-1cc6fe00-c7fc-11e9-8c4f-733676cf663a.png[]

== Contrail Lab environment

=== Contrail Setup

before starting our investigation, let's look at our setup. in this book we
build a setup including the following devices, most of our case studies are
based on it:

* one cenos server running as k8s `master` and contrail controllers
* two cenos servers, each running as a k8s `node` and contrail vrouter
* one Juniper QFX switch running as the underlay "leaf"
* one Juniper MX router running as a gateway router, or a "spine"
* one centos server runs as an Internet host machine

the digaram is here:

//image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]
//image::https://user-images.githubusercontent.com/2038044/63596670-bcc92100-c589-11e9-99f1-7340a24cc8fd.png[]
image::testbed-fabric.drawio.png[]

TIP: To minimize the resource utilization, all "servers" are actually centos
virtual machines created by vmware ESXI hypervisor running in one physical HP
server.

in appendix you will find all details about the setup. the prerequisites,
software/hardware specifications, sample configuration files, and installation
steps. following the steps you will be able to build a same setup in your lab.

=== Contrail Command

Before getting into deeper, we just want to introduce briefly contrail-command(CC) which
is the new user interface (UI) available from contrail 5.0.1. throughout this book
we use both CC and old UI to demonstrate most of lab studies. just keep in mind that
in the future CC will be the only UI and the "legacy" one will be deprecated. 

detail information about CC is available in Juniper documentation website so we
won't elaborate it here.  to access CC use this URL in web browser:
https://Contrail-Command-Server-IP-Address:9091 the "Contrail-Command-Server"
can be the same or different server as kubernetes master or contrail controller
node. in our setup we've installed them in same server.

in CC, the functions and settings are groups in a a "main menu". it is also the
entry point from where you can navigate through different functions. 

.contrail command main menu
image::https://user-images.githubusercontent.com/2038044/60282872-ed684380-98d5-11e9-92f7-e1df07c5fecf.png[]

in order to get this menu, click on group name right next to the "contrail command"
logo on the upper left of the UI. in the above screen capture that group is
"Infrastructure", but regardless it can be any group, just click it and you will get
the main menu, then from there you can select and jump into all other settings.

Again our focus is not CC. we are trying to give base insight about CC which would be
helpful for our primary goal of the book.

== Contrail Namespaces and Isolation

In chapter3 you`ve read about `namespace` or `NS` in kubernetes. in the
beginning of this chapter we've mentioned object mappings between kubernetes and
contrail. in this section we'll see how NS works in contrail environments and
how contrail extends the feature.

one analogy we`ve given when introducing `namespace` concept is openstack
`project`, or `tenant`. that is exactly how contrail is looking at it. whenever
a new `namespace` object is created, `contrail-kube-manager` (KM) gets noticed
about the object creation event and it will create the corresponding `project`
in contrail. 

To differentiate between multiple kubernetes clusters in contrail,
a kubernetes cluster name will be added to the kubernetes NS or project name.
the default kubernetes cluster name is `k8s`.  so if you create a kubernetes NS
`ns-user-1`, `k8s-ns-user-1` project will be created in contrail and you can see
the same in the contrail GUI.

.contrail command: projects
image::https://user-images.githubusercontent.com/2038044/60316467-8fb91300-9938-11e9-9de6-429b56429868.png[]

****
the kubernetes `cluster name` is configurable, during deployment process only.
if you don't configure it `k8s` will be the default. once the cluster is
created, the name can not be changed anymore. to view the `cluster name`, go to
`contrail-kube-manager` (KM) docker and check its the configuration file.

.to locate the `KM` docker container
----
$ docker ps -a | grep  kubemanager
2260c7845964  ...snipped...  ago  Up  2  minutes  kubemanager_kubemanager_1
----

.to login to the `KM` container
----
$ docker exec -it kubemanager_kubemanager_1 bash
----

.find the `cluster_name` option
----
$ grep cluster /etc/contrail/contrail-kubernetes.conf
cluster_name=k8s        #<---
cluster_project={}
cluster_network={}
----

****

NOTE: in the rest part of this book we will refer all these terms `namespace`,
`NS`, `tenant`, `project` interchangeably.

=== Non-Isolated NS

you are aware that kubernetes basic networking requirement is a "flat"/"NATless"
network - any pod can talk to any pod in any namespace, any cni providers should
ensure that. consequently in kubernetes by default all namespaces are **not**
isolated.

NOTE: the term "isolated" and "non-isolated" are in the context of (contrail)
networking only. 

.k8s-default-pod-network and k8s-default-service-network

To provide networking for all non-isolated namespace, there should be a
**common** VRF (virtual routing and forwarding table) or RI (routing instance).
in contrail kubernetes environment, two "default" VNs are pre-configured in k8s
default NS, for pod and service respectively. correspondingly there are
2 VRFs each with same name as their correspondingly VN. 

the name of the two VNs/VRFs are in this format:

    <k8s-cluster-name>-<namespace name>-[pod|service]-network

so for `default` NS with a default cluster name `k8s`, the two VN/VRF names will
become:

* `k8s-default-pod-network`: pod VN/VRF, with the default subnet `10.32.0.0/12`
* `k8s-default-service-network`: service VN/VRF, with a default subnet `10.96.0.0/12`

NOTE: the default subnet for pod or service is configurable.

it is important to know that these 2 default VNs are **shared** between all of
the "non-isolated" namespaces. what that means is, they will be available for
any new non-isolated NS that you create, implicitly.  that is why pods from
all non-isolated NS including default NS can talk to each other.

on the other hand, any VNs that you create will be isolated with other VNs,
regardless of same or different NS. communication between pods in two different
VNs requires contrail network policy.

NOTE: later when you read about kubernetes `service`, you may wonder why packets
destined service VN/VRF can reach the backend pod in pod VN/VRF? the answer is
also contrail network policy. by default contrail network policy is enabled
between service and pod network which allows packets arriving service VN/VRF to
reach the pod, and vice versa. 

for the isolated NS, however, it will be a different story.

=== Isolated NS 

in contrast, "isolated" namespace, will have its own default pod-network and
service-network, accordingly two new VRFs are also created for each "isolated"
namspace. The same flat-subnets `10.32.0.0/12` and `10.96.0.0/12` are shared by
the pod and service networks in the isolated namespaces. however since the
networks are with a different VRF, by default it is isolated with other NS.
pods launched in isolated NS can only talk to service and pods on the same
namespace. Additional configurations, e.g. policy, is required to make the pod
being able to reach the network outside of current namespace.

to illustrate this concept let's take an example. suppose you have 3 namespaces,
the `default` NS and two user NS: `ns-non-isolated` and `ns-isolated`.
in each NS you create one user VN: `vn-left-1`. you will end up to have
following VN/VRFs in contrail:

.NS default

* default-domain:k8s-default:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network

.NS ns-non-isolated

* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network

.NS ns-isolated

* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network

NOTE: The above name is mentioned in the FQDN format. In contrail domain is the
top-level object, followed by project/tenant and followed by virtual-networks.

////
* default-domain:k8s-default:k8s-default-pod-network:k8s-default-pod-network
* default-domain:k8s-default:k8s-default-service-network:k8s-default-service-network
* default-domain:k8s-default:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-pod-network:k8s-ns-isolated-pod-network
* default-domain:k8s-ns-isolated:k8s-ns-isolated-service-network:k8s-ns-isolated-service-network
* default-domain:k8s-ns-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
////

this can be illustrated in below diagram:

.NS and VN
//image::https://user-images.githubusercontent.com/2038044/63223271-13e18700-c181-11e9-8fe4-987cf935a05b.png[]
//image::https://user-images.githubusercontent.com/2038044/64745160-bbc64800-d4d4-11e9-9360-cc0811f99441.png[]
image::ns-isloation.drawio.png[]

here is the yaml file to create an isolated namespace:

----
# ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation" : "true"
  name: ns-isolated
----

to create the NS:

----
kubectl create -f ns-isolated.yaml

$ kubectl get ns
NAME          STATUS    AGE
contrail      Active    8d
default       Active    8d
ns-isolated   Active    1d  #<---
kube-public   Active    8d
kube-system   Active    8d
----

the annotations under metadata are something additional comparing to standard
(non-isolated) k8s namespace, the value of `true` indicates this is an isolated
NS:

  annotations:
    "opencontrail.org/isolation" : "true"

this part of the definition is Juniper's extension. `contrail-kube-manager`
(`KM`) , reads the namespace `metadata` from `kube-apiserver`, parses the
information defined in the `annotations` object, and sees that the `isolation`
flag is set to `true`. it then creates the tenant with the correponding routing
instances(one for pod and one for service) instead of using the default ns
routing instances for the isolated namespace. fundamentally that is how the
"isolation" is implemented. 

in the following sections we'll verify how the routing isolation works.

=== Pods Communication across NS

create a non-isolated namespace and an isolated namespace:

----
#ns-non-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-non-isolated

#ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation": "true"
  name: ns-isolated

$ kubectl apply -f ns-non-isolated.yaml
namespace/ns-non-isolated created

$ kubectl apply -f ns-isolated.yaml
namespace/ns-isolated created

$ kubectl get ns | grep isolate
ns-isolated       Active   79s
ns-non-isolated   Active   73s
----

in both NS and the default NS, create a webserver deployment to launch a
webserver pod:

////
----
#deployment-cirros.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: cirros
  name: cirros1
spec:
  replicas: 1
  selector:
    matchLabels:
      run: cirros
  template:
    metadata:
      labels:
        run: cirros
    spec:
      containers:
      - image: cirros
        imagePullPolicy: Always
        name: cirros
      restartPolicy: Always
----
////

----
#deploy-webserver-do.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver
  labels:
    app: webserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver
    matchExpressions:
      - {key: app, operator: In, values: [webserver]}
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

----
$ kubectl apply -f deploy-webserver-do.yaml -n default
deployment.extensions/webserver created

$ kubectl apply -f deploy-webserver-do.yaml -n ns-non-isolated
deployment.extensions/webserver created

$ kubectl apply -f deploy-webserver-do.yaml -n ns-isolated
deployment.extensions/webserver created

$ kubectl get pod -o wide -n default
NAME                        READY  STATUS   ... IP             NODE     ...
webserver-85fc7dd848-tjfn6  1/1    Running  ... 10.47.255.242  cent333  ...

$ kubectl get pod -o wide -n ns-non-isolated... 
NAME                        READY  STATUS   ... IP             NODE     ...
webserver-85fc7dd848-nrxq6  1/1    Running  ... 10.47.255.248  cent222  ...

$ kubectl get pod -o wide -n ns-isolated
NAME                        READY  STATUS   ... IP             NODE     ...
webserver-85fc7dd848-6l7j2  1/1    Running  ... 10.47.255.239  cent222  ...
----

ping between all pods in 3 namespaces

----
#default ns to non-isolated new ns: succeed
$ kubectl -n default exec -it webserver-85fc7dd848-tjfn6 -- ping 10.47.255.248
PING 10.47.255.248 (10.47.255.248): 56 data bytes
64 bytes from 10.47.255.248: seq=0 ttl=63 time=1.600 ms
^C
--- 10.47.255.248 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.600/1.600/1.600 ms

#default ns to isolated new ns: fail
$ kubectl -n default exec -it webserver-85fc7dd848-tjfn6 -- ping 10.47.255.239
PING 10.47.255.239 (10.47.255.239): 56 data bytes
^C
--- 10.47.255.239 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
----

the test result shows that, bidirectional communication between two non-isolated
namespaces (namespace `ns-non-isolated` and `default` in this case) works, but
traffic from non-isolated NS (`default` NS) toward isolated NS does not pass
through. what about traffic within the same isolated NS? 

with the power of the `deployment` we can quickly test it out: in isolated NS
`ns-isolated`, clone one more pod by `scale` the deployment with `replicas=2`
and ping between the 2 pods:

----
$ kubectl scale deployment webserver --replicas=2
$ kubectl get pod -o wide -n ns-isolated
NAME                        READY  STATUS   RESTARTS  AGE  IP             NODE   
webserver-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222
webserver-85fc7dd848-215k8  1/1    Running  0         8s   10.47.255.238  cent333

$ kubectl -n ns-isolated exec -it webserver-85fc7dd848-6l7j2 -- ping 10.47.255.238
PING 10.47.255.238 (10.47.255.238): 56 data bytes
64 bytes from 10.47.255.238: seq=0 ttl=63 time=1.470 ms
^C
--- 10.47.255.238 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.470/1.470/1.470 ms
----

the ping packet passes through now. to summarize the test results: 

* traffic is not isolated between non-isolated NS
* traffic is isolated between an isolated NS and all other tenant in the cluster
* traffic is not isolated in same NS 

NOTE: pod-level isolation can be achieved via kubernetes network policy, or
security groups in contrail. 
this will be covered later in this chapter.

== Contrail Floating IP

//(with type of loadBalancer or nodePort) 

=== Overlay Internet Access

we've discussed and tested the communication between pods in the same or
different NS. so far we've only tested it **inside** of the same cluster. what
about communication with devices **outside** of the cluster? you may already
know that in traditional (openstack) contrail environment, there are many ways
for the overlay entities (typically a VM) to access the Internet, the 3
frequently used methods among them are:

* floating IP
* fabric SNAT
* logical router

the preferred kubernetes solution to expose any service is via `service` 
and `Ingress` objects which you've read about and got the idea in chapter 3.
in contrail kubernetes environment, floating IP is used in the service and Ingress
implementation to expose them to outside of the cluster. later in this chapter
we'll have a very detail discussion for each of these two objects. befor that,
in this section, we'll review the "floating IP" basis and look at how it works
with kubernetes.

NOTE: `fabric SNAT` and `logical router` are used by overlay workloads(VM and
POD) to reach the internet and the reverse direction is not possible. `floating
IP` however, supports both direction - you can configure it to support ingress
traffic, egress traffic, or both and default is bi-direction. in this book we
focus on `floating IP` only. you can refer contrail documents for detail
information about fabric SNAT and logical router.

=== Floating IP and FIP Pool

`floating IP`, or `FIP` for short, is a "traditional" concept that contrail
supports since very early releases. Essentially it is an openstack concept to
"map" a VM IP, which is typically a private IP address, to a public IP (the
"floating IP" in this context) that is reachable from the outside of the
cluster. Internally the one to one mapping is implemented by NAT. whenever a
vrouter receives packets from outside of the cluster destined to the floating
IP, it will translate it to the VM's private IP and forward the packet to the
VM. similarly it will do the translation on reverse direction. Eventually both
VM and Internet host can talk to each other, and both can initiate the
communication.

NOTE: vrouter is a contrail forwarding plane resides in each compute node handles
workloads traffic

the figure below illustrates the basic work flow of FIP:

.Floating IP
//image::https://user-images.githubusercontent.com/2038044/60388331-be8cd180-9a7d-11e9-8ff7-c202ed9f7349.png[]
//image::https://user-images.githubusercontent.com/2038044/60556767-b8faea00-9d10-11e9-84bb-0e40e3edcc3d.png[]
//image::https://user-images.githubusercontent.com/2038044/60357106-b448d580-99a0-11e9-8ad2-31e15102b6bd.png[]
//image::https://user-images.githubusercontent.com/2038044/63227026-0ee7fc00-c1b0-11e9-8e59-d247ec8d7b2e.png[]
//image::https://user-images.githubusercontent.com/2038044/63263460-a4d66200-c256-11e9-8d83-012ae4a8ab26.png[]
image::fip.drawio.png[]

here are some highlights regarding FIP:

* a FIP is associated with a VM's `port`, or a `VMI` (Virtual Machine
  Interface).
* a FIP is allocated from a `FIP pool`
* a FIP pool is created based on a virtual network(`FIP-VN`)
* the `FIP-VN` will be available to outside of the cluster, by setting matching
  `route-target` (`RT`) attributes of gateway routers VRF table . 
* when a gateway router sees a match with its route import policy in the RT,
  it will load the route into its VRF table. all remote clients connected to
  the VRF will be able to communicate with the FIP.

Regarding the FIP concept and role, there is nothing new in contrail kubernetes
environment. But the usage of floating IP has been extended in kubernetes
`service` and `ingress` object implementation, and it plays an important role
for accessing toward kubernetes `service` and `ingress` from external. 
you can check later sections in this chapter for more details on this.

==== Create FIP Pool

creating a FIP pool is a 3 steps process:

* create a public FIP-VN, 
* set `RT` (route-target) for the VN so it can be advertised and imported into
  the gateway router's VRF.
* create a FIP pool based on the public FIP-VN

again this is nothing new but the same steps as with other contrail environment
without kubernetes. however, as you've learned in previous section, with
kubernetes integration a FIP-VN can now be created in a "kubernetes style":

.create a public FIP-VN named `vn-ns-default`

----
#vn-ns-default.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "101.101.101.0/24"
  name: vn-ns-default
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "contrail-k8s-cni"
  }'

$ kubectl apply -f vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default unchanged

$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-ns-default   22d
----

.set the `RT`

if you need the FIP to be reachable from Internet through gateway router, you'll
need to set a route-target to make the VN prefix getting imported in the gateway
router's VRF table. this step is necessary whenever Internet access is required.

.contrail command: setting RT
image::https://user-images.githubusercontent.com/2038044/60751261-b43c6d00-9f80-11e9-93c5-b06aeb642eb0.png[]

NOTE: the UI navigation path to set RT is:
contrail command(CC): main-menu > Overlay > "Virtual Networks" >
k8s-vn-ns-default-pod-network > Edit > "Routing, Bridging and Policies"


////
NOTE: in the later lab demo of `service` or `ingress`, you always need to set the
RT to the public VN whenever they need to be accessed from Internet host, 
////

.create a FIP pool based on the public VN

this is the final step. from contrail command UI, Create a floating IP pool
based on the public VN:

.contrail command: create a FIP pool
image::https://user-images.githubusercontent.com/2038044/60357727-6d5bdf80-99a2-11e9-90c1-98b037cb0c98.png[]

NOTE: the UI navigation path for this setting is: contrail-command: main-menu >
Overlay > Floating IP > Create

TIP: in contrail UI, you can also set the "external" flag in VN "Advanced"
options so that a FIP pool named "public" will automatically be created.

==== FIP Pool Scope

there are different ways you can refer an floating IP pool in contrail
kubernetes environment, and correspondingly the scope of the pools will also be
different. here are 3 possible levels with descending priority:

* object specific
* Namespace level
* global level

.object specific

this is the most specific level of scope. object specific FIP pool binds itself
only to the object that you specified, it does not affect any other objects in
the same NS or the cluster. E.g. you can specify a service object `web` to get
FIP from FIP pool `pool1`, a service object `dns` to get FIP from another FIP
pool `pool2`, etc.  This gives the most granular control of where the FIP will
be allocated from for an object, the cost is that you need to explicitly specify
it in your yaml file for every object.

.NS level

In a multi tenancy environment each namespace would be associated to a tenant,
and each tenannt would have dedicated FIP pool. In that case it is better to
have a option to define at "NS level" FIP pool, so that all objects created in
that NS will get FIP assignment from that pool. with NS level pool defined
(e.g. `pool-ns-default`), there is no need to specify the FIP-pool name in each
object's yaml file any more. you can still give a different pool name, say
`my-webservie-pool` in an object `webservice` , in that case object `webservice`
will get the FIP from `my-webservice-pool` instead of from the NS level pool
`pool-ns-default`, because the former is more specific.

.global level

the scope of the "global" level pool will be the whole cluster. objects in any
namespaces can use the "global" FIP pool.

you can combine all 3 methods to take advantages of the flexibility. here is a
practical example:

* define a global pool `pool-global-default`, so any objects in a NS that has no
  NS-level or object-level pool defined, will get a FIP from this pool
* for NS `dev`, define a FIP pool `pool-dev`, so all objects created in NS `dev`
  will by default get FIP from `poo-dev`
* for NS `sales`, define a FIP pool `pool-sales`, so all objects created in NS
  `sales` will by default get FIP from `poo-dev`
* for NS `test-only`, do NOT define any NS level pool, so by default objects
  created in it will get FIP from the `pool-global-default`
* when a service `dev-websevice` in NS `dev` needs a FIP from `pool-sales`
  instead of `pool-dev`, specify `pool-sales` in `dev-webservice` object yaml
  file will achieve this goal.

NOTE: Just keep in mind the rule of thumb - the most specific scope will always
prevail.

===== Object FIP Pool

let's first take a look at the object-specific FIP pool. here is an example:

----
#service-web-lb-pool-public-1.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb-pool-public-1
  annotations:
    "opencontrail.org/fip-pool": "{'domain': 'default-domain', 'project': 'k8s-ns-user-1', 'network': 'vn-public-1', 'name': 'pool-public-1'}"
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
----

in this example, service `service-web-lb-pool-public-1` will get an FIP from
pool `pool-public-1`, which is created based on VN `vn-public-1` under current
project `k8s-ns-user-1`. the corresponding kubernetes NS is `ns-user-1`. since
object level FIP pool is assigned for this specific object only, with this
method each new object needs to be assigned a FIP pool explicitly.

===== NS FIP Pool

the next FIP pool scope is in NS level. each NS can define its own FIP
pool.  same way as kubernetes annotations object is used to give a subnet to a
VN, it is also used to specify a FIP pool. the yaml file looks:

----
#ns-user-1-default-pool.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    opencontrail.org/isolation: "true"
    opencontrail.org/fip-pool: "{'domain': 'default-domain', 'project': 'k8s-ns-user-1', 'network': 'vn-ns-default', 'name': 'pool-ns-default'}"
  name: ns-user-1
----

in this example, NS `ns-user-1` is given a NS level FIP pool named
`pool-ns-default`, and the corresponding VN is `vn-ns-default`. once the NS
`ns-user-1` is created with this yaml file, any new service which requires an
FIP, if not created with the object-specific pool name in its yaml file, will
get a FIP allocated from this pool. In practice, most NS (especially
those isolated NS) will need its own NS default pool so you will see this
type of configuration very often in field.

===== Global FIP pool

to specify a global level FIP pool, you need to give the full qualified pool
name (domain > project > network > name) in contrail-kube-manager('KM') docker's
configuration file(`/etc/contrail/contrail-kubernetes.conf`). This file is
automatically generated by the docker during its bootup based on its ENV
parameters, which can be found in '/etc/contrail/common_kubemanager.env` file in
master node:

----
#/etc/contrail/common_kubemanager.env
VROUTER_GATEWAY=10.169.25.1
CONTROLLER_NODES=10.85.188.19
KUBERNETES_API_NODES=10.85.188.19
RABBITMQ_NODE_PORT=5673
CLOUD_ORCHESTRATOR=kubernetes
KUBEMANAGER_NODES=10.85.188.19
CONTRAIL_VERSION=master-latest
KUBERNETES_API_SERVER=10.85.188.19
TTY=True
ANALYTICS_SNMP_ENABLE=True
STDIN_OPEN=True
ANALYTICS_ALARM_ENABLE=True
ANALYTICSDB_ENABLE=True
CONTROL_NODES=10.169.25.19
----

as you can see, this `.env` file contains important environmental parameters
about the setup. to specify a `global FIP pool`, add following line in it:

----
KUBERNETES_PUBLIC_FIP_POOL={'domain': 'default-domain','name': 'pool-global-default','network': 'vn-global-default','project': 'k8s-ns-user-1'}
----

it reads: the global FIP pool is called `pool-global-default`, and it
is defined based on a VN `vn-global-default` under project `k8s-ns-user-1`.
which indicates that the corresponding kubernetes namespace is `ns-user-1`.

now with that piece of configuration placed, you can "re-compose" the
`contrail-kube-manager` docker container to make the change take effect.
essentially you need to tear it down and then bring it back up:

----
$ cd /etc/contrail/kubemanager/
$ docker-compose down;docker-compose up -d
Stopping kubemanager_kubemanager_1 ... done
Removing kubemanager_kubemanager_1 ... done
Removing kubemanager_node-init_1   ... done
Creating kubemanager_node-init_1 ... done
Creating kubemanager_kubemanager_1 ... done
----

now the global FIP pool is specified for the cluster.

NOTE: In all three scopes, FIP is automatically allocated and associated
only to service and ingress objects. If the FIP has to be associated to a
POD it has to be done manually. we'll talk about this in next section.


=== FIP for Pods

once FIP pool is created and available, an FIP can be allocated from the FIP
pool for the pods that requires one. this can be done by associating an FIP
to a VMI (VM or pod interface),

you can manually create a FIP out of a FIP pool in contrail UI, and then
associate it with a pod VMI.

.create FIP
image::https://user-images.githubusercontent.com/2038044/61014424-567b9c80-a355-11e9-832e-3a7f33d2590e.png[]

.associate a FIP in a pod interface
image::https://user-images.githubusercontent.com/2038044/61014684-aa3ab580-a356-11e9-92e7-882e21dd6657.png[]

NOTE: make sure the FIP pool is shared to the project where FIP is going to be
created.

=== Advertising FIP

once a FIP is associated to a pod interface, it will be advertised to the MP-BGP
peers, which are typically gateway routers.

following screenshot shows how to add/edit a BGP peer.

.contrail command: select "main-menu" > INFRASTRUCTURE: "Cluster" > "Advanced Options"
image::https://user-images.githubusercontent.com/2038044/61074698-4c55ae80-a3e6-11e9-81d5-5efa962cbdb5.png[]

.contrail command: select "BGP router" > "create"
image::https://user-images.githubusercontent.com/2038044/63260144-2bd30c80-c24e-11e9-973a-aa911e7d2ae1.png[]

.edit BGP peer parameters
image::https://user-images.githubusercontent.com/2038044/61074999-0cdb9200-a3e7-11e9-80a3-b180d6454267.png[]

input all the BGP peer information, don't forget to associate the controller(s),
which is shown next:

.associate the peer to a controller
image::https://user-images.githubusercontent.com/2038044/61075110-4d3b1000-a3e7-11e9-8eec-ece0304ce4d8.png[]

from the dropdown of `peer` under `Associated Peers`, select the controller(s)
to peer with this new BGP router that you are trying to add.  click `save` when
done. a new BGP peer with ROUTER TYPE "router" will pop up.

.a new BGP router in the BGP router list
//image::https://user-images.githubusercontent.com/2038044/61074880-be2df800-a3e6-11e9-82af-7e58ccd7e710.png[]
image::https://user-images.githubusercontent.com/2038044/61079058-1289a580-a3f0-11e9-93a7-85eb53397a32.png[]

now we've added a peer BGP router as type "router". for local BGP speaker which
is with type "control-node", we just need to double check the parameters by
clicking `edit` button. in our test we want to build MP-IBGP neighborship
between contrail controller and gateway router, so we make sure the ASN and
"Address Families" matches on both end.

.contrail controller BGP parameters: ASN
image::https://user-images.githubusercontent.com/2038044/61075264-94c19c00-a3e7-11e9-90bd-6006dad35ef0.png[]

now you can check BGP neighborship status in gateway router.

----
labroot@camaro> show bgp summary | match 10.169.25.19
10.169.25.19          60100       2235       2390       0      39    18:19:34 Establ
----

once the neighborship is "Established", BGP routes will be exchanged between the
two speakers, that is the time we'll see that the FIP assigned to the kubernetes
object is advertised by master node (`10.169.25.19`) and learned in the gateway
router.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2
Jul 11 01:18:31

k8s-test.inet.0: 8 destinations, 8 routes (8 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.2/32   *[BGP/170] 00:01:42, MED 200, localpref 100, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 47
----

the `detail` version of same command tells more: the FIP route is reflected from
the contrail controller, but "Protocol next hop" being the compute node
(`10.169.25.20`) indicates that the FIP is assigned to a compute node. 
one entity currently running in that compute node own the FIP.

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.2 detail | match "next hop"
Jul 11 01:19:18
                Next hop type: Indirect, Next hop index: 0
                Next hop type: Router, Next hop index: 1453
                Next hop: via gr-2/3/0.32771, selected
                Protocol next hop: 10.169.25.20
                Indirect next hop: 0x900e640 1048601 INH Session ID: 0x70f
----

//in this capture the next hop is on `10.169.25.20`, node `cent222`. 
the dynamic soft GRE configuration make the gateway router automatically create
a soft GRE tunnel interface:

----
labroot@camaro> show interfaces gr-2/3/0.32771
Jul 11 01:19:53
  Logical interface gr-2/3/0.32771 (Index 432) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000 Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` indicates GRE outer IP header, so the "tunnel" is built from
current gateway router whose BGP local address is `192.168.0.204`, to remote 
node `10.169.25.20`, in this case it's one of the contrail compute nodes.

the FIP advertisement process is illustrated in this figure below:

.FIP advertisement
//image::https://user-images.githubusercontent.com/2038044/63262377-c5e98380-c253-11e9-996f-27eecb0df931.png[]
//image::https://user-images.githubusercontent.com/2038044/63263090-a4899700-c255-11e9-8e76-cbee47c2faae.png[]
image::fip-advertisement.drawio.png[]

=== summarization

in this section we created the following objects:

. NS: `ns-user-1`
. FIP-VN: `vn-ns-default`
. FIP-pool: `pool-ns-default`
//. client pod: `cirros`

what will hold all of our test objects is the `ns-user-1` NS/project, which
refers to a NS level pool `pool-ns-default` that is to be created manually.  the
NS level pool is based on a VN `vn-ns-default` that has subnet `101.101.101/24`.
FIP for objects created in NS `ns-user-1` will be assigned from this subnet. 
//a pod `cirros` is needed to start the internal HTTP request towards the Ingress.

[NOTE]
====
if you have yaml files ready for the NS and FIP-VN, to create these
object:

----
$ kubectl apply -f ns/ns-user-1-default-pool.yaml
namespace/ns-user-1 created
$ kubectl apply -f vn/vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default created
----
//$ kubectl apply -f pod/pod-cirros.yaml
//pod/cirros created

TIP: the FIP-pool needs to be created seperately in contrail UI. refer "contrail
floating IP" section for the details.

====

////
----
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
----
////

//in order to create and test Service and Ingress, we will also need to create the following
//objects in our setup before creating Service and Ingress:

with these objects we now have a NS associated with a FIP pool. from inside of
this NS we can proceed to create and study more other kubernetes objects. Next
we'll look at `Service`.

NOTE: In this book, all tests that we are going to demonstrate in `service` and
`Ingress` section will be created under this `ns-user-1` NS.

== Contrail Services

in this section, we look at kubernetes `service` in contrail environment.
specifically, we'll focus on `clusterIP` and `loadbalancer` type of services
that is commonly used in practice. contrail uses its `loadbalancer` object to
implement these two type of services. we'll first review the concept of legacy
contrail neutron loadbalancer, then we'll look into the extended ECMP
loadbalancer object which is the object that these two type of`service` are
based on in contrail, for the rest part of this section we will explore how
`clusterIP` and `loadbalancer` service works in detail, each with a test we
build in our testbed.

=== Kubernetes Service

service is the core object in kubernetes. in chapter 3 you've learned what is
kubernetes service and how to create a `service` object with yaml file.
functional-wise, a service is running as a layer 4 (transport layer) load
balancer that is sitting between clients and servers. client can be anything
"requesting" a service. server in our context is the backend pods "responding"
the request. the client only sees the "frontend" - a service IP and service port
exposed by a service, it does not (and no need to) care about which backend pods
(and with what "pod IP") actually responds the service request. inside of the
cluster, that `service IP`, also called `cluster IP`, is a kind of virtual IP
(`VIP`). 

NOTE: in contrail environment it is implemented through floating IP.

This design model is very powerful and efficient in one sense that, it covers
the fragility of the possible single point failure that may be caused by
failure of any individual pod providing the service, therefore making a
`service` much more robust from client's perspective.

////
`pod` is the one doing the real work, and in kubernetes it is very "cheap" to
launch pods as needed. in chapter 3 you'll learned how fast it is to scale a rc
and deployment to control numbers of running pods dynamically. However, the
nature of a kubernetes pod is "mortal". to understand that just think of if a
screw of a chair breaks for whatever reason, you won't bother to "repair" it but
instead you just grab a new one.
////

in contrail kubernetes integration environment, all 3 types of services are
supported:

* clusterIP
* nodePort
* loadbalancer

next we'll introduce how service is implemented in contrail environment.

=== Contrail Service

in chapter 3 we've introduced kubernetes default implementation of service
through `kube-proxy`. in there we mentioned CNI providers can have its own
implementations. in contrail, `nodePort` service is implemented by kube-proxy`.
however, `clusterIP` and `loadbalancer` services are implemented by contrail's
`loadbalancer` (`LB`).  

before we dive into the details of kubernetes service in contrail, it will
be good to review the legacy openstack based loadbalancer concept in contrail. 

TIP: for brevity we'll sometimes also refer `loadbalancer` as `LB`.

==== Contrail Openstack Loadbalancer

contrail loadbalancer is an relatively "old" feature that is supported since version 1.x.
it enables the creation of a pool of VMs serving applications, sharing one
virtual-ip (`VIP`) as the frontend IP towards clients. 
this diagram below illustrates contrail loadbalancer and its components.

.contrail openstack loadbalancer
image::https://user-images.githubusercontent.com/2038044/60641740-1f5c3700-9dfb-11e9-962f-ed67836d8115.png[]

some highlights of this figure:

* the LB is created with a internal VIP `30.1.1.1`. a `LB listener` is also created for each
  listening ports. 
* all backend VMs together compose a `pool` which is with subnet `30.1.1.0/24`,
  same as LB's internal VIP.
* each backend VM in the `pool`, also called a `member`, is allocated an IP from
  the pool subnet `30.1.1.0/24`.
* to expose the LB to external world, it is allocated another VIP which is
  external VIP `20.1.1.1`. 
* a client only sees one external VIP `20.1.1.1`, representing the whole service

.how it works:

* when LB sees a request coming from the client, it does TCP connection proxying. what that
  means is it establishes the TCP connection with the client, extracts the
  clients' HTTP/HTTPS requests, creates a new TCP connection towards one of the
  backend VMs from the pool, and send the request in the new TCP connection.
* when LB gets its response from the VM, it forwards the response to the client.
* when client closes the connection to the LB, the LB may also close its
  connection with the backend VM.

TIP: when client closes its connection to LB, LB may or may not close its
connection to backend VM. depending on the performance or other consideration it
may use a timeout before it tears down the session.

you see that this loadbalancer model is very similar to kubernetes service
concept:

* VIP is the "service IP" 
* backend VM becomes backend pods
* members are added by kubernetes instead of openstack

in fact, contrail re-uses a good part of this model in kubernetes service
implementation. to support service loadbalancing, contrail extends the
loadbalancer with a new driver, with it service will be implemented as "equal
cost multiple path"(ECMP) loadbalancer working in layer 4(transport layer) .
this is the primary difference comparing with the "proxy" mode that the openstack
loadbalancer type does.

*****
.some more implementation details:

* Actullay any loadbalancer can be integrated with contrail via contrail
  component `conrail-svc-monitor`. 
* Each loadbalancer has a loadbalancer driver that is registerd to
  contrail with a `loadbalancer_provider` type.
* `contrail-svc-monitor` listens to contrail `loadbalancer`, `listener`, `pool`
  and `member` objects, it also calls the registered loadbalancer driver to do
  other necessary jobs based on the `loadbalancer_provider` type. 
* contrail by default provides "ecmp loadbalancer" (`loadbalancer_provider` is
  `native`) and "haproxy loadbalancer" (`loadbalancer_provider` is `opencontrail`). 
* The openstack loadbalancer is using "haproxy loadbalancer".
* ingress, on the other hand, is conceptually even closer with the
openstack loadbalancer in the sense that both are layer 7 (application
layer) "proxy" based. more about ingress will be discussed in later section.

*****

==== Contrail Sevice Loadbalancer

let's take a look at service loadbalancer and the related objects.

.service loadbalancer
//image::https://user-images.githubusercontent.com/2038044/60640833-0f425880-9df7-11e9-91e1-9b0830394aaa.png[]
//image::https://user-images.githubusercontent.com/2038044/60677600-f87c2000-9e4f-11e9-8032-7cffd5f35da7.png[]
//TODO: redraw, add color
//image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]
//image::https://user-images.githubusercontent.com/2038044/63821242-3a8a8500-c91a-11e9-81c9-3d93077b3e94.png[]
image::lb-svc.drawio.png[]

highlights in this figure:

* Each service is represented by a `loadbalancer` object. 
* the loadbalancer object comes with a `loadbalancer_provider` property. for
  service implementation a new `loadbalancer_provider` type called `native` is
  implemented.  
* for each sevice port a `listener` object is created for the same service `loadbalancer`
* for each `listener` there will be a `pool` object
* the `pool` contains `members`, depending on number of backend pod one pool may
  has multiple `members`
* each member object in the pool will map to one of pod backend

.this is how service works in contrail:

* `contrail-kube-manager` listens `kube-apiserver` for k8s service and when a
  `custerIP` or `loadbalancer` type of `service` is created, a `loadbalancer`
  object with `loadbalancer_provider` property `native` is created
* `loadbalancer` will have a "virtual IP" `VIP`, which is same as the `service
  IP` 
* The `service-ip`/`VIP` will be linked to each backend pod's interface. This is
  done by a ecmp loadbalancer driver.
* the linkage from service-ip to multiple backend pods interface creates an ECMP
  next-hop in contrail, traffic will be loadbalanced from the source pod towards
  one of the backend pod directly. later we'll show the ECMP prefix in the pod's
  VRF table
* `contrail-kube-manager` continues to listen to `kube-apiserver` for any changes,
  based on pod list in `Endpoints` it will knows the most current backend pods, and
  update members in the pool .

the most important thing to understand in this diagram, as we've mentioned, is
that in contrast to the legancy neutron loadbalancer (and the ingress
loadbalancer which we'll discussed later), there is no application layer "proxy"
in this process. contrail service implementation is based on layer 4 (transport
layer) ECMP based loadbalancing. 

////
detail discussions of the LB and all surrounding objects are out
of the scope of this book.


NOTE: technically, the LB has `VIP` only, but it also has a reference toward VMI
object which again has a reference to the `instance-ip`. the `instance-ip` is
the same IP as `service-ip`. to avoid confusions we won't cover these level of
implementation details in this book.
////

////
# k8s-5.md
Till 4.1, service ip is allocated from cluster-network even for isolated
namespaces. So, service from one isolated namespaces can reach service from
another isolated namespace. Security groups in isolated namespace prevents
reachability from other namespaces which also prevents reachablity from outside
of the cluster. In order to provide reachablity to external entity, the security
group would be changed to allow all which defeats the isolation. 

To address this, two virtual-networks would be created in the isolated
namespaces. One is for pods(pod-network) and another one is for
services(service-network). Contrail network-policy would be created between
pod-network and service-network for the reachablity between pods and services.
Service uses the same service-ipam which will be a flat-subnet like pod-ipam. It
is applicable for default namespace as well. Since virtual-networks are isolated
by default in contrail, services from one isolated namespace can not reach
service from another isolated namespace.
////

////

=== contrail clusterIP service

the `clusterIP` type of service is the most simple one. it is the default mode
if the `ServiceType` is not given. 

clusterIP service is exposed on a `clusterIP` and a service port. when client
pods need to access the service it sends request toward this clusterIP and
service port. service "binds" itself to certain backend pods via label mapping
between the two objects. `endpoint` is created for each service as long as there
is at least one matching pod available to be its backend. this model works great
if all requests are coming from the same cluster. the nature of the clusterIP
limits the scope of this service to be only within the same cluster. overall by
default the clusterIP is not reachable from external. 

////

==== Contrail Loadbalancer Objects

we've talked a lot about the contrail "loadbalancer object" and you may wonder
what exactly it looks like. now we'll dig a little big deeper to look at the
loadbalancers and the supporting objects: listener, pool, members.

in contrail setup you can pull the object data either from contrail UI, CLI
(`curl`) or third party UI tools based on restapi. in production depending on
which one is available and handy you can select your favorite. 

.explore loadbalancer object with `curl`

with `curl` tool you
just need a FQDN of the URL pointing to the object. 

e.g.: to find the loadbalancer object URL for the service
`service-web-clusterip` from loadbalancers list:

----
$ curl http://10.85.188.19:8082/loadbalancers | \
    python -mjson.tool | grep -C4 `service-web-clusterip`
        {
            "fq_name": [
                "default-domain",
                "k8s-ns-user-1",
                "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
            ],
            "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
            "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        },
----

now with one specific loadbalancer URL, you can pull the specific LB object
details:

----
$ curl \
    http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc \
    | python -mjson.tool
{
    "loadbalancer": {
        "annotations": {
            "key_value_pair": [
                {
                    "key": "namespace",
                    "value": "ns-user-1"
                },
                {
                    "key": "cluster",
                    "value": "k8s"
                },
                {
                    "key": "kind",
                    "value": "Service"
                },
                {
                    "key": "project",
                    "value": "k8s-ns-user-1"
                },
                {
                    "key": "name",
                    "value": "service-web-clusterip"
                },
                {
                    "key": "owner",
                    "value": "k8s"
                }
            ]
        },
        "display_name": "ns-user-1__service-web-clusterip",
        "fq_name": [
            "default-domain",
            "k8s-ns-user-1",
            "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        ],
        "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "id_perms": {
            ...<snipped>...
        },
        "loadbalancer_listener_back_refs": [    #<---
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/loadbalancer-listener/3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc-TCP-8888-3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
                ],
                "uuid": "3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
            }
        ],
        "loadbalancer_properties": {
            "admin_state": true,
            "operating_status": "ONLINE",
            "provisioning_status": "ACTIVE",
            "status": null,
            "vip_address": "10.105.139.153",    #<---
            "vip_subnet_id": null
        },
        "loadbalancer_provider": "native",      #<---
        "name": "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "parent_href": "http://10.85.188.19:8082/project/86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "parent_type": "project",
        "parent_uuid": "86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "perms2": {
            ...<snipped>...
        },
        "service_appliance_set_refs": [
            ...<snipped>...
        ],
        "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "virtual_machine_interface_refs": [
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/virtual-machine-interface/8d64176c-9fc7-491a-a44d-430e187d6b52",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "k8s__Service__service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
                ],
                "uuid": "8d64176c-9fc7-491a-a44d-430e187d6b52"
            }
        ]
    }
}
----

the output is very extensive and includes a whole bunch of details that may not
be of our interests at this moment. but it does tell something interesting:

* in "loadbalancer_properties", the LB use service IP as its VIP
* the LB is connected to a listener by a reference
* `loadbalancer_provider` attribute is `native`, this is a new extension to
  implement layer 4 (transport layer)  ECMP for kubernetes service

.explore LB from UI

in the rest part of the exploration to LB and its related objects, we'll use the
legacy contrail UI.

TIP: you can also use the new contrail command UI to do the same.

for each service there is a LB object, in the below capture it shows 2 LB
objects:

* `ns-user-1-service-web-clusterip`
* `ns-user-1-service-web-clusterip-mp`

.loadbalancer object list
image::https://user-images.githubusercontent.com/2038044/60685179-a0edac80-9e6f-11e9-98c1-e2db001df543.png[]

this indicates 2 services were created. the service loadbalancer object's name
is composed by connecting NS name with service name, hence we can tell the
2 service's name:

* `service-web-clusterip` 
* `service-web-clusterip-mp`

===== Loadbalancer

click on the small triangle icon in left of the first loadbalancer object
`ns-user-1-service-web-clusterip` to expand it, then click on `advanced json
view` icon on the right, you will see the similar detail information as what
you've seen in `curl` capture. for example the `VIP`, `loadbalancer_provider`,
`loadbalancer_listener` object that refers it, etc. 

from here you can keep expanding the `loadbalancer_listener` object by clicking
the `+` character to see the detail information of it. you then see a
`loadbalancer_pool`, expand it again you will see `member`. you can repeat this
process to explore through the object data. by the reference all
of these objects are connected to each other and work together.

.loadbalancer
image::https://user-images.githubusercontent.com/2038044/60685370-bca58280-9e70-11e9-8030-2746766082c8.png[]

===== Listener

click on the LB name and select "listener", then expand it and display the
details with JSON format, you will get the listener details. the listener is
listening on service port 8888, and it is referenced by a `pool`.

TIP: in order to see the detail parameters of an object in JSON format, click
the triangle in the left of the loadbalancer name to expand it, then click on
the "Advanced JSON view" icon
image:https://user-images.githubusercontent.com/2038044/63659232-4b9e8f00-c77e-11e9-85d5-6a1b7a654f05.png[]
on the up right corner in the expanded view. We'll use the JSON view a lot in
this book to explore different contrail objects.

.listener
image::https://user-images.githubusercontent.com/2038044/60685556-b368e580-9e71-11e9-820f-47fb25aacee4.png[]

===== Pool and Member
just repeat the exploring process we will get down to the pool and two
`members` in it. the member is with a port of `80`, which maps to the container
targetPort in pod.

.pool
image::https://user-images.githubusercontent.com/2038044/60685626-15c1e600-9e72-11e9-8539-a24ea28b0bf3.png[]

.members
image::https://user-images.githubusercontent.com/2038044/60685682-6fc2ab80-9e72-11e9-804d-5eccd8e055df.png[]

next we'll examine the vrouter VRF table for the pod to show contrail service
loadbalancer ECMP operation details. in order to better understand the "1 to N"
mapping between loadbalancer and listener shown in the loadbalancer object
figure, we'll also give an example of a "multiple port service" in
our setup.  we'll conclude the ClusterIP service section by inspecting the
vrouter flow table to illustrate the service packet workflow.

=== Contrail ClusterIP Service

in chapter 3 we've demonstrated how to create and verify a clusterIP service. in
this section we'll revisit the lab and look at some important details about
contrail specific implementations. we'll continue and add a few more tests to
illustrate the contrail service loadbalancer implementation details.

==== ClusterIP as FIP

this is the yaml file we used to create a `clusterIP` service:

----
#service-web-clusterip.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
----

let's review what we got from service lab in chapter3:

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.105.139.153  <none>       8888/TCP  45m  app=webserver
----

----
$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ...  IP             NODE     ...  LABELS
client                      1/1    Running  ...  10.47.255.237  cent222  ...  app=client
webserver-846c9ccb8b-g27kg  1/1    Running  ...  10.47.255.238  cent333  ...  app=webserver
----

////
----
$ kubectl get pod -o wide --show-labels
NAME                              READY STATUS   ... IP             NODE     ... LABELS
cirros                            1/1   Running  ... 10.47.255.237  cent222  ... app=cirros
webserver-846c9ccb8b-kvwvw 1/1   Running  ... 10.47.255.238  cent333  ... app=webserver
----
////

here we see one service is created, with one pod running as its backend. the
label in the pod matches to the SELECTOR in service. the pod name also indicates
this is a deploy-generated pod. later we can scale the deploy for ECMP case
study, for now we'll stick to one pod and examine the ClusterIP implementation
details.

in contrail, a `ClusterIP` is essentially implemented in the form of a FIP.
once a service is created, a FIP will be allocated from the service subnet
and associated to all the backend pod VMI to form the ECMP loadbalancing.
Now all backend pods can be reached via cluserIP(along with the POD IP). 
This clusterIP(FIP) is acting as a "VIP" to the client pods inside of the
cluster.

TIP: Why contrail chose FIP to implement clusterIP? In the previous section, we
have learned that contrail does NAT for FIP and service also needs NAT. So it
is natural to use the FIP for clusterIP. 

For loadbalancer type of service, contrail will allocate a second FIP -
the "EXTERNAL-IP" as the VIP, and the external VIP is advertised outside of
the cluster through gateway router. you will get more details about these later.

from UI we'll see the automatically allocated FIP as ClusterIP.

.ClusterIP as FIP
image::https://user-images.githubusercontent.com/2038044/60973473-57c9ac80-a2f6-11e9-81a7-df74349e9877.png[]

the FIP is also associated with the pod VMI and podIP, in this case the VMI is
representing the pod interface.

.pod interface
image::https://user-images.githubusercontent.com/2038044/60975990-df191f00-a2fa-11e9-9f81-e635c141c7e6.png[]

the interface can be expanded to display more details:

.pod interface detail
//image::https://user-images.githubusercontent.com/2038044/63632000-c6d83780-c5fc-11e9-92a6-6bed7f09a944.png[]
image::https://user-images.githubusercontent.com/2038044/63632051-87f6b180-c5fd-11e9-8695-9ec6fc7c88ca.png[]

expand the `fip_list`, we'll see more information below:

----
fip_list:  {
    list:  {
        FloatingIpSandeshList:  {
            ip_addr: 10.105.139.153
            vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
            installed: Y
            fixed_ip: 10.47.255.238
            direction: ingress
            port_map_enabled: true
            port_map:  {
                list:  {
                    SandeshPortMapping:  {
                    protocol: 6
                    port: 80
                    nat_port: 8888
                    }
                }
            }
        }
    }
}
----

service/clusterIP/FIP 10.105.139.153 maps to podIP/fixed_ip 10.47.255.238.  the
`port_map` tells that port `8888` is a `nat_port`, `6` is the protocol number so
it means protocol TCP. overall, clusterIP:port `10.105.139.153:8888` will be
translated to podIP:targetPort `10.47.255.238:80` and vice versa.

now you understand with FIP representing ClusterIP, NAT will happen in service.
later we'll examine NAT again in the flow table.

.Scaling Backend Pods
in chapter 3 clusterIP service example, we have created a sevice and a backend
pod. to verify the ECMP, let's increase the replica to 2 to generate a second
backend pod. this is a more realistic and rebost model: each pod will now be
backing up each other to avoid a single point failure.

instead of using yaml file to manually create a new webserver pod, with the
"kubernetes spirit" in mind you should think of to `scale` a Deployment,
as what you`ve seen earlier in this book. in our service example we`ve been
using `Deployment` object to spawn our webserver pod on purpose:

----
$ kubectl scale deployment webserver --replicas=2
deployment.extensions/webserver scaled

$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ... IP             NODE     ... LABELS
client                      1/1    Running  ... 10.47.255.237  cent222  ... app=client
webserver-846c9ccb8b-7btnj  1/1    Running  ... 10.47.255.236  cent222  ... app=webserver
webserver-846c9ccb8b-g27kg  1/1    Running  ... 10.47.255.238  cent333  ... app=webserver

$ kubectl get svc -o wide
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service-web-clusterip   ClusterIP   10.105.139.153   <none>        8888/TCP   45m   app=webserver
----

immediately after you create a new webserver pod by scaling the deployment with
`replicas 2`, a new pod is launched.  we end up having 2 backend pods now, one
is running in same node `cent222` as the client pod, or a "local" node
for client pod; the other one is running in the other node `cent333` - the
"remote" node from client pod's perspective.  and the `endpoint` objects get
updated to reflect the current set of backend pods behind the `service`.

----
$ kubectl get ep -o wide
NAME             ENDPOINTS                           AGE
service-web-lb   10.47.255.236:80,10.47.255.238:80   20m
----

NOTE: without `-o wide` option, only first endpoint will be displayed properly.

we go ahead and check the FIP again.

.ClusterIP as FIP (ECMP)
image::https://user-images.githubusercontent.com/2038044/60973157-b2163d80-a2f5-11e9-957a-438642355391.png[]

we see the same FIP, but now it is associated with two podIP, each representing
a seperate pod. 

==== ECMP Routing Table
===== Control Node Perspective

first, to examine the ECMP, let's take a look at the routing table in the
controller's routing instance.

.control node routing instance table
image::https://user-images.githubusercontent.com/2038044/60966312-ee41a200-a2e5-11e9-8966-053f0bbc20ea.png[]

the routing instance (RI) has a full name with the following format:

    <DOMAIN>:<PROJECT>:<VN>:<RI>

in most cases RI inheritate the same name from it's VN, so in our case the
full IPv4 routing table has this name:
`default-domain:k8s-ns-user-1:k8s-ns-user-1-pod-network:k8s-ns-user-1-pod-network.inet.0`
the `.inet.0` indicate the routing table type is unicast IPv4. there are many
other tables which is not of our interests right now.

two routing entries with the same exact prefixes of the ClusterIP show up in the
routing table, with two different next hops, each pointing to a different node.
this gives a hint about the route propagation process: both nodes(compute) has
advertised the same clusterIP toward the master(contrail controller), to
indicate the presence of the running backend pods in itself. this route
propagation is via XMPP. master(contrail controller) then reflect the routes to
all other compute nodes.

===== Compute Node Perspective

next, starting from the client pod node `cent222`, we'll look at the the pod's
VRF table to understand how the packets are forwarded towards the backend pods

.vrouter vrf table
image::https://user-images.githubusercontent.com/2038044/60680116-18174680-9e58-11e9-9235-48c152959df7.png[]

the most important part of the screenshot is the routing entry `Prefix:
10.105.139.153 / 32 (1 Route)`, it is our ClusterIP address. underneath the
prefix there is a statement `ECMP Composite sub nh count: 2`. this indicates the
prefix has multiple possible next hop to reach. now expand it by clicking the
small triangle icon in the left, you will be given a lot more details about this
prefix.

.vrouter ECMP nexthop
image::https://user-images.githubusercontent.com/2038044/60680345-ece12700-9e58-11e9-9793-2b609918e146.png[]

among all of the details in this outputs, the most important thing that is of
our focus is `nh_index: 87`, which is the next hop ID (`NHID`) for the clusterIP
prefix. from vrouter agent docker, we can further resolve the "Composite" NHID to 
the `sub-NHs`, which is the "member" nexthops under the "Composite" next hop:

TIP: don't forget to execute the vrouter commands from the vrouter docker.
doing it from the host directly may not work.

////
----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(25) 37(59)              #<---

Id:51         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:37         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00
----

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 51 and nexthop 37, each
  representing a seperate path towards the backend pods
* nexthop 51 represents a "MPLSoUDP" tunnel toward backend pod in the remote
  node, the tunnel is established from current node `cent222`, with source IP
  being local fabric IP `10.169.25.20`, to the other node `cent333` whose fabric
  IP is `10.169.25.21`. if you recall where our two backend pods are located,
  this is the forwarding path between the 2 nodes.
* nexthop 37 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. 

////

----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(43) 37(28)              #<---

Id:51         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:37         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00
----

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 43 and nexthop 28, each
  representing a seperate path towards the backend pods
* nexthop 51 represents a "MPLSoUDP" tunnel toward backend pod in the remote
  node, the tunnel is established from current node `cent222`, with source IP
  being local fabric IP `10.169.25.20`, to the other node `cent333` whose fabric
  IP is `10.169.25.21`. if you recall where our two backend pods are located,
  this is the forwarding path between the 2 nodes.
* nexthop 37 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. 

to resolve the vrouter `vif` interface,  use `vif --get 8` command:

----
$ vif --get 8
Vrouter Interface Table
......
vif0/8      OS: tapeth0-304431
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.236  #<---
            Vrf:2 Mcast Vrf:2 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:455  bytes:19110 errors:0
            TX packets:710  bytes:29820 errors:0
            Drops:455
----

the output displays the corresponding local pod interface's name, IP, etc.

==== ClusterIP Service Workflow

the clusterIP service's loadbalancer ECMP workflow is illustrated in this
figure:

.contrail service loadbalancer ECMP forwarding
//image::https://user-images.githubusercontent.com/2038044/60762382-97f60a00-a02c-11e9-81ad-b1f05d815571.png[]
//image::https://user-images.githubusercontent.com/2038044/60762413-1ce12380-a02d-11e9-8cec-41d5e177bfb9.png[]
//image::https://user-images.githubusercontent.com/2038044/63705462-3287f380-c7fb-11e9-9055-a9f3002708b2.png[]
//image::https://user-images.githubusercontent.com/2038044/63705594-885c9b80-c7fb-11e9-897b-ee55e0d7a70f.png[]
//image::https://user-images.githubusercontent.com/2038044/64903204-b575ce80-d682-11e9-88fc-139869511aeb.png[]
image::svc-clusterip-ecmp.drawio.png[]

this is what happened in the forwarding plane:

* a pod `client` located in node `cent222` needs to access a service
  `service-web-clusterip`, it sends a packet towards the service's clusterIP
  `10.105.139.153` and port `8888`
* `client` sends the packet to `node222` vrouter based on the default route.
* vrouter on `node222` got the packet, it checks its corresponding VRF table, get a
  "Composite" nexthop ID `87`, which resolves to two sub-nexthops `51` and `37`,
  representing a remote and local backend pod respectively. this indicates ECMP.
* vrouter on `node222` starts to forward the packet to one of the pod based on
  its ECMP algorithm. Suppose the remote backend pod is selected, the packet will
  be sent through MPLSoUDP tunnel to the remote pod on node `cent333`, after
  establishing the flow in the flow table. all subsequent packets belongs to the
  same flow will follow this same path. same applys to the local path towards local
  backend pod.

//TODO: this is forwarding flow only, also give control plane flow?

==== Multiple Port Service

we've understood how the service layber 4 ECMP works and we've explored the LB
objects in lab. remember in the figure showing the LB and relevant objects,
we saw that one LB may having 2 or more LB listeners. each listener has an
individual backend pool that has one or multiple member(s). 

.service loadbalancer
//image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]
//image::https://user-images.githubusercontent.com/2038044/63821242-3a8a8500-c91a-11e9-81c9-3d93077b3e94.png[]
image::lb-svc.drawio.png[]

in kubernetes, this 1:N mapping between loadbalancer and listeners indicates
a `multiple port service` - one service with multiple ports.
let's look at the yaml file of it:

//.multiple port service
//====
.svc/service-web-clusterip-mp.yaml
----
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip-mp
spec:
  ports:
  - name: port1
    port: 8888
    targetPort: 80
  - name: port2         #<---
    port: 9999
    targetPort: 90
  selector:
    app: webserver
----
//====

what we've added is another item in the `ports` list: a new service port `9999`
that maps to container's `targetPort` `90`. now with two port mappings we have
to give each port a name, `port1` and `port2` respectively.

NOTE: without a port `name` the multiple ports yaml file won't work.

now we apply the yaml file and a new service `service-web-clusterip-mp` with 2
ports is created:

----
$ kubectl apply -f svc/service-web-clusterip-mp.yaml
service/service-web-clusterip-mp created

$ kubectl get svc
NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)            AGE
service-web-clusterip     ClusterIP  10.105.139.153  <none>       8888/TCP           3h8m
service-web-clusterip-mp  ClusterIP  10.101.102.27   <none>       8888/TCP,9999/TCP  4s

$ kubectl get ep
NAME                       ENDPOINTS                           AGE
service-web-clusterip      10.47.255.238:80                    4h18m
service-web-clusterip-mp   10.47.255.238:80,10.47.255.238:90   69m
----

NOTE: to simply the case study we've scaled down the backend deployment's
replicas number to one.

it looks everything is ok, isn't it? the new service comes up with 2 service
ports exposed, `8888` is the old one we've tested in previous examples, and the
new `9999` port should work equally well.

turns out that is not the case.

service port 8888 works:

----
$ kubectl exec -it client -- curl 10.101.102.27:8888 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.238
                         Hostname = webserver-846c9ccb8b-g27kg
----

service port 9999 doesn't work:

----
$ kubectl exec -it client -- curl 10.101.102.27:9999 | w3m -T text/html | cat
command terminated with exit code 7
curl: (7) Failed to connect to 10.101.102.27 port 9999: Connection refused
----

the request towards port 9999 is rejected. reason is the `targetPort` is not
running in pod container, so there is no way you will get a response from it.

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- netstat -lnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/python
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name    Path
----

`readinessProbe` introduced in chater 3 is the official kubernetes tool to
detect this situation, so in case the pod is not "ready", it will be restarted
and you will catch the events.

to resolve this let's start a new server in pod to listen on the new port `90`
also.  one of the easiest way today to start a HTTP server is to use the
`SimpleHTTPServer` module coming with `python` package. in our test we only need
to set its listening port to `90` (the default value is 8080).

----
$ kubectl exec -it webserver-846c9ccb8b-g27kg -- python -m SimpleHTTPServer 90 
Serving HTTP on 0.0.0.0 port 90 ...                                    
----

now the `targetPort` is on, we can start the request towards service port `9999`
again from the `client` pod. this time it succeed and get the returned webpage
from python SimpleHTTPServer.

----
$ kubectl exec -it client -- curl 10.103.87.232:9999 | w3m -T text/html | cat

Directory listing for /
 ━━━━━━━━━━━━━━━━━━━━━
  • app.py
  • Dockerfile
  • file.txt
  • requirements.txt
  • static/
 ━━━━━━━━━━━━━━━━━━━━━
----

for each incoming request the `SimpleHTTPServer` logs one line output, with an
IP address showing where the request came from. in our case the request coming
from client pod is with the IP `10.47.255.237`.

----
10.47.255.237 - - [04/Jul/2019 23:49:44] "GET / HTTP/1.1" 200 -
----

==== Contrail Flow Table

so far we've tested clusterIP service, and we see client request is sent towards
the service IP. in contrail environment `vrouter` is the module that does all of
the packet forwarding job. when the `vrouter` in client pod gets the packet, it
looks up the corresponding VRF table in vrouter module for the client pod(`client`),
gets the nexthop and resolves the correct egress interface and proper encapsulation.
in our test so far, the client and backend pods are in 2 different nodes, the source
`vrouter` decides the packets need to be sent in MPLSoUDP tunnel, towards the node
where backend pod is running. what interests us the most is:

* how the service IP and backend podIP is translated to each other? 
* is there a way to "capture and see" the two IPs in a flow, "before" and
  "after" the translations for comparison purpose?

the most "straightforward" method you would think of is to capture the packets,
then decode and see. doing that however, may not be as easy as what you've expected.

. first you need to capture the packet at different places:

    * at the pod interface, this is after the address is translated, that part is
      easy
    * the fabric interface, this is before packet is translated and reaches the pod
      interface. here the packets are with MPLSoUDP encapsulation since data plane
      packets are "tunneled" between nodes.

. then you need to copy the pcap file out and load with wireshark to decode. you
probably also need to configure wireshark to recognize the MPLSoUDP encapsulation.

the easier way is to check the vrouter flow table which records IP and port
details about a traffic flow. in this test we will prepare a big file `file.txt`
in backend webserver pod and try to download it from the client pod. 

[TIP]
====
you may wonder to trigger a flow why we don't simply use same curl test to pull
the webpage, as what we've done in early test. in theory that is fine.  the only
problem is that the TCP flow follows the TCP session. in our previous test with
`curl`, the TCP session starts and stops immediately after the webpage is
retrieved, then the vrouter clears the flow right away. you won't be fast enough
to capture the flow table at the right moment. instead, downloading a big file
will hold the TCP session - as long as the file transfer is ongoing the session
will remain, and we can take time to investigate the flow. later on in `ingress`
section we will demonstrate a different method with a one-liner shell script.  

====

now in the client pod curl URL, instead of just give root path `/` to list the
files in folder, we try to pull the file: `file.txt`

----
$ kubectl exec -it client -- curl 10.103.87.232:9999/file.txt
----

in server pod we see the log indicating the file downloading starts:

----
10.47.255.237 - - [05/Jul/2019 00:41:21] "GET /file.txt HTTP/1.1" 200 -
----

now with the file transfer ongoing, we have enough time to collect the flow table
from both client and server node, in the vrouter docker.

.client node flow table

----
(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1361 Added 1361 Deleted 442 Changed 443Processed 1361 Used Overflow entries 0
(Created Flows/CPU: 305 342 371 343)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    40100<=>340544       10.47.255.237:42332                                 6 (3)
                         10.103.87.232:9999
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):59,  Stats:7878/520046,
 SPort 65053, TTL 0, Sinfo 6.0.0.0)

   340544<=>40100        10.103.87.232:9999                                  6 (3)
                         10.47.255.237:42332
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):68,  Stats:142894/205180194,
 SPort 63010, TTL 0, Sinfo 10.169.25.21)
----

highlights in this output:

* client starts TCP connection from its pod IP `10.47.255.237` and a
  rondom source port, towards the service IP `10.103.87.232` and server port
  `9999`
* the flow TCP flag `SSrEEr` indicates the session is established
  bidirectionally.
* Action `F` means "forwarding". note that there is no special processing like
  `NAT` happening here. 

NOTE: with a filter `--match 15.15.15.2`. only flow entries with Internet Host
IP is printed.

we can conclude, from client's perspective, it only see the service IP. it is
not aware of any backend pod IP at all.

.server node flow table

now look at flow table in server node vrouter docker:

----
(vrouter-agent)[root@cent333 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1116 Added 1116 Deleted 422 Changed 422Processed 1116 Used Overflow entries 0
(Created Flows/CPU: 377 319 76 344)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   238980<=>424192       10.47.255.238:90                                    6 (2->3)
                         10.47.255.237:42332
(Gen: 1, K(nh):24, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):24,
 Stats:8448/202185290,  SPort 62581, TTL 0, Sinfo 3.0.0.0)

   424192<=>238980       10.47.255.237:42332                                 6 (2->2)
                         10.103.87.232:9999
(Gen: 1, K(nh):24, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:8067/419582,  SPort 51018, TTL 0, Sinfo 10.169.25.20)
----

let's look at the second flow entry first - the IPs looks same as the one we
just saw in client side capture.  traffic lands vrouter fabric interface from
remote client node, across MPLSoUDP tunnel. destination IP and port are
service IP and service port respectively. it seems nothing special here.

however, the flow `Action` now is set to `N(DPd)`, not `F`. according to the
header lines in the `flow` command output, this means NAT, or specifically,
`DNAT` (Destination address translation) with `DPAT` (Destination port
translation) - both the service IP and service port are translated, to
backend pod IP and port.

now look at the first flow entry. source IP `10.47.255.238` is the backend pod
IP and source port is python server port `90` opened in backend container .
obviously this is the returning traffic indicating the file downloading is still
ongoing. the `Action` is also NAT(`N`), but this time it is the reverse
operation - source NAT (`SNAT`) and source PAT(`SPAT`). vrouter will
translate backend's source IP source port to the service IP and port, before
putting it into the MPLSoUDP tunnel and returning back to client pod in remote
node.

the complete end to end traffic flow is illustrated here:

.clusterIP service traffic flow (NAT)
//image::https://user-images.githubusercontent.com/2038044/60388198-f7c44200-9a7b-11e9-9b08-f34167b0a2b8.png[]
//image::https://user-images.githubusercontent.com/2038044/60762300-96c3dd80-a02a-11e9-8933-452d3ee074a4.png[]
//image::https://user-images.githubusercontent.com/2038044/60763424-32147d00-a042-11e9-813a-a6aa3989c09d.png[]
//image::https://user-images.githubusercontent.com/2038044/64745576-67bc6300-d4d6-11e9-8919-f8b1ea0d8f8f.png[]
image::svc-clusterip.drawio.png[]

=== Contrail Loadbalancer Service

in chapter 3 we've briefly talked about `LoadBalancer` service. in there we
mentioned if the goal is to expose the service to the external world outside of
the cluster, we just specify `ServiceType` as `LoadBalancer` in the service yaml
file. 

whenever a service of `type: LoadBalancer` get created, in contrail environment
what will happen is , not only a `clusterIP` will be allocated and exposed to
other pods within the cluster, but also a `floating ip` from public fip pool
will be assigned to the loadbalancer instance as an "external IP" and exposed to
the public world outside of the cluster. 

while the `clusterIP` is still acting as a `VIP` to the client **inside** of the
cluster, the `floating ip` or `external IP` will essentially act as a `VIP`
facing those client sitting **outside** of the cluster, for example, a remote
Internet host which sends request to the service across the gateway router. 

in this section we'll demonstrate how does the `LoadBalancer` type of service
works in our end to end lab setup, which includes the kubernetes cluster, fabric
switch, gateway router, and Internet host.

==== External IP as FIP

//create `Loadbalancer` service

let's look at the yaml file of a `LoadBalancer` service. it is same as ClusterIP
service except just one more line declaring the service `type`:

----
#service-web-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer    #<---
----

create and verify the service:

----
$ kubectl apply -f service-web-lb.yaml
service/service-web-lb created

$ kubectl get svc -o wide
NAME            TYPE          CLUSTER-IP   EXTERNAL-IP      PORT(S)         AGE    SELECTOR
service-web-lb  LoadBalancer  10.96.89.48  101.101.101.252  8888:32653/TCP  10s    app=webserver
----

comparing with the `clusterIP` service type, this time in the "EXTERNAL-IP"
column there is an IP allocated. if you remember what we've covered in the
"floating IP pool" section, you should understand this "EXTERNAL-IP" is actually
another `FIP`, allocated from the `NS FIP pool` or `global FIP pool` - we did
not give any specific FIP pool information in the service object yaml file, so
based on the algorithm right FIP pool will be used automatically. 

from UI we'll see that for `loadbalancer` service we now have 2 FIPs: one as
clusterIP (internal VIP), the other one as "EXTERNAL-IP" (external VIP):

.2 FIPs for a `loadbalancer` service
image::https://user-images.githubusercontent.com/2038044/63900303-1c7e5c80-c9ce-11e9-85dd-d4282aaa6c46.png[]

both FIPs are associated with the pod interface:

.pod interface
image::https://user-images.githubusercontent.com/2038044/63900502-b6dea000-c9ce-11e9-8537-b44cffc90055.png[]

expend the tap interface, you will see the two FIPs are listed in `fip_list`:

.pod interface detail
image::https://user-images.githubusercontent.com/2038044/63900411-7e3ec680-c9ce-11e9-876f-2deeff06bdef.png[]

////
the `fip_list`, we'll see each FIPs in the list:
----
 fip_list
     FloatingIpSandeshList
       ip_addr: 10.96.89.48
       vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
       installed: Y
       fixed_ip: 10.47.255.238
       direction: ingress
       port_map_enabled: true
       port_map
           SandeshPortMapping
             protocol: 6
             port: 80
             nat_port: 8888
     FloatingIpSandeshList
       ip_addr: 101.101.101.252
       vrf_name: default-domain:k8s-ns-user-1:k8s-vn-ns-default-pod-network:k8s-vn-ns-default-pod-network
       installed: Y
       fixed_ip: 10.47.255.238
       direction: ingress
       port_map_enabled: true
       port_map
           SandeshPortMapping
             protocol: 6
             port: 80
             nat_port: 8888
----
////

now you should understand, the only difference here between the two type of
services, is that for loadbalancer service, an extra FIP is allocated from the
public FIP pool, which is advertised to the gateway router and acts as the
outside-facing VIP. that is how the `loadbalancer` service expose itself to the
external world.

==== Gateway Router VRF Table

in "contrail floating IP" section you've learned how to advertise FIP. here
we'll review the main concepts to understand how it works in contrail `service`
implementation. 

the `route-target` community setting in the FIP VN makes it reachable by the
Internet host, so effectively our service is now also exposed to the Internet
,instead of only to pods inside of the cluster. Examining the gateway router's
VRF table reveals this:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24
Jun 19 03:56:11

k8s-test.inet.0: 23 destinations, 40 routes (23 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:01:11, MED 100, localpref 200, from 10.169.25.19
                      AS path: ?, validation-state: unverified
                    > via gr-2/2/0.32771, Push 40
----

the FIP host route is learned by gateway router, from contrail controller - more
specifically, contrail control node, which acts as a standard MP-BGP VPN `RR`
reflecting routes between compute nodes and the gateway router. A further look
at the detail version of the same route displays more information about this
process:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24 detail
Jun 20 11:45:42

k8s-test.inet.0: 23 destinations, 41 routes (23 active, 0 holddown, 0 hidden)
101.101.101.252/32 (2 entries, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:9
                ......
                Source: 10.169.25.19                    #<---
                Next hop type: Router, Next hop index: 1266
                Next hop: via gr-2/2/0.32771, selected  #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                Indirect next hop: 0x900c660 1048574 INH Session ID: 0x690
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 10:15:38   Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    ......
                Import Accepted
                VPN Label: 44
                Localpref: 200
                Router ID: 10.169.25.19
                Primary Routing Table bgp.l3vpn.0
----
////
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1020006
                    unknown type 8084 value eac4:1030001
////

* the `source` indicates from which BGP peer the route is learned,
  `10.169.25.19` is the contrail controller (and kubernetes master) in our lab
* `protocol next hop` tells who generates the route. `10.169.25.20` is node
  `cent222` where the backend webserver pod is running
* `gr-2/2/0.32771` is an interface representing the (MPLS over) GRE tunnel
  between the gateway router and node `cent222`.

==== Loadbalancer Service Workflow

to summarize, the FIP given to the service as its external ip is advertised to
gateway router, and get loaded in the router's VRF table. when Internet host
sends a request to the FIP, through MPLSoGRE tunnel the gateway router will
forward it to the compute node where backend pod is locating.

the packet flow is illustrated in this figure:

.`loadbalancer` service workflow
//image::https://user-images.githubusercontent.com/2038044/60563159-a7254100-9d28-11e9-94ca-934b8f870b1e.png[]
//image::https://user-images.githubusercontent.com/2038044/63638336-e8fda400-c654-11e9-8938-d98901633b6b.png[]
//image::https://user-images.githubusercontent.com/2038044/64745664-b36f0c80-d4d6-11e9-8019-f901f1c4fb5a.png[]
image::svc-lb.drawio.png[]

here is the full story:

* you create a `FIP pool` from a public VN, with route-target the VN is
  advertised to the remote gateway router via MP-BGP 
* you create a pod with a label `app: webserver`, kubernetes decides the pod
  will be created in node `cent222`. via XMPP the node publish the pod IP
* you create a loadbalancer type of service with `service port` and label
  selector `app=webserver`.  kubernetes allocates a service IP.
* kubernetes finds the pod with the matching label and update the `endpoint`
  with the pod IP and port information. 
* contrail create a loadbalancer instance and assign a FIP to it. contrail also
  associate that FIP with the pod interface, so there will be one to one NAT
  operation between the FIP and podIP.
* via XMPP, node `cent222` advertises this FIP to contrail controller `cent111`,
  which then advertises it to the gateway router.
* on receiving the FIP prefix, gateway router checks and see a the RT of the
  prefix matches to what it is expecting, it will import the prefix in local
  VRF. at this moment the gateway learns the nexthop of the FIP is `cent222`, so
  it generate a soft GRE tunnel toward `cent222`.
* when gateway router see a request coming from Internet toward the FIP, through
  the MPLS over GRE tunnel it will send the request to the node `cent222`
* vrouter in the node sees the packets destined to the FIP, it will perform NAT
  so the packets will be sent to the right backend pod.

===== Verify `Loadbalancer` Service

To verify the end to end service access from Internet host to the backend pod, 
we will login to the Internet host desktop and launch a browser, with URL
pointing to `http://101.101.101.252:8888`. 

TIP: just to keep in mind that the internet host request has to be sent to the
public **FIP**, not to the **service IP**(**clusterIP**) or backend **podIP**
which are only reachable from inside of the cluster!

this is the returned web page:

image::https://user-images.githubusercontent.com/2038044/60388669-ea5e8600-9a82-11e9-87b9-30a98572f7bb.png[]

TIP:
in our testbed we installed a centos desktop as an Internet host. 
////
in our testbed we installed a centos server as an Internet host. as with any
linux distribution, if you need to login the "GUI", you need to install Xwindow
or linux desktop applications and set it up properly. also you need a web
browser if it does not come with the server.
////

To simplify the test, you can also ssh into the Internet host and test it with
`curl` tool:

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | w3m -T text/html | cat
             Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-vl6zs
----

the kubernetes service is available from Internet!

===== Loadbalancer Service ECMP

so far you've seen how loadbalancer type of service is exposed to the Internet
and how the FIP did the "trick". in ClusterIP service section, you've also seen
how the service loadbalancer ECMP works. what you haven't seen yet is how does
the "ECMP" processing works under loadbalancer type of service. To demonstrate
this again we scale the RC to generate one more backend pod behind the
`service`. 

----
$ kubectl scale rc rc-webserver --replicas=2
replicationcontroller/rc-webserver scaled

$ kubectl get pod -l app=webserver -o wide
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
webserver-846c9ccb8b-r9zdt  1/1    Running  0         25m  10.47.255.238  cent333  <none>
webserver-846c9ccb8b-xkjpw  1/1    Running  0         23s  10.47.255.236  cent222  <none>
----

here is the question: with 2 pods on different node as backend now, from the
gatway router's perspective when it get the service request, which node it will
choose to forward the traffic to? let`s check the gateway router`s VRF table
again:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32
Jun 30 00:27:03

k8s-test.inet.0: 24 destinations, 46 routes (24 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 26
                    [BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 26
----

the same FIP prefix is imported as we've seen in previous example, except that
now the same route is learned twice and an additional MPLSoGRE tunnel is
created. previously in ClusterIP service example we use `detail` option in `show
route` command to find the tunnel endpoints, this time we examine the soft GRE
`gr-` interface to find the same:

----
labroot@camaro> show interfaces gr-2/2/0.32771
Jun 30 00:56:01
  Logical interface gr-2/2/0.32771 (Index 392) (SNMP ifIndex 1801)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.21:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None

labroot@camaro> show interfaces gr-2/3/0.32771
  Logical interface gr-2/3/0.32771 (Index 393) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 11
    Output packets: 11
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` of `gr-` interface indicates the two end points of a GRE tunnel:

* `10.169.25.20:192.168.0.204`: tunnel between node `cent222` and gateway router
* `10.169.25.21:192.168.0.204`: tunnel between node `cent333` and gateway router

We end up to have 2 tunnels in the gateway router, each pointing to a different
node where a backend pod is running. now we believe the router will perform
ECMP load balancing between the two GRE tunnel, whenever it got service request
toward the same FIP. let's check it out.

===== Verify `Loadbalancer` Service ECMP

to verify the ECMP we'll just pull the webpage a few more time and we expect to
see both podIP displayed eventually.

turns out this never happens!

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw
----

TIP: `lynx` is another "terminal" web browser pretty much similar with the
`w3m` that we used earlier.

the only webpage we got is from the first backend pod `10.47.255.236`,
`webserver-846c9ccb8b-xkjpw`, running in node `cent222`. the other one never show up.
so the expected ECMP does not happen yet. when we examine the route again with
`detail` or `extensive` keyword we find the root cause:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32 detail | match state
Jun 30 00:48:29
                State: <Secondary Active Int Ext ProtectionCand>
                Validation State: unverified
                State: <Secondary NotBest Int Ext ProtectionCand>
                Validation State: unverified
----

from that we realize that, even if the router learned the same prefix from both
node, only one is `Active` and the other one won't take effect because it is
`NotBest`. therefore, the second route and the corresponding GRE interface
`gr-2/2/0.32771` will never get loaded into the forwarding table:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 00:53:12
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination         Type  RtRef  Next  hop      Type  Index  NhRef  Netif
101.101.101.252/32  user  0      indr  1048597  2
                                Push 26     1272     2 gr-2/3/0.32771
----

this is the default Junos BGP path selection behavior and detail discussion of
it is out of the scope of this book. 

NOTE: for Junos BGP path selection algorithm, check this link:
https://www.juniper.net/documentation/en_US/junos/topics/topic-map/bgp-path-selection.html

the solution is to enable the `multipath vpn-unequal-cost` knob under the VRF:

----
labroot@camaro# set routing-instances k8s-test routing-options multipath vpn-unequal-cost
----

now check the VRF table again:

////
raw capture:
----
labroot@camaro# run show route table k8s-test.inet.0 101.101.101.252/32
Jun 26 20:09:21

k8s-test.inet.0: 27 destinations, 54 routes (27 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 @[BGP/170] 00:00:04, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/1/0.32771, Push 72
                    [BGP/170] 00:00:52, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 52
                   #[Multipath/255] 00:00:04, metric 100, metric2 0
                       via gr-2/1/0.32771, Push 72
                     > via gr-2/2/0.32771, Push 52
----
////

----
labroot@camaro# run show route table k8s-test.inet.0 101.101.101.252/32
Jun 26 20:09:21

k8s-test.inet.0: 27 destinations, 54 routes (27 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 @[BGP/170] 00:00:04, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 72
                    [BGP/170] 00:00:52, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 52
                   #[Multipath/255] 00:00:04, metric 100, metric2 0
                       via gr-2/3/0.32771, Push 72
                     > via gr-2/2/0.32771, Push 52
----

a `Multipath` with both GRE interface will be added under the FIP prefix, the
forwarding table reflects the same:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 01:12:36
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination        Type RtRef Next hop    Type Index    NhRef Netif
101.101.101.252/32 user     0             ulst  1048601     2
                                          indr  1048597     2
                                         Push 26     1272     2 gr-2/3/0.32771
                                          indr  1048600     2
                                         Push 26     1277     2 gr-2/2/0.32771
----

now try to pull the webpage from Internet host multiple times with `curl` or web
browser, we see the random result - both backend pod get the request and
responses back.

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = webserver-846c9ccb8b-xkjpw

[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = webserver-846c9ccb8b-r9zdt
----

the end to end packet flow is illustrated here:

.loadbalancer service ECMP
//image::https://user-images.githubusercontent.com/2038044/60763675-8e2dd000-a047-11e9-91a6-5fb1319517dc.png[]
//image::https://user-images.githubusercontent.com/2038044/64746240-da2e4280-d4d8-11e9-862e-2d68166dced3.png[]
//image::https://user-images.githubusercontent.com/2038044/64795236-c2dc6d00-d54b-11e9-9074-8076978ed18d.png[]
image::svc-lb-ecmp.drawio.png[]

== Contrail Ingress

////
in chapter 3 we've learned that Ingress maps URLs to services with `rules`. this
makes this Ingress section a little bit easier. we don't need to explain
everything that happens in Ingress. instead, we can focus on the Ingress
external IP exposure and service mapping. the rest part of the story is all
about service to backend mapping which we've examined a lot. 

in this chapter we'll introduce details of ingress workflow in contrail
implementation, then we'll use a few test cases to demonstrate and verify how
ingress works exactly in contrail environment
////

in chapter 3 we've learned Ingress basis, the relation to service, Ingress types
and the yaml file of each type.

in this chapter we'll introduce details of ingress workflow in contrail
implementation, then we'll use a few test cases to demonstrate and verify how
ingress works exactly in contrail environment.

=== Contrail Ingress Loadbalancer

like contrail's `service` implementation, contrail `Ingress` is also implemented
through loadbalancer, but with a different `loadbalancer_provider` attribute.
accordingly `contrail-svc-monitor` component takes different actions to
implement `Ingress` in contrail environment.

Remember in "Contrail-Kubernetes architecture" section we gave the "object
mapping" between kubernetes and contrail. in that section you've learned
kubernetes `service` maps to `ECMP loadbalancer (native)` and `Ingress` maps to
`Haproxy loadbalancer`. 

in `service` section when we were exploring the loadbalancer and the relevant
objects (`listener`, `pool`, and `member`), we noticed the loadbalancer's
`loadbalancer_provider` type is `native`. 

        "loadbalancer_provider": "native",

in this section we'll see `loadbalancer_provider` type is `opencontrail`
for Ingress's `loadbalancer`. we'll also look into the similarities and differences
between `service` loadbalancer and `Ingress` loadbalancer.

=== Contrail Ingress Workflow

When an `Ingress` is configured in contrail kubernetes environment, the event
will be noticed by other system components, and a lot of actions will be
triggered.  the deep level implementation is out of the scope of this book, but
in a high level here is the workflow:

. `contrail-kube-manager` keeps listening to the events of`kube-apiserver`
. user creates an `ingress` object (rules)
. `contrail-kube-manager` gets the event from `kube-apiserver`
. `contrail-kube-manager` creates a `loadbalancer` object in contrail
  DB, and set `loadbalancer_provider` type as `opencontrail` for ingress
  (where as it is `native` for `service`).
. As mentioned earlier `contrail-service-monitor` component sees the `loadbalancer`
  creation event, based on `loadbalancer_provider` type, it invokes registered
  loadbalancer driver for the specified `loadbalancer_provider` type:
  - if the `loadbalancer_provider` type is `native`, It will invoke ECMP
    loadbalancer driver for ECMP loadbalancing which we've learned in previous
    section.
  - if the `loadbalancer_provider` type is `opencontrail`, It will invoke
    haproxy loadbalancer driver which triggers haproxy processes to be launched in
    kubernetes nodes.

as you can see, contrail implements `Ingress` with haproxy loadbalancer, this is
what you've read in the section of "contrail kubernetes object mapping".  in
chapter 3, we've talked about "Ingress controller" , and multiple "Ingress
Controllers" can coexist in contrail. in contrail environment the
`contrail-kube-manager` plays `Ingress controller` role. it reads the Ingress
rules that user input and programs them into the loadbalancer.  furthermore:

* for each `Ingress` object, one loadbalancer will be created
* two haproxy processes will be created for `Ingress`, and they are working in
  "active-standby" mode:
  - one compute node runs the "active" haproxy process
  - the other compute node runs the "standby" haproxy process
* both `haproxy` processes are programmed with appropriate configuration, based
  on the rules defined in Ingress object.




=== Contrail Ingress Traffic Flow

client request, as a type of `overlay` traffic, may come from two sources
depending on who initiates the request:

* internal request: requests coming from another pod inside of the cluster
* external request: requests coming from an Internet host outside of the cluster

the only difference between the two, is how the traffic hit the "active"
haproxy. 

an Ingress will be allocated 2 IPs: 

* cluster-internal virtual IP
* external virtual IP
//, contrail implement this with FIP

here is the traffic flow for client request:

.client to Ingress podIP
. for internal request it hits Ingress's "internal" VIP directly. 
. for external request it first hits Ingress's "external" VIP - the FIP, which
  is the one exposed to external, and that is the time when NAT starts to play
  as we've explained in `FIP` section. after NAT processing, traffic is forwarded
  to internal Ingress VIP.

[start=3]
.Ingress podIP to backend service
. from this moment on, both type of requests is processed exactly the same way.
. the requests will be "proxied" to the corresponding service IP. 

[start=5]
.backend service to backend pod
. based on the backend pods' availability, it will be sent to the node where one
  of the backend pods are located and reaches the target pods eventually. 
. In the case that the backend pods are running in a different compute node than
  the one running active haproxy, a MPLS over UDP tunnel is created between the
  two compute node.

here is the end to end service request flow when accessing from a pod in the
cluster:

//TODO: lost the drawing, need to redraw, and give text explain
.Ingress traffic flow: access from internal
//image::https://user-images.githubusercontent.com/2038044/61061849-0b9c6c00-a3cb-11e9-8788-cb1c1dedafc4.png[]
//image::https://user-images.githubusercontent.com/2038044/64801404-7185ab00-d556-11e9-8d2b-37555ace5f01.png[]
image::ingress-detail-vh-internal.drawio.png[]

here is the end to end service request flow when accessing from Internet host:

.Ingress traffic flow: access from external
//image::https://user-images.githubusercontent.com/2038044/60410376-09017180-9b96-11e9-927e-4cf1d98f2cef.png[]
//image::https://user-images.githubusercontent.com/2038044/61061268-eb1fe200-a3c9-11e9-9d36-191955b766e1.png[]
//image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]
//image::https://user-images.githubusercontent.com/2038044/64834385-a40fc200-d5af-11e9-85bb-f6e33e13f4e9.png[]
image::ingress-detail-vh-external.drawio.png[]

////
. the "haproxy driver" will create a service instance (SI) with
  `haproxy-loadbalancer` type of template applied.
. the SI will has a "port tuple" linked to a linux netns VM
. the linux netns VM VM has its VMI, and a reference to an instance-ip
. `contrail-svc-monitor` launches the HAProxy process, with appropriate
  configuration, based on the ingress rules defined in the yaml file.

Whenever an ingress is configured in kubernetes, `contrail-kube-manager` that is
watching the kube-apiserver get the events and creates an loadbalancer object in
contrail-controller.  `contrail-svc-monitor` component listens for the load
balancer objects and takes a different action based on its
`loadbalancer_provider` attribute. when it sees `loadbalancer_provider`
attribute being `opencontrail`, it launches two haproxy processes, each in a
seperate compute node. both `haproxy` processes are programmed with appropriate
configuration based on the ingress rules you defined. the two haproxy processes
work in "active-standby" mode. 

contrail Ingress is also implemented through loadbalancer (like service), but
Ingress's loadbalancer is with a different `loadbalancer_provider` attribute,
which makes `contrail-svc-monitor` takes a different action than what it does
for service. now it is the time to tell that the `loadbalancer_provider` is
`opencontrail`, and accordingly the `contrail-svc-monitor` action is to launch a
haproxy process running with ingress rules in its configuration file. this
basically explains what we see now.

that is only a high level overview about the contrail's implementation of
ingress. 
in fact that for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

this is how it works:

////

contrail supports all 3 types of ingress:

* http-based single-service ingress, 
* simple-fanout ingress
* name-based virtual hosting ingress.

we'll look into each type of ingress.

=== Single Service Ingress

single service Ingress is the most basic form of Ingress. it does not define any
rules and its main usage is to expose service to the outside world. it will
proxy all incoming service request to the same "single" backend service.

    www.juniper.net --|                 |
    www.cisco.com   --|  101.101.101.1  |-> webservice
    www.google.com  --|                 |

to demonstrate `single service` type of Ingress, the objects that we need to
create are:

* an `Ingress` object that defines the backend service
* a backend service object
* at least one backend pod for the service

////
additionally, a "client" pod is needed to test the ingress from inside of the
cluster. we can use the same `client` pod we've used in earlier examples as
cluster-internal client.

besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` Objects Definition

===== `Ingress` Definition

in our single service ingress test lab, we want to achieve this goal:

* request toward any URLs will be directed to `webservice-1` with `servicePort`
  8888

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
----

this does not look anything fancy. basically in this `single service Ingress`
there is nothing else but a reference to one "single service" `webserver-1` as
its "backend". all HTTP request will be dispatched to this service, and from
there the request will reach a backend pod. next we'll look at the backend
service.

===== Backend `service` Definition

we can use exactly the same service as introduced in `service` example. 

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  #type: LoadBalancer
----

NOTE: the service `type` is optional. with `Ingress`, `service` does not need to
be exposed to external directly anymore. therefore `LoadBalancer` type of
service is not required. 

===== Backend `pod` Definition

same as in `service` example, we can use exactly the same `webserver` deployment
to launch backend pods:

----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver
  labels:
    app: webserver
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

===== An "all in one" Yaml File

as usual, we can create an individual yaml file for each of the objects. but
considering in `Ingress`, these objects always need to be created and removed
together, it is better to "merge" definitions of all these objects into one yaml
file. yaml syntax supports this by using a "document delimitor", a `---` line
between each object definition. 

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-ss
spec:
  backend:
    serviceName: service-web-clusterip
    servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  #type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rc-webserver
  selector:
    matchLabels:
      app: webserver
spec:
  replicas: 1
  selector:
    app: webserver
  template:
    metadata:
      name: webserver
      labels:
        app: webserver
    spec:
      containers:
      - name: webserver
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

////
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
////
the benefits of this all-in-one yaml file are:

* you can create/update all objects in the yaml file in one go, using just one `kubectl
  apply` command
* similarly, if anything goes wrong and you need to clean up, you can delete
  all objects created with the yaml file in one `kubectl delete` command
* whenever needed, you can still delete each individual objects
  independently, by giving the object name

TIP: During test process you may need to create and delete all objects as a
whole very often, grouping multiple objects in one yaml file can be very
convenient.

===== Deploy the Single Service Ingress

before applying the yaml file to get all objects created, let's take a quick
look at our two nodes. we want to see if there is any `haproxy` process running
without Ingress, so later after we deploy Ingress we can compare:

----
$ ps aux | grep haproxy
$ 
----

So the answer is no in both node. haproxy will be created only after we create
`Ingress` and the corresponding loadbalancer object is seen by
`contrail-service-monitor`.  we'll check this again after we create an
`Ingress`.

----
$ kubectl apply -f ingress/ingress-single-service.yaml
ingress.extensions/ingress-ss created
service/service-web-clusterip created
replicationcontroller/rc-webserver created
----

the Ingress, one service and one Deployment object are now created.

==== `Ingress` Post Examination

===== Ingress Object

let's start to examine the Ingress object.

----
$ kubectl get ingresses.extensions -o wide
NAME         HOSTS   ADDRESS                       PORTS   AGE
ingress-ss   *       10.47.255.238,101.101.101.1   80      29m

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1", "kind":"Ingress",
        "metadata":{"annotations":{},"name":"ingress-ss","namespace":"ns-user-1"},
        "spec":{"backend":{"serviceName":"webservice-1", "servicePort":80}}}
    creationTimestamp: 2019-07-18T04:06:29Z
    generation: 1
    name: ingress-ss
    namespace: ns-user-1
    resourceVersion: "845969"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-ss
    uid: 6b48bd8f-a911-11e9-8112-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 80
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

as expected, the backend service is applied to the Ingress properly. In this
`single service Ingress` there is no explicit rules defined to map a certain URL
to a different service - all HTTP requests will be dispatched to the same
backend service.

[TIP]
====
in the `items` -> `metadata` -> `annotations` ->
`kubectl.kubernetes.io/last-applied-configuration` section of the output:

    {"apiVersion":"extensions/v1beta1", "kind":"Ingress",
    "metadata":{"annotations":{},"name":"ingress-ss","namespace":"ns-user-1"},
    "spec":{"backend":{"serviceName":"webservice-1", "servicePort":80}}}

it actually contains the configuration information that you gave. can format it
(with JSON formatting tool like python `json.tool` module) to get a better view:

    {
        "apiVersion": "extensions/v1beta1",
        "kind": "Ingress",
        "metadata": {
            "annotations": {},
            "name": "ingress-ss",
            "namespace": "ns-user-1"
        },
        "spec": {
            "backend": {
                "serviceName": "webservice-1",
                "servicePort": 80
            }
        }
    }

you can do same formatting for all other objects to make it more readable.

====

what may confuse you is the two IP addresses shown here: 

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

we've seen these two subnets in service examples:

* `10.47.255.x` is an cluster-internal `podIP` allocated from the pod's default
  subnet
* `101.101.101.x` is the public `FIP` associated with an internal IP.

but the question is why an Ingress even requires a `podIP` and `FIP`?

////
this is the IP addresses allocated to the haproxy "virtual machine". 
what is a haproxy "virtual machine" anyway? isn't it just a process running in
the compute node? or, does compute node spawned some hidden VMs behind the
scene? 
////

let's hold the answer for now and continue to check service and pod
object created from the all-in-one yaml file. we'll come back to this question
shortly.

===== Service Objects

----
$ kubectl get svc -o wide
NAME                   TYPE       CLUSTER-IP    EXTERNAL-IP  PORT(S)   AGE  SELECTOR
service-web-clusterip  ClusterIP  10.97.226.91  <none>       8888/TCP  28m  app=webserver
----

the service is also created and allocated a clusterIP. we've seen
this before and it looks nothing special. now look at the pods.

===== Backend and Client Pod

----
$ kubectl get pod -o wide --show-labels
NAME                        READY  STATUS   ... IP             NODE     ... LABELS
client                      1/1    Running  ... 10.47.255.237  cent222  ... app=client
webserver-846c9ccb8b-9nfdx  1/1    Running  ... 10.47.255.236  cent333  ... app=webserver
----

everything looks fine. there is a backend pod running for the service. we have
learned how selector and label works in service-pod associations so there is
nothing new here. next we'll examine the haproxy and try to make some sense out
of the 2 IPs allocated to Ingress object.

===== Haproxy Processes

earlier before the Ingress is created, we were looking for haproxy process in
node but could not see anything. let's check it again and see if any magic
happens:

.node `cent222`

----
$ ps aux | grep haproxy
188  23465  0.0  0.0  55440  852  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  23447
----

.node `cent333`

----
$ ps aux | grep haproxy
188   16335  0.0  0.0  55440  2892  ?  Ss  00:58  0:00  haproxy  
  -f  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf  
  -p  /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid  
  -sf  16317
----

right after ingress got created, we see a haproxy process created in each of our
two nodes!

remember earlier when we talk about ingress contrail implementation, we've said
contrail `Ingress` is also implemented through loadbalancer (just like
`service`). Since Ingress's `loadbalancer_provider` type is `opencontrail`,
`'contrail-svc-monitor` invokes haproxy loadbalancer driver. The haproxy driver
generates required haproxy configuration for the ingress rules and triggers
haproxy processes to be launched (in active-standby mode) with the generated
configuration in kubernetes nodes.

////
that is only a high level overview about the contrail's implementation of
ingress. in fact the for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will generate a service-instance (SI).
next we'll explore the objects in a little bit more details.

----
$ ps aux | grep haproxy
188      16335  0.0  0.0  55440  2892 ?        Ss   00:58   0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317
root     18937  0.0  0.0 112716   984 pts/0    S+   01:21   0:00 grep --color=auto haproxy

$ pstree -lnaps 16335
systemd,1 --switched-root --system --deserialize 22
  └─dockerd,6268
      └─docker-containe,6756 --config /var/run/docker/containerd/containerd.toml
          └─docker-containe,3114 -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/5e9c1c3a14cf7e2d5dca2512784b227808890b4d260c9badef9b8aab8aaaa76b -address /var/run/docker/containerd/docker-containerd.sock -containerd-binary /usr/bin/docker-containerd -runtime-root /var/run/docker/runtime-runc
              └─haproxy,16335 -f /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.conf -p /var/lib/contrail/loadbalancer/haproxy/5be035d8-a918-11e9-8112-0050569e6cfc/haproxy.pid -sf 16317

$ docker ps -a | grep 5e9c
5e9c1c3a14cf        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest         "/entrypoint.sh /usr…"   5 weeks ago         Up 2 weeks                                   vrouter_vrouter-agent_1

$ docker exec -it vrouter_vrouter-agent_1 ps ef
  PID TTY      STAT   TIME COMMAND
17141 pts/0    Ssl+ 233:02 /usr/bin/python /usr/bin/contrail-nodemgr --nodetype=
16837 pts/0    Ss     0:00 -bash USER=root LOGNAME=root HOME=/root PATH=/usr/loc
17090 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
17279 pts/0    T      0:00  \_ less _fzf_orig_completion_tee=complete -F %s tee
19281 pts/0    Sl+    0:00  \_ docker exec -it vrouter_vrouter-agent_1 ps ef _fz
19297 pts/2    Rs+    0:00 ps ef PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/
16646 pts/1    Ss+    0:00 bash PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/u
 3130 pts/0    Ss+    0:00 /bin/bash /entrypoint.sh /usr/bin/contrail-vrouter-ag
 3307 pts/0    Sl+  358:06  \_ /usr/bin/contrail-vrouter-agent HOSTNAME=cent333
 5675 tty1     Ss+    0:00 /sbin/agetty --noclear tty1 linux LANG= PATH=/usr/loc

$ docker exec -it vrouter_vrouter-agent_1 ps 16335
  PID TTY      STAT   TIME COMMAND
16335 ?        Ss     0:00 haproxy -f /var/lib/contrail/loadbalancer/haproxy/5be
----
////


===== Ingress Loadbalancer Objects

////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

we've mentioned "Ingress loadbalancer" for a few time but we haven't looked into
it yet. In `service` section, we've looked into service loadbalancer object in
UI and we inspected some details about the object data structrure. now after we
created Ingress object, let's check the list of loadbalancers object again and
see what Ingress brings in here.

.loadbalancers (configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]

2 loadbalancers are generated after we applied the all-in-one yaml file.

* loadbalancer `ns-user-1__ingress-ss` for Ingress `ingress-ss`
* loadbalancer `ns-user-1__webservice-clusterip` for service `webserver-clusterip`

we've learned the service loadbalancer object previously, if you expand the
service you will see more details, but nothing would surprise us now.

.service loadbalancer object (click the triangle in the left of the loadbalancer name)
//image::https://user-images.githubusercontent.com/2038044/61049744-409cc480-a3b3-11e9-8a8e-5cdff7e6a931.png[]
//image::https://user-images.githubusercontent.com/2038044/61050199-64143f00-a3b4-11e9-9f7d-339775a3ae0e.png[]
image::https://user-images.githubusercontent.com/2038044/61433906-8bae5e80-a902-11e9-8039-fc5c5414c15a.png[]

as expected, the service loadbalancer has a ClusterIP, and a listener object
that is listening on port 8888. one thing we want to highlight here again is the
`loadbalancer_provider`. the type is "native", so the action
`contrail-svc-monitor` takes is layer 4 (application layer) ECMP process, which
we've explored a lot in service section. now let's expand Ingress loadbalancer
and look at the details.

.ingress loadbalancer object 
//image::https://user-images.githubusercontent.com/2038044/61021789-f98ddf80-a370-11e9-9cce-30a0c2671bc2.png[]
//image::https://user-images.githubusercontent.com/2038044/61434308-97e6eb80-a903-11e9-8a34-58f4bbaddf30.png[]
image::https://user-images.githubusercontent.com/2038044/64143219-0fea7180-cddd-11e9-9d93-28e08af1f6f2.png[]

some highlights in the figure:

////
* `loadbalancer_provider` is `opencontrail`
* Ingress loadbalancer has a reference to a `service-instance` (SI) object
* `SI` object has a property `ha_mode` set to `active-standby`
* `SI` object has interface IPs same as those in loadbalancer, hence you can see
  the same IP `10.47.255.238`
////
* `loadbalancer_provider` is `opencontrail`
* Ingress loadbalancer has a reference to a `virtual-machine-interface` (VMI)
  object
* the `VMI` object is referred by an `instance-ip` object with an (fixed) IP
  `10.47.255.238` and a `floating-ip` object with an (floating) IP
  `101.101.101.1`

at this moment,  we can explain the Ingress IP `10.47.255.248` seen in ingress.
basically:

* it is an cluster-internal IP address allocated from the default pod network as
  loadbalancer vip
* it is the frontend IP that the Ingress loadbalancer will listen for HTTP
  requests
* it is also what the public FIP `101.101.101.1` maps to with NAT

TIP: in this book we'll refer this private IP with different names
interchangeably: "Ingress Internal IP", "Ingress internal VIP", "Ingress private
IP", "Ingress loadbalancer interface IP", etc.  to differentiate it from the
Ingress public FIP, we can also name it as "Ingress podIP" since the internal vip
is allocated from the pod-network. similarly we'll refer the Ingress public FIP
as "Ingress external IP".

Now to compare the different purposes of these two IPs:

* Ingress `podIP` is the VIP facing other pods inside of the cluster. To reach
  Ingress from inside of the cluster, requests coming from other pods will have
  their destination IP set to Ingress `podIP`.
* Ingress `FIP` is VIP facing "Internet host" outside world. To reach Ingress
  from outside of the cluster, requests coming from Internet hosts need to have
  their destinations IP set to Ingress FIP.  when node receives traffic destined
  to the Ingress FIP from outside of the cluster, vrouter will translate it into
  the Ingress `podIP`

****
the detail Ingress loadbalancer object implementation refers to a SI (service
instance), and the SI again includes other data structure or reference to other
objects (VM, VMI, etc). overall it is more complicated and involves more details
than what we've covered and it is hard to put everything in this book. we've
tailored the details into a high level overview so that important concepts like
haproxy and the two Ingress IPes can be understood.
****

Once a HTTP/HTTPS request arrives to the Ingress `podIP`, from internal or
external, Ingress loadbalancer will do HTTP/HTTPS proxy operation through
haproxy process, and dispatch the requests towards the service and eventually to
the backend pod.

we've seen the haproxy process is running, to examine more details of this proxy
operation, next we can further check its configuration file for the running
parameters details.
//configuration file and confirm the ingress rules are programmed properly.

===== `haproxy.conf` File

in each (compute) node, under `/var/lib/contrail/loadbalancer/haproxy/` folder
there will be a subfolder for each loadbalancer uuid. the file structure looks
like this:

----
  8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  ├── haproxy.conf
  ├── haproxy.pid
  └── haproxy.sock
----

you can check haproxy.conf file for the haproxy configuration:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ......
        ulimit-n 200000
        maxconn 65000
        ......
        stats socket
        /var/lib/contrail/loadbalancer/haproxy/6b48bd8f-a911-11e9-8112-0050569e6cfc/haproxy.sock
            mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend f3a7a6a6-5c6d-4f78-81fb-86f6f1b361cf
        option tcplog
        bind 10.47.255.238:80                                   #<---
        mode http                                               #<---
        option forwardfor
        default_backend b45fb570-bec5-4208-93c9-ba58c3a55936    #<---

backend b45fb570-bec5-4208-93c9-ba58c3a55936                    #<---
        mode http                                               #<---
        balance roundrobin
        option forwardfor
        server 4c3031bb-e2bb-4727-a1c7-95afc580bc77 10.111.216.190:80 weight 1
                                                    ^^^^^^^^^^^^^^^^^
----

the configuration is simple, and here is the illustration of it:

.single service Ingress 
//image::https://user-images.githubusercontent.com/2038044/61689786-db6f9a00-acf5-11e9-9625-4ba1e570d354.png[]
//image::https://user-images.githubusercontent.com/2038044/63656134-88f32480-c75e-11e9-97e9-5a84ffa874cd.png[]
image::ingress-terse-ss.drawio.png[]

* the haproxy `frontend` represents the "frontend" of an Ingress, facing clients
* the haproxy `backend` represents the "backend" of an Ingress, facing services.
* the haproxy `frontend` defines a `bind` to the Ingress podIP and `mode` `http`.
  these knobs indicate what the frontend is listening.  
* the haproxy `backend` section defines the `server`, which is backend `service`
  in our case. it has a format of `serviceIP:servicePort`, which is the exact
  `service` object we've created using the all-in-one yaml file.
* the `default_backend` in `frontend` section defines which backend is the
  "default": it will be used when a haproxy receives a URL request that has no
  explicit match anywhere else in the `frontend` section. in this case the
  `default_backend` refers to the only `backend` service `10.111.216.190:80`.
  this is due to the fact that there is no `rules` defined in `single service
  Ingress`, so all HTTP requests will go to the same default_backend service,
  regardless of what URL the client sent.

NOTE: later in `simple fanout Ingress` and `name-based virtual hosting Ingress`
examples, we will see another type of configuration statement
`use_backend...if...` that can be used to force each URL to go to a different
backend.

through this configuration, the haproxy implemented our single service Ingress.

===== Gateway Router VRF Table

we've explored a lot inside of the cluster. now let's look at the gateway
router's VRF table. 

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61

----

Same as in service example, from outside of the cluster, only FIP is visible.
`detail` version of it conveys more information:

----
labroot@camaro> show route table k8s-test 101.101.101.1 detail

k8s-test 24 destinations, 49 routes (24 active, 0 holddown, 0 hidden)
101.101.101.1/32 (1 entry, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:5     #<---
                ......
                Source: 10.169.25.19
                Next hop: via gr-2/2/0.32771, selected
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                Indirect next hop: 0x900d320 1048597 INH Session ID: 0x6f9
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 34         Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                Import Accepted
                VPN Label: 61
                Localpref: 200                          #<---
                Router ID: 10.169.25.19
----

////
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1040000
////

//TODO: add diagram

* through XMPP, vrouter advertises the FIP prefix to contrail controller.
  at least 2 pieces of information from the output indicates who represents the
  FIP in this example - node `cent222`:
  - `Protocol next hop` being `10.169.25.20`
  - `Route Distinguisher` being `10.169.25.20:5`
* through MP-BGP, contrail controller "reflects" the FIP prefix to the gateway
  router, `Source: 10.169.25.19` indicates this fact.

so it looks `cent222` is "selected" to be the active haproxy node, and the other
node `cent333` is the standby one. therefore you should expect client request
coming from Internet host goes to node `cent222` first. of course, the overlay
traffic will be carried in MPLS over GRE tunnel, same as what you've seen from
service example. 

the FIP advertisement towards gateway router is exactly the same in all types of
Ingresses.

****
another fact that we've skipped on purpose is the different "local
preference" value used by the active and standby node when advertising FIP
prefix. saying that will involve other complex topics like the active node
selection algorithm and so on. but it is worth to undertand from high level:

both nodes have loadbalancer and haproxy running so both will advertise the FIP
prefix `101.101.101.1` to gateway router. however, they are advertised with
different local preference value. the "Active" node advertise with a value of
`200` and the "standby" node with `100`. contrail controller have both routes
from the 2 nodes, but only the "winning" one will be advertised to the gateway
router. that is why the "other" BGP route is dropped and only one is
displayed.  `Localpref` being `200` proves it is coming from the active compute
node. this applies to both Ingress public FIP route and internal VIP route
advertisement.
****

===== `Ingress` Verification: Internal

we've explored a lot about ingress loadbalancer and the related service, pod
objects, etc. now it is time to verify the "end-to-end" test result. since the
`Ingress` serves both inside and outside of the cluster, our verification will
start from the client pod inside of cluster, then from an Internet host outside
of it.

.from inside of cluster

----
$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx

$ kubectl exec -it client -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx

$ kubectl exec -it client -- \
curl -H 'Host:www.google.com' 10.47.255.238 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
$ kubectl exec -it client -- \
curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
----

we still use the `curl` command to trigger HTTP requests towards the ingress's
private IP. the return proves our `Ingress` works: requests towards different
URLs are all proxied to the same backend pods, through the default backend
services `service-web-clusterip`. 

in the fourth request we didn't give a URL via `-H`, `curl` will fill `host`
with the request IP address - `10.47.255.238` in this test, again it goes to the
same backend pod and get the same returned response.

NOTE: The `-H` option is important in Ingress test with `curl`. it carries the
full URL in HTTP payload that the Ingress loadbalancer is waiting for. without
it the HTTP header will carry `Host: 10.47.255.238`, which has no matching rule,
so it will be treated same as with a unknown URL.

===== `Ingress` Verification: External (Internet host)

the more exciting part of the test is to visit the URLs from external. overall
we hope `Ingress` meant to expose services to the Internet host, even though it
does not have to. 

to make sure the URL resolves to the right FIP address, we need to update
`/etc/hosts` file by adding one line in the end - you probably don't want to
just end up with a nice webpage from `Juniper`/`cisco`'s official website as
your test result.

----
# echo "101.101.101.1  www.juniper.net www.cisco.com www.google.com" >> /etc/hosts
# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
101.101.101.1  www.juniper.net www.cisco.com www.google.com     #<---
----

now, from internet host's "desktop", we launch chrome browser, and input one of
the 3 URLs: `www.juniper.net`, `www.cisco.com` or `www.google.com`. By keep
refreshing the pages we can confirm all HTTP request is returned by the same
backend pod.

//image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

.access `www.juniper.net` from Internet host
//image::https://user-images.githubusercontent.com/2038044/63735716-adc6c500-c84e-11e9-9267-0112916c7ea8.png[]
image::https://user-images.githubusercontent.com/2038044/63737822-a951da80-c855-11e9-99ef-1c769f71d8b6.png[]

same result can be seen from `curl` also. the command is exactly the same as
what we've seen when testing from a pod, except this time we send requests to
Ingress external FIP, instead of the Ingress internal podIP.

.from Internet host machine

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-9nfdx
----

everything works!

next we'll look at the second Ingress type `simple fanout Ingress`. before
movig forward, it is better to clean up everything.  now we can take advantage
of the all-in-one yaml file - everything can be cleared with one `kubectl
delete` command using the same all-in-one yaml file:

----
$ kubectl delete -f ingress/ingress-single-service.yaml
ingress.extensions "ingress-ss" deleted
service "webservice-1" deleted
replicationcontroller "Vwebserver-1-846c9ccb8b" deleted
----

=== Simple Fanout Ingress

//TODO: adjust the TOC, default backend
both `simple fanout Ingress` and `name-based virtual host Ingress` support "URL
routing", the only difference is the former is based on `path` and the latter is
based on `host`.

with `simple fanout Ingress`, based on the URL path and rules, an ingress
loadbalancer directs traffic to different backend services.

    www.juniper.net/qa --|                 |-> webservice-1
                         |  101.101.101.1  |
    www.juniper.net/dev -|                 |-> webservice-2

to demonstrate `simple fan-out` type of Ingress, the objects that we need to
create are:

* an `Ingress` object: defines the rules, mapping 2 paths to 2
  backend services
* 2 backend services objects
* each service requires at least one pod as backend

we use the same `client` pod as cluster-internal client we've used in previous
examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` Objects Definition

===== `ingress` Definition

in our `simple fanout Ingress` test lab, we want to achieve these goals for host
`www.juniper.net`:

* request toward path `/dev` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward path `/qa` will be directed to a service `webservice-2`
  with `servicePort` 8888

here is the corresponding yaml file to implement these goals:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
----

in contrast to `single service Ingress`, in `simple fanout Ingress` object (and
"name-based virtual host Ingress") we see "rules" defined - here it is the
mappings from multiple "paths" to different backend services. 

===== backend `service` definition

since we defined 2 rules each for a `path`, we need two services accordingly. we 
can "clone" the previous service in `single service Ingress` example and
just change the service's name and selector to generate the second service.
e.g.: this is definition of `webservice-1` and `webservice-2` service.

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
----

----
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
----

===== backend `pod` definition

because we have 2 backend services now, apparently we also need at least two
backend pods each with a label matching to a service. we can clone the previous
`Deployment` into two and just change the name and label of the second
`Deployment`.

There are the definition of the `Deployment`:

.`Deployment` for webserver-1
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----


.`Deployment` for webserver-2
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

===== deploy `simple fanout Ingress`

same as in `single service Ingress`, we put everything together to get an
"all-in-one" yaml file to test `simple fanout Ingress`:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-sf
spec:
  rules:
  - host: www.juniper.net
    http:
      paths:
      - path: /dev
        backend:
          serviceName: webservice-1
          servicePort: 8888
      - path: /qa
        backend:
          serviceName: webservice-2
          servicePort: 8888
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
  #type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file to create all objects

----
$ kubectl apply -f ingress/ingress-simple-fanout.yaml
ingress.extensions/ingress-sf created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the Ingress, two `service` and two `Deployment` objects are now created.

==== `Ingress` post examination

===== ingress objects and ingress loadbalancer

let's look at the kubernetes objects created from the all-in-one yaml file:

.ingress objects
----
$ kubectl get ingresses.extensions
NAME        HOSTS            ADDRESS                      PORTS  AGE
ingress-sf  www.juniper.net  10.47.255.238,101.101.101.1  80     7s

$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"ingress-sf","namespace":"ns-user-1"},"spec":{"rules":[{"host":"www.juniper.net","http":{"paths":[{"backend":{"serviceName":"webservice-1","servicePort":8888},"path":"/dev"},{"backend":{"serviceName":"webservice-2","servicePort":8888},"path":"/qa"}]}}]}}
    creationTimestamp: 2019-08-13T06:00:28Z
    generation: 1
    name: ingress-sf
    namespace: ns-user-1
    resourceVersion: "860530"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-sf
    uid: a6e801fd-bd8f-11e9-9072-0050569e6cfc
  spec:
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /dev
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /qa
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the "rules" are defined properly, within each rule there is a mapping from a
`path` to the corresponding `service`. we see same Ingress internal podIP and
external FIP as we've seen in the previous `single service Ingress` example:

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.238

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

That is why from gateway router's perspective, there is no differences between
all types of Ingress. in all cases a public FIP will be allocated to the Ingress
and it is advertised to the gateway router:

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61
----

now check backend services and pods:

.service objects
----
$ kubectl get svc -o wide
NAME          TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE    SELECTOR
webservice-1  ClusterIP  10.111.234.187  <none>       8888/TCP  4m46s  app=webserver-1
webservice-2  ClusterIP  10.97.77.82     <none>       8888/TCP  4m46s  app=webserver-2
----

////
.the real capture
----
$ kubectl get svc -o wide
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE   SELECTOR
webservice-1   ClusterIP   10.96.51.227    <none>        8888/TCP   85m   app=webserver-1
webservice-2   ClusterIP   10.100.156.38   <none>        8888/TCP   85m   app=webserver-2
----
////

.backend and client pod
----
$ kubectl get pod -o wide
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
client                        1/1    Running  ... 44d  10.47.255.237  cent222 ..
webserver-1-846c9ccb8b-wns77  1/1    Running  ... 13m  10.47.255.236  cent333 ..
webserver-2-846c9ccb8b-t75d8  1/1    Running  ... 13m  10.47.255.235  cent333 ..

$ kubectl get pod -o wide -l app=webserver-1
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
webserver-1-846c9ccb8b-wns77  1/1    Running  ... 156m 10.47.255.236  cent333 ..

$ kubectl get pod -o wide -l app=webserver-2
NAME                          READY  STATUS   ... AGE  IP             NODE    ..
Vwebserver-2-846c9ccb8b-t75d8  1/1    Running  ... 156m 10.47.255.235  cent333 ..
----

two services are created, each with a different clusterIP allocated.
for each service there is a backend pod. later when we verify Ingress from
client we'll see these podIPs in the returned web pages.

.contrail Ingress loadbalancer object
comparing with `single service Ingress`, in here the only difference is one more
`service` loadbalancer:

.`simple fanout Ingress` loadbalancers (UI: configuration > Networking > Floating IPs)
//image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]
//image::https://user-images.githubusercontent.com/2038044/61432850-aa5f2600-a8ff-11e9-9d9f-932a386bf81f.png[]
//image::https://user-images.githubusercontent.com/2038044/61433696-ff03a080-a901-11e9-96c1-3dfd4886c322.png[]
image::https://user-images.githubusercontent.com/2038044/61790253-befe5b00-ade4-11e9-8a97-40c7d924b7e6.png[]

totally 3 loadbalancers are generated in this test:

* loadbalancer `ns-user-1__ingress-sf` for Ingress `ingress-sf`
* loadbalancer `ns-user-1__webservice-1` for service `webserver-1`
* loadbalancer `ns-user-1__webservice-2` for service `webserver-2`

we won't explore the details of each objects again this time since we've
investigated the key parameters of `service` and `Ingress` loadbalancers in
`single service Ingress` - there is really nothing new here.

===== haproxy process and haproxy.cfg file

in `single service Ingress` example, we've demonstrated the two haproxy
processes invoked by `contrail-svc-monitor` when it sees `loadbalancer` appears
with `loadbalancer_provider` set to `opencontrail`. in the end of that example,
after we removed the `single service Ingress`, since there is no more `Ingress`
left in the cluster, the two haproxy processes will be killed.  now with a new
Ingress creation, two new haproxy processes are invoked again:

.node `cent222`

----
$ ps aux | grep haproxy
188   29706  0.0  0.0  55572   2940  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf 
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid 
    -sf  29688
----

.node `cent333`

----
[root@b4s42 ~]# ps aux | grep haproxy
188   1936  0.0  0.0  55572   896  ?      Ss  04:04  0:00  haproxy  
    -f /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.conf
    -p /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.pid  
    -sf  1864
----

what interests us is how the `simple fanout Ingress` "rules" are programmed in
the haproxy.conf file this time. let's look at the haproxy configuration file:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583
$ cat haproxy.conf
global
    daemon
    user haproxy
    group haproxy
    log /var/log/contrail/lbaas/haproxy.log.sock local0
    log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
    tune.ssl.default-dh-param 2048
    ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:......
    ulimit-n 200000
    maxconn 65000
    stats socket
        /var/lib/contrail/loadbalancer/haproxy/b32780cd-ae02-11e9-9c97-002590a54583/haproxy.sock
        mode 0666 level user

defaults
    log global
    retries 3
    option redispatch
    timeout connect 5000
    timeout client 300000
    timeout server 300000

frontend acd9cb38-30a7-4eb1-bb2e-f7691e312625
    option tcplog
    bind 10.47.255.238:80
    mode http
    option forwardfor
    acl 020e371c-e222-400f-b71f-5909c93132de_host hdr(host) -i www.juniper.net
    acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa
    use_backend 020e371c-e222-400f-b71f-5909c93132de if
        020e371c-e222-400f-b71f-5909c93132de_host
        020e371c-e222-400f-b71f-5909c93132de_path


    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_host hdr(host) -i www.juniper.net
    acl 46f7e7da-0769-4672-b916-21fdd15b9fad_path path /dev
    use_backend 46f7e7da-0769-4672-b916-21fdd15b9fad if
        46f7e7da-0769-4672-b916-21fdd15b9fad_host
        46f7e7da-0769-4672-b916-21fdd15b9fad_path


backend 020e371c-e222-400f-b71f-5909c93132de
    mode http
    balance roundrobin
    option forwardfor
    server c13b0d0d-6e4a-4830-bb46-2377ba4caf23 10.97.77.82:8888 weight 1

backend 46f7e7da-0769-4672-b916-21fdd15b9fad
    mode http
    balance roundrobin
    option forwardfor
    server d58689c2-9e59-494b-bffd-fb7a62b4e17f 10.111.234.187:8888 weight 1
----

NOTE: the configuration file is slightly formatted to make it fit to a page
width. 
//also we moved the default backend to the end.

the configuration looks a little bit more complicated than the one for `single
service Ingress`, but the most important part of it is looks pretty
straightforward.

* the haproxy `frontend` section: it now defines URLs. each URL is represented
  by a pair of `acl` statement, one for `host`, and the other for `path`. in a
  nutshell, `host` is the domain name and `path` is what follows the `host` in
  the URL string. here for `simple fanout Ingress` there are is a host
  `www.juniper.net` with two different paths: `\dev` and `\qa`.
* the haproxy `backend` section: now we see 2 of them. for each `path` there is
  a dedicated service.
* `use_backend...if...` command in `frontend` section: this statement declares
  the ingress rules: "if" the URL request includes a specified `path` that
  matches to what is programmed in one of the two ACLs pairs, "use" the
  corresponding "backend" (that is a service), to forward the traffic. 
  
for example, `acl 020e371c-e222-400f-b71f-5909c93132de_path path /qa` defines
path `/qa`. if the URL request contains such a path, haproxy will "use_backend"
`020e371c-e222-400f-b71f-5909c93132de`, which you can find in `backend` section.
The backend is a UUID referring to `server c13b0d0d-6e4a-4830-bb46-2377ba4caf23
10.97.77.82:8888 weight 1`, which essentially is a service. you can identify
this by looking at the `serviceIP:port`: `10.97.77.82:8888`.

////
* `default_backend` defines which backend is the "default": it will be used when
  a haproxy receives a URL request that is other than the two defined one
////

this configuration file can be illustrated in this figure:

.simple fanout Ingress
//image::https://user-images.githubusercontent.com/2038044/61764125-0f09fd00-ada6-11e9-8a30-61e3a2ed2db3.png[]
//image::https://user-images.githubusercontent.com/2038044/61790000-1cde7300-ade4-11e9-9a78-d1f5c13c7046.png[]
//image::https://user-images.githubusercontent.com/2038044/62929112-c8496a80-bd87-11e9-8b53-fdc85ca959d0.png[]
//image::https://user-images.githubusercontent.com/2038044/63776303-1e9dc980-c8af-11e9-8c25-cdbc7d6cc73a.png[]
image::ingress-terse-sf.drawio.png[]

with this `proxy.conf` file, the haproxy implements our `simple fanout Ingress`:

* if the full URL composes `host` "www.juniper.net" and `path` "/dev" , request
  will be dispatched to `webservice-1` (`10.111.234.187:8888`)
* if the full URL composes `host` "www.juniper.net" and `path` "/qa" , request
  will be dispatched to `webservice-2` (`10.97.77.82:8888`)
* for any other URLs the request will be dropped because there is no
  corresponding backend service defined for it.

NOTE: in practice we often need a the `default_backend` service to process all
those HTTP request with no matching URLs in the rules. we've seen it in the
previous examle of `single service Ingress`. later in `name-based virtual
hosting Ingress` section we'll combine the `use_backend` and `default_backend`
together to privde this type of flexibility.


==== `Ingress` verification: from internal

.from inside of the cluster

----
$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-1-846c9ccb8b-wns77

$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-t75d8

$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ kubectl exec -it client -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

////
----
$ kubectl exec -it client -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249

$ kubectl exec -it client -- curl -H 'Host:www.juniper.net' 10.47.255.250/qa
Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248

$ kubectl exec -it client -- curl -H 'Host:www.juniper.net' 10.47.255.250/abc
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

$ kubectl exec -it client -- curl -H 'Host:www.juniper.net' 10.47.255.250
<html><body><h1>503 Service Unavailable</h1>
No server is available to handle this request.
</body></html>

----
////

//again we use the `curl` command to trigger HTTP requests towards the ingress's loadbalancer IP. 
the return proves our `Ingress` works: the 2 requests towards
"/qa" and "/dev" paths are proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. 

the third request with a path `abc` composes a "unknown" URL which does not have
a matching service in `Ingress` configuration, so it won't be served. same for
the last 2 requests, without a path, or with a different Host, the URL
become unknown to our Ingress so it won't be served.

you may think that we should add more rules to include these scenarios. doing
that works fine but not scalable apparently - you can never cover all possible
paths and URLs that could come into your server. as we mentioned earlier, one
solution is to use `default_backend` service to process all "other" HTTP
requests.  we'll cover this in the next example.

////
same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. 

in our test, we have just one backend pod for each
service, so the podIP in returned webpage tells who is who. except in the second
test the returned podIP `10.47.255.235` represent `webservice-2`, all other three
tests returns podIP for `webservice-1`, as expected.
////

==== `Ingress` verification: from external (Internet host)

////
with chrome, this time we launch two chrome page side by side, and input URLs
`www.juniper.net/qa` and `www.juniper.net/dev`. By keep refreshing the 2 pages
we can confirm "qa" page is always returned by RC `rc-webserver-1` pod
`10.47.255.236`, "dev" page is always returned by RC `rc-webserver-2` pod
`10.47.255.235`. 

//TODO: capture
image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. 
////

to test `simple fanout Ingress` from outside of the cluster, the command is the
same as what we've seen when initiating the HTTP request from inside of a pod,
except this time we are initiating from an Internet host. we will send the HTTP
requests to the Ingress's public FIP, instead of its internal podIP.

from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1/qa | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-t75d8

$ curl -H 'Host:www.juniper.net' 101.101.101.1/dev | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                         Hostname = webserver-846c9ccb8b-wns77

$ curl -H 'Host:www.juniper.net' 101.101.101.1/ | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.juniper.net' 101.101.101.1/abc | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.

$ curl -H 'Host:www.cisco.com' 101.101.101.1/dev | w3m -T text/html | cat
503 Service Unavailable
No server is available to handle this request.
----

=== Virtual Hosting Ingress

`Virtual Hosting Ingress` support routing HTTP traffic to multiple
host names at the same IP address. based on the URL and rules, an Ingress
loadbalancer directs traffic to different backend services, and each service
direct traffic to its backend pods. 

    www.juniper.net --|                 |-> webservice-1
                      |  101.101.101.1  |
    www.cisco.com   --|                 |-> webservice-2

to demonstrate `virtual host` type of Ingress, the objects that we need to
create are same as previous `simple fanout Ingress`:

* an `Ingress` object: the rules, mapping 2 URLs to 2 backend services
* 2 backend services objects
* each service requires at least one pod as backend

//again we use the same `client` pod as cluster-internal HTTP client we've used in
//earlier examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `Ingress` objects definition

===== `ingress` definition

in our virtual host ingress test lab, we define the following rules:

* request toward URL `www.juniper.net` will be directed to a service `webservice-1`
  with `servicePort` 8888
* request toward URL `www.cisco.com` will be directed to a service `webservice-2`
  with `servicePort` 8888
* request toward any URLs other than these 2, will be directed to `webservice-1`
  with `servicePort` 8888. Effectively we want `webservice-1` to become the
  default backend service in here.

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
----

.backend `service` and `pod` definition

same exact service and Deployment definition that were used in `simple fanout
Ingress` can be used here.

===== an "all in one" yaml file

----
#ingress/ingress-virtual-host.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: webservice-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: webservice-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: webservice-2
              servicePort: 8888
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
---
apiVersion: v1
kind: Service
metadata:
  name: webservice-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: contrailk8sdayone/contrail-webserver
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

.`apply` the all-in-one yaml file to create Ingress and all necessary objects

----
$ kubectl apply -f ingress/ingress-virtual-host-test.yaml
ingress.extensions/ingress-vh created
service/webservice-1 created
service/webservice-2 created
deployment.extensions/webserver-1 created
deployment.extensions/webserver-2 created
----

the `Ingress`, two services and two `Deployment` objects are now created.

==== `Ingress` post examination

===== examine ingress objects

let's start to look at the `Ingress` object.

----
$ kubectl get ingresses.extensions -o wide
NAME        HOSTS                          ADDRESS                      PORTS  AGE
ingress-vh  www.juniper.net,www.cisco.com  10.47.255.248,101.101.101.1  80     8m27s
----

//the internal and external Ingress IP remains the same, but 
comparing with `simple fanout Ingress`, this time we see two hosts instead of
one. Each host represents a domain name.

----
$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    ......
    generation: 1
    name: ingress-vh
    namespace: ns-user-1
    resourceVersion: "830991"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-vh
    uid: 8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  spec:
    backend:
      serviceName: webservice-1
      servicePort: 8888
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: webservice-1
            servicePort: 8888
          path: /
    - host: www.cisco.net
      http:
        paths:
        - backend:
            serviceName: webservice-2
            servicePort: 8888
          path: /
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

the rules are defined properly, within each rule there is a mapping from a
`host` to the corresponding service. 

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

the services, pods and FIP prefix advertisement to gateway router behavior are
all exactly the same as those in `simple fanout Ingress`.

===== exploring Ingress loadbalancer objects

//TODO: should skip if simple fan-out Ingress can capture this.
////
.contrail object: SI, port tuple, VMI
image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]
////

3 loadbalancers are generated after we applied the all-in-one yaml file.

* 1 for Ingress
* 2 for services

loadbalancers created in this test is almost the same as the ones created in
`simple fanout Ingress` test:

.loadbalancers
image::https://user-images.githubusercontent.com/2038044/61021698-91d79480-a370-11e9-923d-674d8a7b348c.png[]

next we can check haproxy configuration file for `name-based virtual host
Ingress`.

===== examine `haproxy.conf` file

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ulimit-n 200000
        maxconn 65000
        stats socket /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.sock mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend acf8b96d-b322-4bc2-aa8e-0611baa43b9f

        option tcplog
        bind 10.47.255.248:80                   #<---Ingress loadbalancer podIP
        mode http
        option forwardfor

        #map www.juniper.net to backend "xxx4e6a681ec8e6", which maps to "webservice-1"
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host hdr(host) -i www.juniper.net
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path path /
        use_backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6 if
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host
            77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path

        #map URL www.cisco.net to backend "xxx44d1ca50a92f", which maps to "webservice-2"
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host hdr(host) -i www.cisco.net
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path path /
        use_backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f if
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host
            1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path

        #map other URLs, to default backend "xxx4e6a681ec8e6"
        default_backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46

backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6    #<---webservice-1
        mode http
        balance roundrobin
        option forwardfor
        server 33339e1c-5011-4f2e-a276-f8dd37c2cc51 10.101.158.92:8888 weight 1

backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f    #<---webservice-2
        mode http
        balance roundrobin
        option forwardfor
        server aa0cde60-2526-4437-b943-6f4eaa04bb05 10.104.4.232:8888 weight 1

backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46    #<---default
        mode http
        balance roundrobin
        option forwardfor
        server e8384ee4-7270-4272-b765-61488e1d3e9c 10.101.158.92:8888 weight 1
----

//NOTE: the config file is slightly formatted to make it fit to a page width.
//also we moved the default backend to the end.

here are the highlights:

* the haproxy `frontend` section defines each URL, or `host`, and its path. here
  the 2 hosts are `www.juniper.net` and `www.cisco.com`. both `path` is `/`. 
* the haproxy `backend` section defines the "servers", which is all `service` in
  our case. it has a format of `serviceIP:servicePort`, which is the 
  `service` we've created.
* `use_backend...if...` command in `frontend` section declares the ingress
  rules: `if` the request includes a specified URL and path, "use" the
  corresponding "backend" to forward the traffic
* `default_backend` defines the service that will act as the "default": it will
  be used when a haproxy receives a URL request that is has no explicit match in
  the defined rules.

this configuration file can be illustrated in this figure:

.virtual host Ingress
//image::https://user-images.githubusercontent.com/2038044/63788692-e785e300-c8c3-11e9-92be-a37ac7d83104.png[]
image::ingress-terse-vh.drawio.png[]

through this configuration, the haproxy implemented our ingress:

////
.haproxy frontend:

* 10.47.255.248:80 is the frontend IP and port facing clients

.haproxy backend:
////

* `www.juniper.net` and `/` composes the full URL, request will be dispatched to
  `webservice-1` (`10.101.158.92:8888`)
* `www.cisco.net` and `/` composes the full URL, request will be dispatched to
   `webservice-2` (`10.104.4.232:8888`)
* other URLs goes to default backend which is service `webservice-1`. 

Next we'll verify these behaviors.

==== `Ingress` verification: from internal

////
we've explored a lot about ingress configuration and objects examination, now it
is time to verify the test result. since the `Ingress` serves both inside and
outside of the cluster, our verification will start from the client pod
inside of cluster, then from the Internet host outside of it.
////

.from inside of cluster

----
$ kubectl exec -it client -- \
    curl -H 'Host:www.juniper.net' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg

$ kubectl exec -it client -- \
    curl -H 'Host:www.cisco.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-m2272

$ kubectl exec -it client -- \
    curl -H 'Host:www.google.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg

$ kubectl exec -it client -- \
    curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
----

//we still use the `curl` command to trigger HTTP requests towards the ingress's
//loadbalancer IP. 
the return proves our `Ingress` works: the 2 requests towards
"juniper" and "cisco" URL is proxied to 2 different backend pods, through 2
backend services `webservice-1` and `webservice-2` respectively. the third request
towards "google" is a "unknown" URL which does not have a matching service in
`Ingress` configuration, so it goes to the default backend service -
`webservice-1` and reaches the same backend pod.

same rule applies to the fourth request. without given a URL via `-H`, `curl`
will fill `host` with the request IP address, `10.47.255.238` in this test, and
since that "URL" does not have a defined backend service so the default backend
service will be used. in our test, for each service we use backend pods spawned
by same Deployment, so the podIP in returned webpage tells who is who. except in
the second test the returned podIP `10.47.255.235` represent `webservice-2`, all
other three tests returns podIP for `webservice-1`, as expected.

==== `Ingress` verification: from external (Internet host)

From internet host's "desktop", we launch two chrome page side by side, and
input URLs `www.juniper.net` and `www.cisco.com`. keep refreshing the 2 pages
we can confirm "juniper" page is always returned by Deployment `webserver-1` pod
`10.47.255.236`, "cisco" page is always returned by Deployment `webserver-2` pod
`10.47.255.235`. we launch a third chrome page and input `www.google.com`, we
see "google" page is returned by the same pod serving "Juniper" URL.

.`name based virtual hosting Ingress`: access `www.juniper.net` from Internet host
image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

same result can be seen from `curl` also. 

.from Internet host machine:

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = Vwebserver-2-846c9ccb8b-m2272

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = Vwebserver-1-846c9ccb8b-g65dg
----

=== Service vs Ingress Traffic Flow

in contrail, eventough both `service` and `Ingress` are implemented based via
`loadbalancers` (but with different `loadbalancer_provider` types). the forwarding
mode for `service` and `Ingress` are quite different. with `service` forwarding
it's "one hop" process: client sends request to the clusterIP or FIP, through NAT
the request reaches the destination backend pod; while with `Ingress` forwarding,
the traffic takes a "two hops" process to arrive the destination pod: the request
first goes to the active haproxy, which then start a HTTP/HTTPS level proxy procedure
and do the `service` forwarding to reach the final pod. NAT processing happens in both
forwarding process, since both Ingress and service public FIP implementation relies on
it. The next section will give a detailed view of packet flow for traffics in contrail.

== Packet Flow in Contrail: End to End View

so far we've looked at `floating IP`, `service`, and `Ingress` in details.  You
probably found that all these objects are related to each other in certain
aspects. in contrail, both `service` and `Ingress` are implemented based on
`loadbalancers` (but with different `loadbalancer_provider` types).
conceptually, `Ingress` is designed based on `service`.  VIP of both type of
loadbalancers are implemented based on `floating IP`.  


//source NAT and source PAT take place in ingress direction, destination NAT and
//destination PAT take places in egress direction

in order to undrestand the detail packet flow in contrail kubernetes
environment, let's examine the end to end HTTP request from external Internet
host to the destination pod in our `Ingress` lab setup. We'll examine the
forwarding state step by step, starting from Internet host, through gateway
router, then active haproxy, backend service and to the final destination pod. 

after this section, you will get a deep understanding to the packet flow and you
will be able to troubleshoot the forwarding plane problems in contrail
kubernetes environment. 

=== Setup and Utils/Tools

you've seen this figure in `Ingress` section:

.Ingress traffic flow: access from external
//image::https://user-images.githubusercontent.com/2038044/61061427-3f2ac680-a3ca-11e9-9364-f11bea477319.png[]
//image::https://user-images.githubusercontent.com/2038044/64834385-a40fc200-d5af-11e9-85bb-f6e33e13f4e9.png[]
image::ingress-detail-vh-external.drawio.png[]

earlier we looked at the external gateway router's VRF routing table and use the
`protocol next hop` information to find out which node get the packet from the
client. in practice, very often you need to find out the same from the cluster
and nodes themselves. a contrail cluster typically comes with a group of
built-in utils that you can use to inspect the packet flow and forwarding state.
in `service` section you've seen the usage of `flow`, `nh`, `vif`, etc. in this
section we'll revisit these utils and introduce some more useful utils/tools to
demonstrate additional information about the packet flow.


here are some of the available utils/tools that we can use for this test:

* on any linux machine: 
  - `curl` (with debug option), `telnet` as HTTP client tool
  - `tcpdump` and `wireshark` as packet capture tool
  - shell script can be used to automate command line tasks
* on vrouter: `flow`/`rt`/`nh`/`vif` and etc.

//we'll then conclude the ingress section by a figure showing the end to end flow.

.curl

//`curl` is a powerful tool and it has many options to support versatile useful
//features for testing HTTP protocol. 
one behavior in the `curl` tool implementation is that when it runs in a
shell terminal, it will always close the TCP session right after the HTTP response is
returned. although this is a safe and clean behavior in practice, it may bring
some difficulties to our test. in this lab we actually want to "hold" the TCP
connection so we can look into the details.  However, a TCP flow entry in
contrail `vrouter` is "bound" to the TCP connection - when TCP session closes
the flow will also be cleared.  the problem is, `curl` get its job done "too
fast". it establish the TCP connection, send HTTP request, get the response,
close the session right after. this process is too fast to allow us any time to
capture anything with the vrouter utilities (e.g. `flow` command). as soon as
you hit "enter" to start the `curl` command, the command returns in less than 1
or 2 seconds. by the time you type in `flow` command in compute node, everything
is done and you end up with no useful information. we actually prefer the
connection to remain open for a while so we can take time to capture the flow
table.

there are some methods to workaround that. here are some examples:

.large file transfer

one method is to install a large file in the webserver and try to pull it with
`curl`, that way the file transfer process "holds" the TCP session.  we've seen
this method in "service" section.

.telnet

we can also make use of `telnet` protocol. with it first we establish the TCP
connection toward the URL's corresponding IP and port, then and manually input a
few HTTP commands and headers to trigger the HTTP request. doing this allow you
some period of time before the haproxy times out and tear down the TCP
connection toward the client. 

NOTE: however, haproxy may still tear down its session immediately toward the
backend pod. how haproxy behaves varies depending on haproxy's implementation
and configurations.

from Internet host, telnet to `Ingress` public FIP `101.101.101.1` and port
`80`:

----
[root@cent-client ~]# telnet 101.101.101.1 80
Trying 101.101.101.1...
Connected to 101.101.101.1.
Escape character is '^]'.
----

now the TCP connection is established - we'll check what is the other end in a
while. next we'll send HTTP `GET` command and `host` header:

----
GET / HTTP/1.1
Host: www.juniper.net
----

basically what it does is to send a HTTP `GET` request to retrieve data and
`Host` provides the URL of the request. one more `return` indicates the end of
the request, which will trigger a response from the server immediately:

----
HTTP/1.0 200 OK
Content-Type: text/html; charset=utf-8
Content-Length: 359
Server: Werkzeug/0.12.1 Python/2.7.12
Date: Mon, 02 Sep 2019 04:05:44 GMT
Connection: keep-alive


<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    rc-webserver-1-g65dg</h3>
  </body>
  </div>
</html>
----

from this moment, we can collect flow table in active haproxy compute node for
later analysis.

.shell script

a script can "type" much faster than we do. so another method is to automate the
test process and repeat the curl and flow command at the same time over and
over. with a small shell script in compute node to collect flow table
periodically, and another script in Internet host to keep sending request with
`curl`, over the time we will have a good chance to have the flow table captured
in compute node at the right moment.

for instance, Internet host side script can be:

----
while :; do curl -H 'Host:www.juniper.net' 101.101.101.1; sleep 3; done
----
//lynx -stdin --dump | cat; sleep 3; done

a compute side script may look like:

----
while :; do flow --match 10.47.255.238; sleep 0.2; done
----

first shell one-liner starts a new test every 3 second, then the second one
captures a specific flow entry every 0.2 second. 20 tests can be done in a 2
minutes and we will capture some useful information in a short while.

In this section we'll use the "script method" to capture the required information
from compute nodes.

=== Packet Flow Analysis

==== Internet Host: Analyze HTTP Request

.curl `-v`

earlier we've used `curl` tool a lot to trigger HTTP requests for our test.
`curl` supports extensive options for various features. we've seen `-H` option
which specify the `host` field in a HTTP request. this time for debugging
purpose we use another useful option - `-v` in `curl` command:

----
[root@cent-client ~]# curl -vH 'Host:www.juniper.net' 101.101.101.1
* About to connect() to 101.101.101.1 port 80 (#0)
*   Trying 101.101.101.1...
* Connected to 101.101.101.1 (101.101.101.1) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Accept: */*
> Host:www.juniper.net
>
* HTTP 1.0, assume close after body
< HTTP/1.0 200 OK
< Content-Type: text/html; charset=utf-8
< Content-Length: 359
< Server: Werkzeug/0.12.1 Python/2.7.12
< Date: Tue, 02 Jul 2019 16:50:46 GMT
* HTTP/1.0 connection set to keep alive!
< Connection: keep-alive
<

<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    Vwebserver-1-846c9ccb8b-g65dg</h3>
  </body>
  </div>
</html>
* Connection #0 to host 101.101.101.1 left intact
----

with this option, it prints more verbose information about the HTTP interaction:

* `>` lines are the messages content that `curl` sent out
* `<` lines are message content that it receives from remote. 

from the interaction we see:

* `curl` sent a HTTP `GET` with path `/` to the FIP `101.101.101.1`, and with
  `Host` filled with "juniper" URL. 
* it gets the response with code `200 OK`, indicating the request has succeeded.
* there are a bunch of other headers in the response that are not important for
  our test so we can skip. 
* the rest part of the response is the HTML source code of a returned web page. 
* the connection is closed immediately afterward.

now you've seen the verbose interactions curl performed under the hood, and you
can understand the GET command and host head we sent in `telnet` test - in that
test we were just emulating what `curl` would do but we just did it manually!

==== Internet Host to Gateway Router

first let's start from the client - the Internet host. 

.Internet Host: send a HTTP request
//image::https://user-images.githubusercontent.com/2038044/64836098-93167f00-d5b6-11e9-9bc2-e4b380867d33.png[]
image::ingress-detail-vh-external-fromhost.drawio.png[]


.Internet host routing table: default route

as in any host, the routing table is pretty simple. static route, or more
typically a default route, pointing to gateway route is all what it needs.

----
[root@cent-client ~]# ip r
default via 10.85.188.1 dev ens160 proto static metric 100
10.85.188.0/27 dev ens160 proto kernel scope link src 10.85.188.24 metric 100
15.15.15.0/24 dev ens192 proto kernel scope link src 15.15.15.2 metric 101
101.101.101.0/24 via 15.15.15.1 dev ens192      #<---
----

the last entry is the static route that we've manually configured, pointing to
our gateway router.

//TODO
NOTE: in this setup, we configured a VRF in the gateway router to connect the
host machine into the same MPLS/VPN, so that it can communicate with the overlay
networks in contrail cluster. In practice, there are other ways to achieve  the
same goal. for example, the gateway router can also choose to leak routes with
policies between VPN and Internet routing table, so that an Internet host which
is not part of the VPN can also access the overlay networks in contrail.


==== Gateway router to Ingress Public FIP: MPLS over GRE
//of active haproxy node: MPLS over GRE

earlier in Ingress section we've seen gateway router's routing table, from the
"protocol next hop" we can find out that the packet will be sent to active
haproxy node `cent222` via MPLSoGRE tunnel.  

.gateway router to Ingress public FIP
//image::https://user-images.githubusercontent.com/2038044/64836185-f7d1d980-d5b6-11e9-9aa3-90866cb36dd3.png[]
image::ingress-detail-vh-external-fromgw.drawio.png[]

now with flow table collected on both computes, we can find out the same
information. let's take a look at the flow entries of active proxy compute:

//another way to demonstrate the
//forwarding flow is to collect the flow table in node `cent222`. 

////
----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
......
Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)

   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)
----
* a user from Internet Host sends a http request by typing the URL
  "http://www.juniper.net" and hit enter
* DNS resolves the host to FIP address
* via default route Internet Host send HTTP request to gateway router's VRF
* gateway router learns the VIP prefix in VRF with next-hop pointing to the
  compute node running active haproxy, in this case node `cent222`

////

----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
Flow table(size 80609280, entries 629760)

Entries: Created 586803 Added 586861 Deleted 1308 Changed 1367Processed 586803
Used Overflow entries 0
(Created Flows/CPU: 147731 149458 144549 145065)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   114272<=>459264       15.15.15.2:42786                                    6 (2->2)
                         101.101.101.1:80
(Gen: 3, K(nh):89, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):61,
 Stats:2/112,  SPort 50985, TTL 0, Sinfo 192.168.0.204)

   459264<=>114272       10.47.255.238:80                                    6 (2->5)
                         15.15.15.2:42786
(Gen: 1, K(nh):89, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):89,
 Stats:1/74,  SPort 60289, TTL 0, Sinfo 8.0.0.0)


(vrouter-agent)[root@cent222 /]$ nh --get 89
Id:89         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:7          Vrf:2
              Flags:Valid, Policy, Etree Root,
              EncapFmly:0806 Oif:8 Len:14
              Encap Data: 02 c0 0a c1 e6 6c 00 00 5e 00 01 00 08 00
----

//this flow table rephrases the same fact as what we've seen from gateway router's VRF table:

This flow reflect the state of the TCP connection originated from Internet host
client to active haproxy. let's first look at the first entry in the capture:

* the first flow entry displays the source and destination of the http request,
  it is coming from Internet host (`15.15.15.2`) and lands the `Ingress` FIP in
  current node `cent222`

* `S(nh):61` is the next hop to the source of the request - the Internet
  host.  this is similiar concept like the reverse path forwarding(RPF). vrouter
  always maintains the path toward the source of the packet in the flow.

* `nh --get` command resolves the nexthop 61 with more details, we see a
  `MPLSoGRE` flag is set, `Sip` and `Dip` is the two end of the GRE tunnel,
  they are current node and gateway router's loopback IP respectively. 

* `TCP:SSrEEr` is TCP flags showing the state of this the TCP connection.
  vrouter detected the `SYN` (`S`), `SYN-ACK` (`Sr`), so bidirectional
  connection is established (`EEr`).

////
TODO: why 2->2, should be 2-5?
* `Proto(V)` indicates the protocol type of the traffic that reaches the
  vrouter's corresponding VRF. in this case protocol `6` means `TCP` because
  this is HTTP request. `(2->2)` indicates the packet come ?
////

overall the first flow entry confirms: the request packet from Internet host
traverses gateway router, and via MPLSoGRE tunnel it hit the Ingress external
VIP `101.101.101.1`.  NAT will happen and we'll look into it next.

==== Ingress Public FIP to Ingress Pod IP: FIP(NAT)

//on active haproxy node: NAT
////
----
(vrouter-agent)[root@cent222 /]$ flow --match 101.101.101.1
Flow table(size 80609280, entries 629760)

Entries: Created 1856648 Added 1856785 Deleted 3015 Changed 3234Processed 1856648 Used Overflow entries 0
(Created Flows/CPU: 467916 472342 457241 459149)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead
Listing flows matching ([101.101.101.1]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)

(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.238
......
Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)
----
////

to verify the NAT operation, we only need to dig a little bit more out of the
previous flow output.

* the `Action` flag, `N(D)` in the first entry indicates destination NAT -
  `DNAT`. destination Ingress external FIP `101.101.101.1` which is the external
  Ingress will be translated to the Ingress internal VIP

* the `Action` flag, `N(S)` in the second entry, indicates source NAT - `SNAT`.
  indicate source NAT - `SNAT`, source IP `10.47.255.238` which is the internal
  internal Ingress VIP will be translated to the Ingress external VIP.
  
.Ingress public FIP to Ingress Pod IP
//image::https://user-images.githubusercontent.com/2038044/64836244-25b71e00-d5b7-11e9-88d4-269942f99163.png[]
image::ingress-detail-vh-external-activeproxy.drawio.png[]

in summary, what the flow table of active haproxy node `cent222` tells is that on
receiving the packet destined to the Ingress FIP, vrouter on node `cent222`
performs NAT operation and translates destination FIP (`101.101.101.1`) to the
Ingress's internal VIP (`10.47.255.238`).  after that the packet lands the Ingress
loadbalancer’s VRF and forwarded to the active haproxy's listening interface. from
this moment HTTP proxy operation will happen and we'll talk about it next.

NOTE: in vrouter flow, the second flow entry is also called a "reverse flow" of
the first one. it is the flow entry vrouter uses to send returning packet
towards Internet host.  from Ingress loadbalancer's perspective it only uses
`10.47.255.238` assigned from the default pod network as its source IP, it does
not knows anything about the FIP. same thing for the external Intenet host, it
only knows how to reach the FIP and has no clues about the private Ingress
internal VIP. it is vrouter that is doing the two way NAT translations in
between.

==== Ingress Pod IP to Service IP: MPLS over UDP
//on active haproxy node 

now the packet lands in Ingress loadbalancer's VRF and it is in the frontend of
the haproxy.  what the haproxy supposes to do is:

* haproxy listening on the frontend IP (Ingress internal podIP/VIP) and port 80
  see the packet.
* haproxy checks the ingress rule programmed in its config file, decides that
  the requests need to be proxied to service IP of `webservice-1`. 
* vrouter checks the Ingress loadbalancer's VRF table and sees the prefix of
  `webservice-1` service IP is learned from a destination node `cent333`, which
  will be the next hop to forward the packet. 
* between compute node the forwarding path is programmed with MPLSoUDP tunnel,
  so vrouter sends it through MPLS over UDP tunnel with right MPLS Label.

.Ingress pod IP to service IP
//image::https://user-images.githubusercontent.com/2038044/64836298-50a17200-d5b7-11e9-9704-b48e7b737f4b.png[]
image::ingress-detail-vh-external-activeproxy2service.drawio.png[]

.Ingress loadbalancer's VRF table
image::https://user-images.githubusercontent.com/2038044/64873035-46fb2700-d616-11e9-866f-a252a0f5c2c3.png[]

please note that all traffic from Ingress to service happens in overlay. between
contrail compute nodes that means all overlay packets should be encapsulated in
MPLS over UDP tunnel. to verify haproxy process packet processing details, we
capture packets on the physical interface of node `cent222`, where the active
haproxy process is running.

with a wireshark display filter `ip.addr == 10.47.255.238`, we can get the
underlay TCP flow:

.packet capture on active haproxy node `cent222` fabric interface

//image::https://user-images.githubusercontent.com/2038044/60518123-e1ea9300-9cae-11e9-82ec-d341e32e42c8.png[]
//image::https://user-images.githubusercontent.com/2038044/60539848-aadea680-9cdb-11e9-8896-c4824d17dd9d.png[]
image::https://user-images.githubusercontent.com/2038044/64902003-ee577880-d66d-11e9-8aaf-f4aab850f7b5.png[]

from the wireshark screenshot, we see clearly that:

* frame 43-45, Ingress private podIP established a new TCP connection toward
  service IP and port, this happens in overlay.
* frame 46, on the new TCP connection, haproxy start a HTTP request to the
  service IP. 
* frame 50, the HTTP response returns back.

frame 46 is also the one we use as an example to show the packet encapsulation.
you will see this IP packet containing the HTTP request is MPLS-labeled, and it
is embedded inside of a UDP datagram. the "outer" source and destination IP of
the packet are `10.169.25.20` (compute node `cent222`) and `10.169.25.21`
(compute node `cent333`) respectively.

////
* sending overlay packets between compute node requirs MPLSoUDP tunnel. 

  which is on the other node `cent333`. that is indicated by the
  underlay destination IP of the request - `10.169.25.21`
////

."forward" vs "proxy"

if you are observant enough, you should have noticed something "weird" in
this capture. questions are:

* shouldn't the source IP address be the Internet host's IP `15.15.15.2`,
  instead of loadbalancer's frontend IP? 
* is the original HTTP request "forwarded" at all?
* is the transaction within the same TCP session sourcing from Internet host,
  accrossing gateway router and loadbalancer node `cent222`, all the way down to
  the backend pod sitting in node 'cent333`?

The answers are NO. the haproxy in this test is doing layer 7 (application
layer) loadbalancing. what it does is:

* establish TCP connection with Internet host and keep monitoring the HTTP request. 
* whenever it see a HTTP request coming in, it checks its rule and initiates a
  "brand new" TCP connection to the corresponding backend
* it "copies" the original HTTP request it received from Internet host and
  "paste" into the new TCP connection with its backend. precisely speaking, the
  http request is "proxied", not "forwarded".

extending the wireshark display filter to include both `15.15.15.2` and
`101.101.101.1` will discover the "whole story":

.packet capture on active haproxy node `cent222` fabric interface: the "whole story"
//image::https://user-images.githubusercontent.com/2038044/60540296-d1e9a800-9cdc-11e9-8914-fbe4fc59ae60.png[]
//image::https://user-images.githubusercontent.com/2038044/64901844-bea77100-d66b-11e9-9ee2-9a8436bd9a55.png[]
image::https://user-images.githubusercontent.com/2038044/64902086-0e3b6c00-d66f-11e9-83bd-9aac0842d545.png[]

* frame 39-41: Internet host established a TCP connection toward Ingress
  external public FIP
* frame 42: Internet host send HTTP request
* frame 43-52: active haproxy establish a new TCP connection toward service,
  send the HTTP request, retrieve the HTTP response, and close the connection
* frame 53-54: active haproxy send the HTTP response back to Internet host
* frame 55-57: Internet host close the HTTP connection

here we use frame 42 to display the MPLS over GRE encapsulation between active
haproxy node cent222 and gateway router. comparing with frame 46 in the previous
screenshot, you will notice this is a different label. the MPLS label carried in
the GRE tunnel will be stripped before vrouter deliever the packet to the active
haproxy. a new label will be assigned when active haproxy start a new TCP
session to the remote node.

at the moment we know the http request is "proxied" to haproxy's backend.
according to Ingress configuration that backend is a kubernetes `service`. now,
to reach the `service` the request is sent to a "destination node" `cent333`
where all backend pod is sitting. next we'll look at what will happen in
destination node.

==== Service IP to Backend Pod IP: FIP(NAT)

.service IP to backend pod IP
//image::https://user-images.githubusercontent.com/2038044/64836348-79296c00-d5b7-11e9-9757-2cdc4028c41d.png[]
image::ingress-detail-vh-external-service2pod.drawio.png[]

on destination node `cent333`, when packet comes in from Ingress internal IP
`10.47.255.238` toward the service IP `10.99.225.17` of `webservice-1`, vrouter
again do the NAT translation operations. it translates the service IP to the
backend podIP `10.47.255.236`, pretty much the same way as what we've seen 
in node `cent222`, where vrouter translate between Ingress public FIP with
Ingress internal podIP.

here is the flow table we captured with shell script. this flow shows the state
of the "second" TCP connection between active haproxy and the backend pod.

----
evrouter-agent)[root@cent333 /]$ flow --match 10.47.255.238
Flow table(size 80609280, entries 629760)
Entries: Created 482 Added 482 Deleted 10 Changed 10Processed 482 Used Overflow entries 0
(Created Flows/CPU: 163 146 18 155)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   403188<=>462132       10.47.255.236:80                                    6 (2->4)
                         10.47.255.238:54500
(Gen: 1, K(nh):23, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):23,
 Stats:2/140,  SPort 52190, TTL 0, Sinfo 4.0.0.0)

   462132<=>403188       10.47.255.238:54500                                 6 (2->2)
                         10.99.225.17:8888
(Gen: 1, K(nh):23, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:3/271,  SPort 65421, TTL 0, Sinfo 10.169.25.20)
----

You've seen something similar in `service` section so you may not have issues to
read it through.  obviously the second entry is triggered by the incoming
request from active haproxy IP (the Ingress podIP) towards service IP.
//active haproxy (in node `cent222`) follows its rules inheritated from our
//Ingress definition and dispatchs the request of "juniper" URL to
//`webservice-1`, whose IP:port is `10.99.225.17:8888`. 
vrouter knows the service IP is a FIP that maps to the  backend podIP
`10.47.255.236`, and service port maps to the container targetPort in backend
pod. it does `DNAT+DPAT` (`DPd`) in incoming direction and `SNAT+SPAT` (`SPs`)
in outgoing direction.

finally, pod sees the HTTP request and responds back with a web page. this
returning traffic is reflected by the first flow entry in the capture, which
shows: 

* original source IP as backend podIP `10.47.255.236`
* original source port as webserver port `80`
* destination IP as `Ingress` internal podIP `10.47.255.238`

==== Backend Pod: Analyze HTTP Request

////
this may not be very convincing in one aspect. the packet capture shows the
communication is between loadbalancer IP and podIP, that part is fine. this is
solely from the pod's perspective "after" the NAT operation. it does
not shows what happens right before NAT.
////

another `tcpdump` packet capture on the backend pod interface helps to reveal
the packet interaction between the Ingress intenral IP and backend podID.

----
$ tcpdump -ni tapeth0-baa392 -v
12:01:07.701956 IP (tos 0x0, ttl 63, id 32663, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.238.54500 > 10.47.255.236.http: Flags [S], cksum 0xd88d (correct), seq 2129282145, win 29200, options [mss 1420,sackOK,TS val 515783670 ecr 0,nop,wscale 7], length 0
12:01:07.702012 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.236.http > 10.47.255.238.54500: Flags [S.], cksum 0x1468 (incorrect -> 0x8050), seq 3925744891, ack 2129282146, win 28960, options [mss 1460,sackOK,TS val 515781436 ecr 515783670,nop,wscale 7], length 0
12:01:07.702300 IP (tos 0x0, ttl 63, id 32664, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.54500 > 10.47.255.236.http: Flags [.], cksum 0x1f57 (correct), ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 0
12:01:07.702304 IP (tos 0x0, ttl 63, id 32665, offset 0, flags [DF], proto TCP (6), length 159)
    10.47.255.238.54500 > 10.47.255.236.http: Flags [P.], cksum 0x6fac (correct), seq 1:108, ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 107: HTTP, length: 107
        GET / HTTP/1.1
        User-Agent: curl/7.29.0
        Accept: */*
        Host:www.juniper.net
        X-Forwarded-For: 15.15.15.2

12:01:07.702336 IP (tos 0x0, ttl 64, id 12224, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.54500: Flags [.], cksum 0x1460 (incorrect -> 0x1eee), ack 108, win 227, options [nop,nop,TS val 515781436 ecr 515783671], length 0
12:01:07.711882 IP (tos 0x0, ttl 64, id 12225, offset 0, flags [DF], proto TCP (6), length 69)
    10.47.255.236.http > 10.47.255.238.54500: Flags [P.], cksum 0x1471 (incorrect -> 0x5f06), seq 1:18, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 17: HTTP, length: 17
        HTTP/1.0 200 OK
12:01:07.712032 IP (tos 0x0, ttl 64, id 12226, offset 0, flags [DF], proto TCP (6), length 550)
    10.47.255.236.http > 10.47.255.238.54500: Flags [FP.], cksum 0x1652 (incorrect -> 0x1964), seq 18:516, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 498: HTTP
12:01:07.712152 IP (tos 0x0, ttl 63, id 32666, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.54500 > 10.47.255.236.http: Flags [.], cksum 0x1ec7 (correct), ack 18, win 229, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712192 IP (tos 0x0, ttl 63, id 32667, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.54500 > 10.47.255.236.http: Flags [F.], cksum 0x1ccb (correct), seq 108, ack 517, win 237, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712202 IP (tos 0x0, ttl 64, id 12227, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.54500: Flags [.], cksum 0x1460 (incorrect -> 0x1cd5), ack 109, win 227, options [nop,nop,TS val 515781446 ecr 515783681], length 0
----

////
----
$ tcpdump -ni tapeth0-baa392
23:37:29.754864 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [S], seq 1528773587, win 29200, options [mss 1420,sackOK,TS val 384765722 ecr 0,nop,wscale 7], length 0
23:37:29.754922 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [S.], seq 953745157, ack 1528773588, win 28960, options [mss 1460,sackOK,TS val 384763489 ecr 384765722,nop,wscale 7], length 0
23:37:29.755247 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 0
23:37:29.755253 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [P.], seq 1:71, ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 70: HTTP: GET / HTTP/1.1
23:37:29.755291 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 71, win 227, options [nop,nop,TS val 384763489 ecr 384765724], length 0
23:37:29.766886 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [P.], seq 1:18, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 17: HTTP: HTTP/1.0 200 OK
23:37:29.767032 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [FP.], seq 18:516, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 498: HTTP
23:37:29.767188 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 18, win 229, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767210 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [F.], seq 71, ack 517, win 237, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767218 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 72, win 227, options [nop,nop,TS val 384763501 ecr 384765736], length 0
----
////

==== Return Traffic

on the reverse direction, podIP runs webserver and responds with it's web page.
the response follows the reverse path of the request:

* pod responds to loadbalancer frontend IP, across MPLSoUDP tunnel
* vrouter on node `cent333` perform SNAT+SPAT, translating podIP:podPort into
  serviceIP:servicePort
* respond reaches to active haproxy running on node `cent222`
* haproxy "copies" the http response from backend pod, and "paste" into its connection
  with the remote Internet host
* vrouter on node `cent222` perform SNAT, translating loadbalancer
  frontend IP to FIP
* response is sent to gateway router, which forwards to Internet host
* Internet host gets the response.

== Contrail Multiple Interface Pod

in Kubernetes cluster, typically each pod only has one network interface (except
the `loopback` interface). In reality, there are scenarios where multiple
interfaces are required. e.g. a vnf(virtual network function) typically needs a
"left", "right" and optionally a "management" interface to do network fucntions.
a pod may requires a "data interface" to carry the data traffic, and a "management
interface" for the reachability detection. Service Providers also tend to keep the
management and tenant networks independent for isolation, and management purpose.
Multiple interfaces provide a way for containers to be connected to multiple devices
in multiple networks simultaneously.

=== Contrail as a CNI

////
As you probably already know containers use namespaces to isolate resources and
rate limit their use. Linux’s network namespaces are used to glue container
processes and the host networking stack. Docker spawns a container in the
containers own network namespace and later on runs a veth pair (a cable with two
ends) between the container namespace and the host network stack
////

in container technology, A veth(Virtual ETHernet) pair is functioning pretty much
like a virtual "cabel", that can be used to create tunnels between network namespaces.
one end of it is "plugged" in the container and the other end is in the host or docker
bridge. 
//it can also be used to create a bridge to a physical network device in another
namespace.

A "CNI plugin" is the one who is responsible for inserting the network interface
(that is one end of the veth pair) into the container network namespace and it will
also makes all necessary changes on the host. e.g. attaching the other end of the veth
into a bridge, assigning IP, configuring routes, and so on.

//TODO: need redraw
.container and veth pair
//image::https://user-images.githubusercontent.com/2038044/60554760-ee9ad580-9d06-11e9-9628-f01af759f6e1.png[]
//image::https://user-images.githubusercontent.com/2038044/64902914-47c7a380-d67e-11e9-8f2d-b3197fb003e4.png[]
image:vethpair.drawio.png[]

there are many such CNI plugin implementations that are publicly available
today. contrail is one of them.  for a comprehensive list you can check
https://github.com/containernetworking/cni where contrail is also listed.

`multus-cni`, is another CNI plugin that "enables attaching multiple
network interfaces to pods". multipe-network support of `multus-cni` is
accomplished by Multus calling multiple other CNI plugins. because each plugin
will create its own network, multiple plugins make the pod be able to have
multiple networks. one of the main advantages that contrail provides, comparing
with `mutus-cni` and all other current implementations in the industry, is that
contrail by itself provides the ability to attach multiple network interfaces to
a kubernetes pod, without the need to call any other plugins. this brings
support to a truly "multi-homed" pod.

===  NetworkAttachmentDefinition CRD

////
Kubernetes supports a custom extension to represent networks in its object
model, through its `CustomResourceDefinition(CRD)` feature. This extension adds
support for a new kind of object called `NetworkAttachmentDefinition`, which
represents a network in Kubernetes data model.

in contail, a CRD object defines the template for a network object
`NetworkAttachmentDefinition`, which contains all information about each
network's specification, and tells Kubernetes API how to understand and expose
it. 
////

contrail CNI follows the Kubernetes Network `CRD`(Custom Resource Definition)
- NetworkAttachmentDefinition to provide a standardized method to specify the
configurations for additional network interfaces. there is no change to the 
standard kubernetes upstream APIs, making the implementation coming with the
most compatibilities.

in contrail setup the NetworkAttachmentDefinition CRD is created by
`contrail-kube-manager`(`KM`). when bootup, `KM` validates if a network CRD
`network-attachment-definitions.k8s.cni.cncf.io` is found in the Kubernetes API
server, and creates one if not yet.

here is a `CRD` object yaml:

----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  version: v1
  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
    - net-attach-def
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            config:
             type: string
----

in contrail kubernetes setup, the CRD has been created and can be verified:

----
$ kubectl get crd
NAME                                             CREATED AT
network-attachment-definitions.k8s.cni.cncf.io   2019-06-07T03:43:52Z
----

////
----
$ kubectl get crd -o yaml
apiVersion: v1
items:
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    creationTimestamp: 2019-06-07T03:43:52Z
    generation: 1
    name: network-attachment-definitions.k8s.cni.cncf.io
    resourceVersion: "1170"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/network-attachment-definitions.k8s.cni.cncf.io
    uid: 77f15393-88d6-11e9-a8b1-0050569e6cfc
  spec:
    additionalPrinterColumns:
    - JSONPath: .metadata.creationTimestamp
      description: |-
        CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.

        Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
      name: Age
      type: date
    group: k8s.cni.cncf.io
    names:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    scope: Namespaced
    version: v1
    versions:
    - name: v1
      served: true
      storage: true
  status:
    acceptedNames:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    conditions:
    - lastTransitionTime: 2019-06-07T03:43:52Z
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: null
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
    storedVersions:
    - v1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
////

using this new kind `NetworkAttachmentDefinition` created from the above CRD,
now we have the ability to create `vitual-network` in contrail kubernetes
environments.

to create a virtual-network from kubernetes, use a yaml template like this:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: <network-name>
  namespace: <namespace-name>
  annotations:
    "opencontrail.org/cidr" : [<ip-subnet>]
    "opencontrail.org/ip_fabric_snat" : <True/False>
    "opencontrail.org/ip_fabric_forwarding" : <True/False>
spec:
  config: '{
    “cniVersion”: “0.3.0”,
    "type": "contrail-k8s-cni"
}'
----

like many other standard kubernetes object, basically you specify the VN name,
namespace under `metadata`, and `annotations` which is used to carry additional
information about a network. in contrail the following annotations are used in
`NetworkAttachmentDefinition` CRD to enable certain attributes for the
virtual-network:

* `opencontrail.org/cidr`: CIDR, which defines the subnet for a VN
* `opencontrail.org/ip_fabric_forwarding`: a flag to enable/disable `ip fabric
  forwarding` feature
* `opencontrail.org/ip_fabric_snat`: a flag to enable/disable `ip fabric snat`
  feature

****
In contrail, `ip-fabric-forwarding` feature enables ip fabric based forwarding
without tunneling for the VN. When two `ip_fabric_forwrding` enabled virtual
networks communicate with each other, overlay traffic will be forwarded directly
using the underlay. 

With the Contrail `ip-fabric-snat` feature, pods that are in the overlay can
reach the Internet without floating IPs or a logical-router. The
`ip-fabric-snat` feature uses compute node IP for creating a source NAT to reach
the required services.

`ip fabric forwarding` and `ip fabric snat` features are not covered in this
book.
****

alternatively, you can define a new VN by referring an existing VN:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: extns-network
  annotations:
    "opencontrail.org/network" : '{"domain":"default-domain", "project": "k8s-extns", "name":"k8s-extns-pod-network"}'
spec:
  config: '{
    “cniVersion”: “0.3.1”,
    "type": "contrail-k8s-cni"
}'
----

throughout this book we'll use the first template to define our VNs in all
examples.

=== Multiple Interface Pod

with multiple VNs created, we can "attach" (you may also say "plug", or
"insert") any of them into a pod, with a pod yaml file like this:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      { "name": "VN-a" },
      { "name": "VN-b" },
      { "namespace": "other-ns", "name": "VN-c" }
    ]'
spec:
  containers:
----

another valid format:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: 'VN-a,VN-b,other-ns/VN-c'
spec:
  containers:
----

you probably notice, pods in a namespace not only can refer to the networks
defined in local NS, but also networks created on other namespaces using their
fully scoped name. this is very useful - the same network does not has to be
duplicated again and again in every NS that needs it, it can be defined just one
time and then referred anywhere else.

we've understood the basic theories and explored the various templates. now it's
time to look at a "working example" in the real world. we'll start from
creating two VNs, examining the VN objects, then create a pod and attach the 2
VNs into it. we'll conclude the test and this section by examining the pod
interfaces and connectivity with other pods sharing the same VNs.

////
now you may want to test these theories in your setup
starting from creating your own yaml files based on these templates. if this is
the first time you work on this, you will most likely run into all kinds of
small issues here and there.
////

here is a yaml file of two VNs: `vn-left-1` and `vn-right-1`

----
#vn-left-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "10.10.10.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-left-1
spec:
  config: '{ 
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

----
#vn-right-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "20.20.20.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-right-1
  #namespace: default
spec:
  config: '{
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

create both VNs:

----
$ kubectl apply -f vn-left-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-left-1 created

$ kubectl apply -f vn-right-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-right-1 created
----

examine the VNs:

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-left-1       3s
vn-right-1      10s
----

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io vn-left-1 -o yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"k8s.cni.cncf.io/v1","kind":"NetworkAttachmentDefinition","metadata":{"annotations":{"opencontrail.org/cidr":"10.10.10.0/24","opencontrail.org/ip_fabric_forwarding":"false"},"name":"vn-left-1","namespace":"ns-user-1"},"spec":{"config":"{ \"cniVersion\": \"0.3.0\", \"type\": \"contrail-k8s-cni\" }"}}
    opencontrail.org/cidr: 10.10.10.0/24
    opencontrail.org/ip_fabric_forwarding: "false"
  creationTimestamp: 2019-06-13T14:17:42Z
  generation: 1
  name: vn-left-1
  namespace: ns-user-1
  resourceVersion: "777874"
  selfLink: /apis/k8s.cni.cncf.io/v1/namespaces/ns-user-1/network-attachment-definitions/vn-left-1
  uid: 01f167ad-8de6-11e9-bbbf-0050569e6cfc
spec:
  config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

the VNs are created, as expected. it seems nothing much exciting here. However,
if you login to the contrail UI, you will see something "unexpected". 

//old GUI:
//image::https://user-images.githubusercontent.com/2038044/59985880-f5886080-9601-11e9-98c9-791fec2fbe55.png[]

.contrail command: "main-menu" -> "virtual networks"

image::https://user-images.githubusercontent.com/2038044/60283772-f78b4180-98d7-11e9-9358-1ed47aeeef57.png[]

NOTE: make sure you select a correct "project", in this case it is `k8s-default`.

you won't be able to find any VN with the exact name `vn-left-1` or `vn-right-1`
in the UI. instead, what you will find are two VNs named
`k8s-vn-left-1-pod-network` and `k8s-vn-right-1-pod-network` got created. 

there is nothing wrong here. What happened is whenever a VN get created from
kubernetes, contrail automatically adds kubernetes cluster name(by default
`k8s`) as a prefix to the VN name that you give in the network yaml file, and a
suffix `-pod-network` in the end. This makes sense because we know a VN can be
created by different methods.  with these extra keywords embeded in the name, it
is easier to tell how the VN was created(from kubernetes or from the UI
manually), what will it be used for, etc. also potential VN name conflicts can
be avoided across multiple kubernetes clusters.

here is yaml file of a multiple interfaces pod:

////
----
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
----
////

----
#pod-webserver-multivn-do.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webserver-mv
  labels:
    app: webserver-mv
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: webserver-mv
    image: contrailk8sdayone/contrail-webserver
    imagePullPolicy: Always
  restartPolicy: Always
----

in pod annotations under metadata, we insert 2 VNs: `vn-left-1` and
`vn-right-1`. Now guess how many interfaces will the pod has on bootup?  you may
think it will be two because that is what we gave in the file. let's create the
pod and verify:

----
$ kubectl get pod -o wide
NAME          READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
webserver-mv  1/1    Running  0         20s  10.47.255.238  cent222  <none>

$ kubectl describe pod webserver-mv
Name:               webserver-mv
Namespace:          ns-user-1
Priority:           0
PriorityClassName:  <none>
Node:               cent222/10.85.188.20
Start Time:         Wed, 26 Jun 2019 12:51:30 -0400
Labels:             app=webserver-mv
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.250",
                              "mac": "02:87:cf:6c:9a:98",
                              "name": "vn-left-1"
                          },
                          {
                              "ips": "10.47.255.238",
                              "mac": "02:87:98:cc:4e:98",
                              "name": "cluster-wide-default"
                          },
                          {
                              "ips": "20.20.20.1",
                              "mac": "02:87:f9:f9:88:98",
                              "name": "vn-right-1"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ 
                        { "name": "vn-left-1" }, { "name": "vn-right-1" } ]
                    kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":
                      {"annotations":{"k8s.v1.cni.cncf.io/networks":"[
                      { \"name\": \"vn-left-1\" }, { \"name\": \"vn-...
Status:             Running
IP:                 10.47.255.238
...<snipped>...
----

in `Annotations`, under `k8s.v1.cni.cncf.io/network-status` we see a list
`[...]`, which has 3 items each represented by a curly brace block `{}` of
key-value mappings. each curly brace block includes information about one
interface: the allocated IP, MAC and the VN it belongs to. so you will end up to
have 3 interfaces created in the pod instead of 2.  please notice the second
item which gives IP address `10.47.255.238`, that is the interface attached to
the "default pod network" named "cluster-wide-default", which is created by the
sytem. you can look at the default pod network as a "managment" network because
it is always "up and running" in every pod's network namespace. funtionally
it is no much different with the VN you create - except that you can't delete
it.

we can "login to" the pod, list the interfaces and verify the IP and MAC.

----
$ kubectl exec -it webserver-mv sh
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
37: eth0@if38: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:47:06:d8:98 brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.238/12 scope global eth0
       valid_lft forever preferred_lft forever
39: eth1@if40: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:6b:a0:e2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.250/24 scope global eth1
       valid_lft forever preferred_lft forever
41: eth2@if42: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:8e:8a:80:98 brd ff:ff:ff:ff:ff:ff
    inet 20.20.20.1/24 scope global eth2
       valid_lft forever preferred_lft forever
----

we see one lo interface and 3 interfaces plugged by contrail CNI, each with the
IP allocated from the corresponding VN. also you will notice the MAC addresses
match what we've seen in `kubectl describe` command output. 

NOTE: having the MAC address in the annotations could be useful under certain
cases. for example, in "service chaining" section, you will run into a scenario
where you have to use the MAC address to locate the proper interface, so that
you can assign the right podIP which kubernetes allocated from a VN. check
"service chaining" section for more details.

////
NOTE: having the MAC address in the annotations could be useful under certain
cases. for example, in "service chaining" section, you will run into a scenario
when you need to use the MAC address to locate the proper interface, before you
can even tell which interface should be configured with which podIP that
kubernetes allocated from a VN. check "service chaining" section for more
details.

- you login a pod and for some reason you lose the track
of interface to VN mapping (e.g., you manually changed/removed the IPs, or the
pod's application reset the IP, etc) you can count on the MAC address! later In
////

you will see multiple-interfaces pod again in sevice-chaining example later on.
in that example the pod will be based on Juniper CSRX image instead of a general
docker image. but the basic idea remains the same.

== Contrail Service Chaining with CSRX

=== Contrail Service Chaining Introduction

service chaining is the idea of forwarding traffic through multiple network
entity in a certain order, each network entity do specific function such as
firewall, IPS , NAT , LB , …,etc. the legacy way of doing service chaining would
use standalone HW appliances which made service chaining inflexible, expensive
and takes a long time to setup. Dynamic service chaining is where network
functions deployed as VM or Container and could be chained automatically in a
logical way. in the next example we use contrail for services chaining between
two PODs in two different networking using CSRX container L4-L7 firewall to
secure the traffic between these two networks as shown in the figure:

.service chaining
//image::https://user-images.githubusercontent.com/2038044/60268925-85a4ff00-98bb-11e9-94c3-219d41038642.png[]
image::https://user-images.githubusercontent.com/2038044/63706136-ad9dd980-c7fc-11e9-9d73-0b76bbcb6979.png[]

[NOTE]
====
- left and right networks are just a common name used for simplicity and
  expected the traffic to follow from left to right but you can use your own
  names 
- make sure to configure the network before you attached a POD to it otherwise
  POD would fail to be created 
====

=== Bringing Up Client and CSRX Pods
==== Create VNs

so let’s start create two networks using this YAML files 

----
#vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.10.10.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-left
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'

#vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.20.20.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-right
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

----
# kubectl create -f vn-left.yaml
# kubectl create -f vn-right.yaml
----

Verify using Kubectl 
 
----
# kubectl get network-attachment-definition
NAME       AGE
vn-left    19d
vn-right   17d

# kubectl describe network-attachment-definition
Name:         vn-left
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.10.10.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-25T20:28:22Z
  Generation:          1
  Resource Version:    83111
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-left
  UID:                 a44fe276-7f2b-11e9-9ff0-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>


Name:         vn-right
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.20.20.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-28T07:14:02Z
  Generation:          1
  Resource Version:    380427
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-right
  UID:                 2b8d394f-8118-11e9-b36d-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>
----

It’s a good practice to confirm these two networks are seen now in contrail
before proceeding.  From the Contrail UI, select Configure > Networking >
Networks > default-domain > k8s-default, As shown in the figure which focus on
left network

NOTE: using `default` namespace in the YAML file for a network will create
it in domain “default-domain” and project “K8s-default”

image::https://user-images.githubusercontent.com/2038044/60268927-863d9580-98bb-11e9-965a-b50f91d811d1.png[]

==== Create Client Pods

Create two ubuntu Pods, one in each network using the annotation object

----
#left-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: left-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-left" }]'
spec:
  containers:
    - name: ubuntu-left-pod-sc
      image: contrailk8sdayone/ubuntu
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN


#right-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: right-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-right" }]'
spec:
  containers:
    - name: ubuntu-right-pod-sc
      image: contrailk8sdayone/ubuntu
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN

# kubectl create -f right-ubuntu-sc.yaml
# kubectl create -f left-ubuntu-sc.yaml


# kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
left-ubuntu-sc    1/1     Running   0          25h
right-ubuntu-sc   1/1     Running   0          25h

# kubectl describe pod 
Name:               left-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:20 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.1",
                              "mac": "02:7d:b1:09:00:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.47.255.249",
                              "mac": "02:7d:99:ff:62:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }]
Status:             Running
IP:                 10.47.255.249
Containers:
  ubuntu-left-pod-sc:
    Container ID:   docker://2f9a22568d844c68a1c4a45de4a81478958233052e08d4473742827482b244cd
    Image:          contrailk8sdayone/ubuntu
    Image ID:       docker-pullable://contrailk8sdayone/ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
   
...<snipped>...

Name:               right-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 04:09:18 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.20.20.1",
                              "mac": "02:89:cc:86:48:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.252",
                              "mac": "02:89:b0:8e:98:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-right" }]
Status:             Running
IP:                 10.47.255.252
Containers:
  ubuntu-right-pod-sc:
    Container ID:   docker://4e0b6fa085905be984517a11c3774517d01f481fa43aadd76a633ef15c58cbfe
    Image:          contrailk8sdayone/ubuntu
    Image ID:       docker-pullable://contrailk8sdayone/ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
  
 ...<snipped>...
----

==== Create CSRX Pod

create Juniper CSRX container that have one interface on the left network and
one interface on the right network using this YAML file 

----
#csrx1-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: csrx1-sc
  labels:
    app: webapp-sc
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left" },
       { "name": "vn-right" }
   ]'
spec:
  containers:
  - name: csrx1-sc
    #image: hub.juniper.net/security/csrx:18.1R1.9
    #image: csrx
    image: contrailk8sdayone/csrx
    ports:
    - containerPort: 22
    imagePullPolicy: Never
    stdin: true
    tty: true
    securityContext:
      privileged: true
  imagePullSecrets:
  - name: secret-jnpr

# kubectl create -f csrx1-sc.yaml
----
////
NOTE: in order for kubernetes to pull the image from juniper private registry, a
secret is required. refer to chapter 3 `secret` section for the configuration of
secret object `secret-jnpr` that we referred here in csrx yaml file.
////

Confirm the interface placement in the correct network 

----
# kubectl describe pod csrx1-sc
Name:               csrx1-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:31 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.2",
                              "mac": "02:84:71:f4:f2:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.20.20.2",
                              "mac": "02:84:8b:4c:18:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.248",
                              "mac": "02:84:59:7e:54:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }, { "name": "vn-right" } ]
Status:             Running
IP:                 10.47.255.248
Containers:
  csrx1-sc:
    Container ID:   docker://82b7605172d937895269d76850d083b6dc6e278e41cb45b4cb8cee21283e4f17
    Image:          contrailk8sdayone/csrx
    Image ID:       docker://sha256:329e805012bdf081f4a15322f994e5e3116b31c90f108a19123cf52710c7617e

...<snipped>...

----

NOTE: each container has one interface belong to “cluster-wide-default” network
regardless the use of the annotations object because annotations object above
creates and put one extra interface in a specific network 

==== Verify podIP

.verify podIP

Login to the left, right Pods and the CSRX to confirm the IP/MAC address 
 
----
# kubectl exec -it left-ubuntu-sc bash
root@left-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:99:ff:62:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.249/12 scope global eth0
       valid_lft forever preferred_lft forever
15: eth1@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:b1:09:00:8d brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.1/24 scope global eth1
       valid_lft forever preferred_lft forever



# kubectl exec -it right-ubuntu-sc bash
root@right-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
23: eth0@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:b0:8e:98:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.252/12 scope global eth0
       valid_lft forever preferred_lft forever
25: eth1@if26: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:cc:86:48:8d brd ff:ff:ff:ff:ff:ff
    inet 10.20.20.1/24 scope global eth1
       valid_lft forever preferred_lft forever


# kubectl exec -it csrx1-sc cli
root@csrx1-sc>
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d
----

NOTE: unlike other PODs the CSRX didn’t acquire IP with DHCP and it start with
factory default configuration hence it need to be configured. 

NOTE: By default, CSRX eth0 is visible only from shell and used for management.
And when attaching networks, the first attach network is mapped to eth1 which is
GE-0/0/1 And the second attach is mapped to eth2 which is GE-0/0/0

.configure CSRX IP
Configure this basic setup on the CSRX, to assign the correct IP address use the
MAC/IP address mapping from the “ kubectl describe pod” command show output as
well configure default security policy to allow everything for now 

----
set interfaces ge-0/0/1 unit 0 family inet address 10.10.10.2/24
set interfaces ge-0/0/0 unit 0 family inet address 10.20.20.2/24

set security zones security-zone trust interfaces ge-0/0/0
set security zones security-zone untrust interfaces ge-0/0/1 
set security policies default-policy permit-all 
commit
----

verify the IP address assigned on the CSRX

----
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

  Logical interface ge-0/0/1.0 (Index 100)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.10.10.0/24, Local: 10.10.10.2

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d

  Logical interface ge-0/0/0.0 (Index 200)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.20.20.0/24, Local: 10.20.20.2
----

==== Ping Test

From the Left POD try to ping the left POD, ping would fail as there is no route 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms

root@left-ubuntu-sc:/# ip r
default via 10.47.255.254 dev eth0 
10.10.10.0/24 dev eth1  proto kernel  scope link  src 10.10.10.1 
10.32.0.0/12 dev eth0  proto kernel  scope link  src 10.47.255.249
----

Adding static route to the left and right PODs and try to ping again 

----
root@left-ubuntu-sc:/# ip r add 10.20.20.0/24 via 10.10.10.2

root@right-ubuntu-sc:/# ip r add 10.10.10.0/24 via 10.20.20.2

root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
----

Still ping failed, as we didn’t create the service chaining which will also take
care of the routing. let’s see what happen to our packets 

----
root@csrx1-sc# run show security flow session 
Total sessions: 0
----

No session on the CSRX.  

==== Troubleshooting Ping Issue

Login to the compute node “cent22” that host this container to dump the traffic
using tshark and check the routing To get the interface linking the containers 

----
[root@cent22 ~]# vif -l
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

...<snipped>...

vif0/3      OS: tapeth0-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.252
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10760  bytes:452800 errors:0
            TX packets:14239  bytes:598366 errors:0
            Drops:10744

vif0/4      OS: tapeth1-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.1
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13002  bytes:867603 errors:0
            TX packets:16435  bytes:1046981 errors:0
            Drops:10805

vif0/5      OS: tapeth0-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.249
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10933  bytes:459186 errors:0
            TX packets:14536  bytes:610512 errors:0
            Drops:10933

vif0/6      OS: tapeth1-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.1
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:12625  bytes:1102433 errors:0
            TX packets:15651  bytes:810689 errors:0
            Drops:10957

vif0/7      OS: tapeth0-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.248
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:20996  bytes:1230688 errors:0
            TX packets:27205  bytes:1142610 errors:0
            Drops:21226

vif0/8      OS: tapeth1-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.2
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13908  bytes:742243 errors:0
            TX packets:29023  bytes:1790589 errors:0
            Drops:10514

vif0/9      OS: tapeth2-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.2
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:16590  bytes:1053659 errors:0
            TX packets:31321  bytes:1635153 errors:0
            Drops:10421

...<snipped>...

----

Note that Vif0/3 and Vif0/4 are bounded with the right POD and both linked to
tapeth0-89a4e2 and tapeth1-89a4e2 respectively same goes for the left POD for
Vif0/5 and vif0/6 while vif0/7, vif 0/8 and vif0/9 are bound with CSRX1.| from
that you can also see the number of packets/bytes hits that interface as well
the VRF which is this interface belong in here VRF 3 is for the
default-cluster-network while VRF 6 for the left network and VRF 5 for the right
network in this figure you can see the interface mapping from the all
prospective (container, Linux , vr-agent) 

//image::https://user-images.githubusercontent.com/2038044/60268930-863d9580-98bb-11e9-9dc3-b0c5598ff528.png[]
image::https://user-images.githubusercontent.com/2038044/63706210-d3c37980-c7fc-11e9-92b8-e5de88e22e76.png[]

try to ping again from the left POD to the right POD and use tshark on the tap
interface for the right POD for further inspection 

----
[root@cent22 ~]# tshark -i tapeth1-89a4e2
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-89a4e2'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.254 (Request)
  2 0.000037656 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.253 (Request)
  3 1.379993896 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Who has 10.20.20.1?  Tell 10.20.20.253
----

Looks like the ping isn’t reaching the right POD at all , lets see on the CSRX
left network tap interface  

----
[root@cent22 ~]# tshark -i tapeth1-844f1c
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-844f1c'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Who has 0.255.255.252?  Tell 0.0.0.0
  2 0.201392098   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=410/39425, ttl=63
  3 0.201549430   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  4 1.201444156   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=411/39681, ttl=63
  5 1.201600074   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  6 1.394074095 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.254 (Request)
  7 1.394108344 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.253 (Request)
  8 2.201462515   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=412/39937, ttl=63
----

We can see the packet but there is nothing in the CSRX security prospective to
drop this packet

checking the routing table of the left network VRF by logging to the
`vrouter_vrouter-agent_1` docker in the compute node 

----
[root@cent22 ~]# docker ps | grep vrouter
9a737df53abe        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest   "/entrypoint.sh /usr…"   2 weeks ago         Up 47 hours                             vrouter_vrouter-agent_1
e25f1467403d        ci-repo.englab.juniper.net:5000/contrail-nodemgr:master-latest         "/entrypoint.sh /bin…"   2 weeks ago         Up 47 hours                             vrouter_nodemgr_1

[root@cent22 ~]# docker exec -it vrouter_vrouter-agent_1 bash
(vrouter-agent)[root@cent22 /]$ 
(vrouter-agent)[root@cent22 /]$ rt --dump 6 | grep 10.20.20.
(vrouter-agent)[root@cent22 /]$
----

Note that 6 is the routing table VRF of the left network, same would goes for
the right network VRF routing table there is missing route 

    (vrouter-agent)[root@cent22 /]$ rt --dump 5 | grep 10.10.10.
    (vrouter-agent)[root@cent22 /]$

So even if all the PODs are hosted on the same compute nodes, they can’t reach
each other. And if these PODs are hosted on different compute nodes then you
have a bigger problem to solve. Service chaining isn’t about adjusting the routes
on the containers but mainly about exchange routes between the vrouter-agent
between the compute nodes regardless of the location of the POD, as well adjust
that automatically if the POD moved to another compute node. Before we build
service chaining lets address an important concerns for network administrator
who are not fan of this kind of CLI troubleshooting, can we do the same
troubleshooting using contrail controller GUI? 

the answer is yes and lets do it.

From the Contrail Controller UI, select monitor > infrastructure > virtual
router then select the node the that host the POD , in our case “Cent22.local” 

image::https://user-images.githubusercontent.com/2038044/60268931-863d9580-98bb-11e9-9682-d330878fa386.png[]

as shown in the figure from the interface tab which is equivalent to running “
vif -l” command on the vrouter_vrouter-agent-1 container and even showing more
information notice the mapping between the instance ID and tap interface naming
where the first 6 character of the instance ID are always reflected in the tap
interface naming

to check the routing tables of each VRF move to the “routes” tab and select the
VRF you want to see

image::https://user-images.githubusercontent.com/2038044/60268935-86d62c00-98bb-11e9-8eaa-820578b11127.png[]

If we select the left network ( the name is longer as it include the domain ,
project ) we can confirm there is not 10.20.20.0/24 prefix from the right
network We can also check the mac address learned in the left network by
selecting L2 ( which is equvilant to “rt --dump 6 --family bridge” command 

image::https://user-images.githubusercontent.com/2038044/60268936-86d62c00-98bb-11e9-9050-ca104b278a1a.png[]

=== Service Chaining
==== Create Service Chaining

Now lets utilize the CSRX to service chaining using contrail command GUI

creating Service chaining is 4 steps make sure to do them in this order 

1. create Service template 
2. creating service instance based on the service template you created before
3. creating network policy and select the service instance you created before
4. apply this network policy on network   

NOTE: since contrail command GUI is the solution to provide a single point of
management for all environments, we will use it to build service changing but
you still can use the normal contrail controller GUI to build service changing
 
Login to contrail command GUI ( in our setup https://10.85.188.16:9091/) then select service > catalog > create 

image::https://user-images.githubusercontent.com/2038044/60268937-86d62c00-98bb-11e9-8744-b8213b5246ed.png[]
image::https://user-images.githubusercontent.com/2038044/60268938-876ec280-98bb-11e9-991b-a54dedadfbcd.png[]
 
insret a name of services template “myweb-CSRX-CS” in here then chose v2 ,
virtual machine ( no other option available) for service mode we will work with
In-network and firewall as service type  

image::https://user-images.githubusercontent.com/2038044/60268941-876ec280-98bb-11e9-8f68-5c49af9b06d1.png[]

Select interfaces management, left and right then click create
 
image::https://user-images.githubusercontent.com/2038044/60268942-876ec280-98bb-11e9-8c7c-ac2a95da9ab0.png[]

Now select deployment and click create to create the service instances

image::https://user-images.githubusercontent.com/2038044/60268943-876ec280-98bb-11e9-9cf8-12b240de0286.png[]

Insert a name for this service instance then select from the drop down menu the
name of the template you created before then chose the proper network from the
prospective of the CSRX being the instance (container in that case) that will do
the service chaining and click on port tuples to expand it 

image::https://user-images.githubusercontent.com/2038044/60268945-88075900-98bb-11e9-87fa-375337170b12.png[]

then for each of the three interface bound one interface of the CSRX then click create

NOTE: the name of the virtual machine interface isn’t shown in the drop down
menu instead the instance ID, you can identify that from the tap interface name
as we showed before.  In other word all you have to know is most 6 left
character for any interface belong to that container as all the interface in a
given instance ( VM or container)  share the same first characters from the left 
 
Before you procced make sure the status of the three interfaces are up and they
are showing the correct IP address of the CSRX instance 
 
image::https://user-images.githubusercontent.com/2038044/60268947-88075900-98bb-11e9-9b0a-ecf0c6a03e33.png[]

To create network policy go to overlay > network policies > create 

image::https://user-images.githubusercontent.com/2038044/60268948-88075900-98bb-11e9-88ba-f5f7a02161b0.png[]
 
Insert a name for your network policy then in the first rule add left network as source network and right network as destination with action pass 

image::https://user-images.githubusercontent.com/2038044/60268949-889fef80-98bb-11e9-844a-326b5d506038.png[]

Select advanced option to attached the service instance you create before and click create 

image::https://user-images.githubusercontent.com/2038044/60268951-889fef80-98bb-11e9-84c5-f354b3d8938e.png[]

To attach this network policy to network click virtual network and select the left network and edit 

image::https://user-images.githubusercontent.com/2038044/60268953-889fef80-98bb-11e9-8826-2626a76c3d4a.png[]

In network policies select the network policy you just created from the drop down menu then click save 
do the same for the right network

image::https://user-images.githubusercontent.com/2038044/60268955-89388600-98bb-11e9-9605-14fbc8d30fbe.png[]

==== Verify Service Chaining

Now lets check the effect of this service changing on routing 
From the Contrail Controller module control node (http://10.85.188.16:8143 in
oursetup), select monitor > infrastructure > virtual router then select the node
the that host the POD , in our case “Cent22.local” then select the “routes” tab
and select the left VRF 
 
image::https://user-images.githubusercontent.com/2038044/60268956-89388600-98bb-11e9-9e82-7d5fbddf38f8.png[]

Now we can the right networks host routes has been leaked to the left network
(10.20.20.1/32 , 10.20.20.2/32 in this case) 

Now let’s try to ping the right pod from the left pod to see the session created on the CSRX 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
64 bytes from 10.20.20.1: icmp_seq=1 ttl=61 time=0.863 ms
64 bytes from 10.20.20.1: icmp_seq=2 ttl=61 time=0.290 ms
^C
--- 10.20.20.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.290/0.576/0.863/0.287 ms

root@csrx1-sc# run show security flow session 
Session ID: 5378, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/2 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/2;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 

Session ID: 5379, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/3 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/3;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 
Total sessions: 2

----

==== Security Policy

Now let try to create security policy on the CSRX to allow only http and https

----
root@csrx1-sc# show security 
policies {
    traceoptions {
        file ayma;
        flag all;
    }
    from-zone trust to-zone untrust {
        policy only-http-s {
            match {
                source-address any;
                destination-address any;
                application [ junos-http junos-https ];
            }
            then {
                permit;
                log {
                    session-init;
                    session-close;
                }
            }
        }
        policy deny-ping {
            match {
                source-address any;
                destination-address any;
                application any;        
            }                           
            then {                      
                reject;                 
                log {                   
                    session-init;       
                    session-close;      
                }                       
            }                           
        }                               
    }                                   
    default-policy {                    
        deny-all;                       
    }                                   
}                                       
zones {                                 
    security-zone trust {               
        interfaces {                    
            ge-0/0/0.0;                 
        }                               
    }                                   
    security-zone untrust {             
        interfaces {                    
            ge-0/0/1.0;                 
        }                               
    }                                   
}
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2000ms
----

the ping failed as the policy on the CSRX drop it 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:04:01 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/8->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5394 N/A N/A -1
Jun 14 23:04:02 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/9->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5395 N/A N/A -1
Try to send http traffic from the left to the right POD and verify the session status on the CSRX
root@left-ubuntu-sc:/# wget 10.20.20.1
--2019-06-14 23:07:34--  http://10.20.20.1/
Connecting to 10.20.20.1:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11510 (11K) [text/html]
Saving to: 'index.html.4'

100%[======================================>] 11,510      --.-K/s   in 0s      

2019-06-14 23:07:34 (278 MB/s) - 'index.html.4' saved [11510/11510]
----

And in the CSRX we can see the session creation 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Adding resolved unicast NH. dest: 10.20.20.1, proto v4 (peer initiated)
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Sending resolve request for stale ARP entry (b). NH: 5507 dest: 10.20.20.1
Jun 14 23:07:34 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CREATE: session created 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 N/A(N/A) ge-0/0/1.0 UNKNOWN UNKNOWN UNKNOWN N/A N/A -1
Jun 14 23:07:35 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CLOSE: session closed TCP FIN: 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 14(940) 12(12452) 2 UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 UNKNOWN N/A N/A -1
----

////

===== problem of old method

we'll compose similar curl commands to test `simple fanout Ingress`. this time
we give different paths and see how Ingress distribute them.

----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238/dev
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<title>404 Not Found</title>
<h1>Not Found</h1>
<p>The requested URL was not found on the server.  If you entered the URL
manually please check your spelling and try again.</p>
----

it doesn't work!

the reason is our webserver coming with the backend pods does not support the
`path` `dev` or `qa` in the URL. 

[TIP]
=====
checking the webserver's code reveals the issue:

----
$ kubectl exec -it webserver-1-846c9ccb8b-wns77 bash
root@webserver-1-846c9ccb8b-wns77:/app# ls
Dockerfile  app.py  requirements.txt  static
root@webserver-1-846c9ccb8b-wns77:/app# cat app.py
# Simple Web-Server
from flask import Flask
import subprocess
app = Flask(__name__)
def workers():
    cmd_ip = 'ifconfig | sed -n 2p | cut -d ":" -f2 | cut -d " " -f1 | tr -d "\n"'
    cmd_hostname = 'hostname | tr -d "\n"'
    ip_addr = str(subprocess.check_output(cmd_ip, shell=True))
    hostname = str(subprocess.check_output(cmd_hostname, shell=True))
    return '''
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b> pod</h2><br><h3>IP address = ''' + ip_addr + '''<br>Hostname = ''' + hostname + '''</h3>
  </body>
  </div>
</html>
'''
@app.route('/')                 #<---
def root():
    return workers()
@app.route('/contrail')         #<---
def contrail():
    return workers()
if __name__ == '__main__':
    app.run(debug=True,host='0.0.0.0', port=80)
----

in the code only path `/` and `/contrail` is supported.

=====

===== workaround

there are several ways to workaround the problem:

* change the current server code and restart the server
* create a new server that supports URLs with the 2 paths

in this section we will demonstrate how to use an existing python module to
create a HTTP server, and make it to serve the "path" that we give in the URL:

* in each backend pods we'll create a new HTTP server.
* the server is created from python module `SimpleHTTPServer` coming with
  the webserver image of each pods.
* to avoid confliction, the new HTTP servers will listen on a different port
  other than default port `80`, which had been used by the existing server
  process.
* accordingly, we update the `targetPort` (from `80` to `90`) of `service` in
  order to deliver the request to the new HTTP server.
  
NOTE: don't forget to delete the old services and create the new ones with the
updated parameters.

===== create web pages

in order to make the webserver responding to the URL with given paths, we can
create some web pages with file names being same as the `path`: `dev`, `qa`,
`abc` and etc. to make the test to return consistent output with the one
returned by the old server, we simply use the same template for our web page.
make sure to change the IP address and Hostname to the value of the pod that
this file is going to be copied to. for example pod `webserver-1-846c9ccb8b-wns77` is
with IP `10.47.255.236`, so we generate a file `dev` for this pod:

.generate a webpage `dev`
----
cat <<EOF > dev
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    webserver-1-846c9ccb8b-wns77</h3>
  </body>
  </div>
</html>
EOF
----

.copy the webpage into the backend pod

----
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/dev
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/qa
$ kubectl cp dev Vwebserver-1-846c9ccb8b-wns77:/app/abc
$ kubectl exec -it Vwebserver-1-846c9ccb8b-wns77 -- ls -lt
total 24
-rw-r--r--. 1 root root 364 Aug 13 06:26 abc    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 dev    #<---
-rw-r--r--. 1 root root 364 Aug 13 06:26 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

similarly, just change the IP and Hostname of this webpage to the vaule of the
other backend pod and then copy into it:

//$ sed 's/10.47.255.236/10.47.255.235/g; s/webserver-846c9ccb8b-wns77/Vwebserver-2-846c9ccb8b-t75d8/g' dev > dev2

----
$ sed 's/10.47.255.236/10.47.255.235/g' dev > temp
$ sed 's/webserver-1-846c9ccb8b-wns77/webserver-2-846c9ccb8b-t75d8/g' temp > dev2
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/dev
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/qa
$ kubectl cp dev2 Vwebserver-2-846c9ccb8b-t75d8:/app/abc
$ kubectl exec -it Vwebserver-2-846c9ccb8b-t75d8 -- ls -lt
total 24
-rw-r--r--. 1 root root 366 Aug 13 06:39 abc    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 dev    #<---
-rw-r--r--. 1 root root 366 Aug 13 06:39 qa     #<---
-rw-r--r--. 1 root root 937 Apr 21  2017 app.py
drwxr-xr-x. 2 root root  23 Apr 21  2017 static
-rw-r--r--. 1 root root 254 Apr  6  2017 Dockerfile
-rw-r--r--. 1 root root   6 Apr  6  2017 requirements.txt
----

TIP: we create a page `abc` that has no corresponding path to test the scenario
when incoming HTTP request contains an "unknown" path.

===== start new web server

for each pod open a seperate terminal, then start a web server with python
module `SimpleHTTPServer`, listening on port 90:

----
$ kubectl exec -it Vwebserver-1-846c9ccb8b-wns77 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
$ kubectl exec -it Vwebserver-2-846c9ccb8b-t75d8 -- python -m SimpleHTTPServer 90
Serving HTTP on 0.0.0.0 port 90 ...
----

////
create some files with name being same as the `paths`: `dev`, `qa`, and `abc`.

----
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > dev"
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > qa"
kubectl exec -it Vwebserver-1-846c9ccb8b-s2zn9 -- bash -c "echo Vwebserver-1-846c9ccb8b-s2zn9:10.47.255.249 > abc"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > dev"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > qa"
kubectl exec -it Vwebserver-2-846c9ccb8b-k9x26 -- bash -c "echo Vwebserver-2-846c9ccb8b-k9x26:10.47.255.248 > abc"
----

////

===== the new all-in-one yaml file

the last step before testing our Ingress is to update the previous services
`targetPort` 80 in the "all-in-one" yaml file with the new HTTP server's port
90. delete the current services and apply the updated all-in-one yaml file
again:

.updated yaml file ingress-simple-fanout2.yaml (diff only)
----
$ diff ingress-simple-fanout.yaml ingress-simple-fanout2.yaml
26c26
<     targetPort: 80
---
>     targetPort: 90
38c38
<     targetPort: 80
---
>     targetPort: 90
----

.delete the old services and apply yaml file again
----
$ kubectl delete svc/webservice-1
service "webservice-1" deleted

$ kubectl delete svc/webservice-2
service "webservice-2" deleted

$ kubectl apply -f ingress/ingress-simple-fanout2.yaml
ingress.extensions/ingress-sf unchanged
service/webservice-1 created    #<---
service/webservice-2 created    #<---
deployment.extensions/webserver-1 unchanged
deployment.extensions/webserver-2 unchanged
----

only the two services are updated and deleted, so when we apply the new yaml
file, only these two objects are recreated - all other objects keep unchanged.

////

== contrail Network Policy (ch3)

//resources:
//https://www.juniper.net/documentation/en_US/contrail4.1/topics/concept/security-policy-enhancements.html
//https://github.com/ahmetb/kubernetes-network-policy-recipes
//https://medium.com/@reuvenharrison/an-introduction-to-kubernetes-network-policies-for-security-people-ba92dd4c809d

////
Kubernetes brings another security dynamic to the table – its defaults are
geared towards making it easy for users to get up and running quickly, as well
as being backward compatible with earlier releases of Kubernetes that lacked
important security features. Consequently, many important Kubernetes
configurations are not secure by default.

One important configuration that demands attention from a security perspective
is the network policies feature. Network policies specify how groups of pods are
allowed to communicate with each other and other network endpoints. You can
think of them as the Kubernetes equivalent of a firewall.
////

=== network policy introduction

kubernetes networking model requires all pod can access the the other pods by
default. we call this a "flat network" sometimes because it follows a
"allow-any-any" model - basically a kubernetes pod can reach any other pods *by
default*. this makes the design and implementation of kubernetes networking
significantly simplified and much more scalable. 

NOTE: In chapter 4 we'll read more about the requirements that kubernetes
enforces on the networking implementation.

on the other hand, with companies large and small rapidly adopting the platform,
security has emerged as an important concern. In reality, in many cases certain
level of network segmentation methods are required to ensure that only certain
pods can talk to each other. that is when kubernetes network policy comes into
the picture. a Kubernetes "network policy" defines the access permissions for
groups of pods in a way pretty much like a security group in the cloud is used
to control access to VM instances. 

kubernetes supports network policy via the `NetworkPolicy` object, which is a
Kubernetes resource just like `pod`, `service`, `ingress`, and many others that
we've learned earlier in this chapter. the role of `NetworkPolicy` object is to
define how groups of pods are allowed to communicate with each other. 
//later we'll provide a test case to demonstrate how it works
now let's explain the way kubernetes network policy works:

. initially, in a kubernetes cluster, all pods are non-isolated by default. they
  works in "allow-any-any" model so anyone can talk to any others.
. now you apply a network policy named `policy1` to pod A. in policy `policy1`,
  you define a rule to explicitly allow A to talk to pod B. in this case we will
  call pod A a "target" pod, because it is the pod that the network policy will
  act on.
. from this moment on, a few things happen:
  * target pod A can talk to pod B, and can talk to *pod B only*, because B is
    the only pod you allowed in the policy. due to the nature of the policy
    rules, we can call the rules a "whitelist". 
  * for target pod A only, any connections that are not explicitly allowed by
    the "whitelist" of this network policy `policy1` will be rejected. you don't
    need to explicitly define this in policy `policy1`, it will be enforced by
    the nature of kubernetes network policy. we can call this an implicit policy
    - the `"deny all"` policy. 
  * as for other non-target pods, for example, pod B or C, which are NOT applied
    with this network policy `policy1`, and not any other network policies
    either, will continue to follow the "allow-any-any" model. therefore they
    are not "affected" and can continue to communicate to all other pods in the
    cluster. we can call this another implicit policy - A `"allow all"` policy.
. assuming you want pod A to also be able to communicate to pod C, you need to
  update the network policy `policy1` and it's rules to *explicit* allow it. in
  another word,
  you need to keep updating the "whitelist" to allow more traffic types.

////
//multiple network policies behavior is not clear to me. not tested, so skip.
. multiple network-polices can be applied to a pod. for example, you can add a
  second policy `policy2` which also select pod A to apply its forwarding rules.
////

as you see, when you define a policy, essentially at least three policies will
be applied in the cluster:

* explicit `policy1`: the network policy you defined, with the whitelist rules allowing
  certain type of traffic for the selected (target) pod.
* an implicit `"deny all"` network policy: deny all other traffic that is not in
  the whitelist for the target pod. 
* an implicit `"allow all"` network policy: allow all other traffic for other
  non-targeted pods that are not selected by the `policy1` network policy. we'll
  see "deny all" and "allow all" policies again later in chapter 8.

here are some highlights of kubernetes network policy:

////
* pod specific
* implicit "allow all"
* "whitelist" based rules
* Mutiple network policies is allowed
* flows based
////

.pod specific
network policy specification applies to one pod or a group of pods based on
`label`, same way as RC or Deploy do.

."whitelist" based rules
explicit rules compose a "whitelist", each rule describe a certain type of
traffic to be allowed. all other traffic that is not described by any rules in
the whitelist will be dropped for the target pod.

.implicit "allow all" 
a pod will be "affected" only if it is selected as the target by any network
policy, and it will be "affected" by the selecting network policy only. absence
of network policy applied on a pod indicates an implicit "allow all" policy to
this pod. in another word, if a non-targeted pod continues its "allow-any-any"
networking model.

.seperation of ingress and egress
policy rules need to be defined for a specific direction. the direction can
be `Ingress`, `Egress`, none or both. 
//only traffic in the specific direction will be allowed by the corresponding rules defined for it.

."flow" based (vs. "packet" based)
once the initiating packet is allowed, the return packet in same flow will also
be allowed. suppose an ingress policy applied on pod A allows an ingress HTTP
request, then the whole HTTP interaction will be allowed for pod A. this
includes the 3 way TCP connection establishment and all data and acknowledgment
in both directions.

////
if traffic from pod A to pod B is allowed by the configured policy, then the
return packets for that flow from B -> A are also allowed, even if the policy in
place would not allow B to initiate a connection to A. 
////

////
//has to skip to play saft
.multiple network policies are allowed

//because...says here:
//https://medium.com/@reuvenharrison/an-introduction-to-kubernetes-network-policies-for-security-people-ba92dd4c809d
//Mutiple network policies can compose a network policy chain. when applied on any
//pod, traffic matching any of the network policies will be permitted.

//Kubernetes can only combine policies with different policyTypes (Ingress or
//Egress). Multiple policies that specify ingress (or egress) will overwrite
//each-other.
////

NOTE: Network polices are implemented by the network component, so you must be
using a network solution which supports network policy. Simply creating the
`NetworkPolicy` resource without a controller to implement it will have no
effect. in our book contrail is such a network components with network policy
implemented. in chapter 8, you will see how these network policies works in
contrail.

=== network policy definition

like all other objects in kubernetes, network policy can be defined in a yaml
file. let's go ahead to look at an example of it. 

NOTE: this is the same example that we're going to read again in chapter 8.

----
#policy1-do.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy1
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: webserver-dev
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.169.25.20/32
    - namespaceSelector:
        matchLabels:
          project: jtac
    - podSelector:
        matchLabels:
          app: client1-dev
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: dbserver-dev
    ports:
    - protocol: TCP
      port: 80
----

before explaining it in detail, let's look at the `spec` part of this yaml file
since other sections are self-explanatory after you've read yaml file of other
objects. the `spec` has the following structure:

----
spec:
  podSelector:
    ......
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    ......
  egress:
  - to:
    ......
----

here we see that a network policy definition yaml file can logically be divided
into four sections:

* `podSelector`: this defines the pods selection. it identifies pods where the
  current Network policy would be applied to.
* `policyTypes`: specify type of policy rules, `Ingress`, `Egress` or both.
* `ingress`: define the ingress policy rules for the target pods
* `egress`: define the egress policy rules for the target pods

next we'll look at each sections in more detail.

==== selecting target pods

when you define a network policy, kubernetes needs to know which pods you want
this policy to act on. Similar to how the service select its backend pods, the
network policy select which pods it will be applied to based on `labels`:

----
podSelector:
  matchLabels:
    app: webserver-dev
----

here all pods which has the label `app: webserver-dev` is selected to be the
"target" pods by the network policy. all of the following contents in `spec`
will apply to the target pods only.

==== policy types

The second section defines the `policyTypes` for the target pods. 

----
policyTypes:
  - Ingress
  - Egress
----

policyTypes:
  - Ingress


it can be either `ingress`, `egress`, or both. both types define specific
traffic types in the form of one or more rules, which we'll discuss next.

////
//ASK: what?
//`ingress` is the default policy type. 
//below is quote from k8s
If no policyTypes are specified on a NetworkPolicy then by default Ingress will
always be set and Egress will be set if the NetworkPolicy has any egress rules
////

==== policy rules

`ingress` and `egress` section defines the direction of traffic, from the
selected target pods's perspective. for example considering the following
simplified example:

----
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: client1-dev
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: client2-dev
    ports:
    - protocol: TCP
      port: 8080
----

assuming target pod is `webserver-dev` pod, and there is only one pod
`client1-dev` in the cluster having a matching label `client1-dev`. two things
will happen:

* ingress direction: pod `webserver-dev` can accept a TCP session with a
  destination pod 80, initiated from pod `client1-dev`. this explains why we
  said kubernetes network policy is "flow" based instead of "packet" based. the
  TCP connection would not be able to establish if the policy would have been
  designed on "packet" based, becaue on receiving the incoming TCP "sync", the
  returning outgoing TCP "sync-ack" would have been rejected without a matching
  egress policy.
* egress direction: pod `webserver-dev` can initiate a TCP session with a
  destination pod 8080, towards pod `client1-dev`.

TIP: for the egress connection to go through, the other end needs to define an
ingress policy to allow the incoming connection.

===== network policy rules

each `from` or `to` statement defines a `rule` in the network policy:

* a `from` statement defines an ingress policy rule 
* a `to` statement defines an egress policy rule
* both rules can optionally has `ports` statement, which will be discussed
  later.

so you can define multiple rules to allow complex traffic mode for each
direction:

  ingress:
  INGRESS RULE1
  INGRESS RULE2
  egress:
  EGRESS RULE1
  EGRESS RULE2

each rule identifies the network endpoints where the target pods can
communicate. Network endpoint can be identified by different methods:

* `ipBlock`: select pods based on ip address block 
* `namespaceSelector`:  select pods based on label of namespace (NS), all pods
  in the matching namespaces will be
* `podSelector`: select pods based on label of pod

NOTE: `podSelector` select different things when it is used in different places
of a yaml file. previously (under `spec` directly) it selects pods that the
network policy applies, which we've called "target" pods. here in a rule (under
`from` or `to`), it selects which pods the target pods is communicating with.
sometime we can call these pods "peering pods", or "endpoints".

//TODO: a diagram?

so the yaml structure for a rule can look like this:

----
  ingress:
  - from:
    - ipBlock:
      .....
    - namespaceSelector:
      .....
    - podSelector:
      .....
    ports:
      ......
----

for example in our example:

----
  ingress:
  - from:
    - ipBlock:
        cidr: 10.169.25.20/32
    - namespaceSelector:
        matchLabels:
          project: jtac
    - podSelector:
        matchLabels:
          app: client1-dev
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: dbserver-dev
    ports:
    - protocol: TCP
      port: 80
----

here:

* The ingress network endpoints are 
  - subnet 10.169.25.20/32, *or*
  - all pods in namespaces which has the label `project: jtac`, *or*
  - pods which has the label `app: client1-dev` in current namespace (namespace
    of target pod)

* The egress network point is pod `dbserver-dev`

we'll come to the `ports` part soon.

===== `AND` vs `OR`

It is also possible to specify only a few pods from namespaces instead of all
pods to communicate with. in our example `podSelector` is used along, which
assumes the same namespace as the target pod. 
another method is to use `podSelector` along with a
`namespaceSelector`. in that case, the namespaces that the pods belongs to is
those with matching labels with `namespaceSelector`, instead of same as the
target pod's namespace. 

for example, assuming the target pod is `webserver-dev` and its namespace is
`dev`, and only namespace `qa` has a label "project=qa" matching to the
`namespaceSelector`:

----
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        project: qa
    podSelector:
      matchLabels:
        app: client1-qa
----

here, the target pod can only communicate with those pods that are:

* in namespace qa, *AND* (not *OR*) -
* with label `app: client1-dev` *in the namespace qa*

Please be careful it is totally different than the below definition, which allow
the target pod to talk to those pods that are:

* in namespaces qa, *OR* (not *AND*) -
* with label `app: client1-qa` *in the target pod's namespace `dev`*

----
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        project: qa
  - podSelector:
      matchLabels:
        app: client1-qa
----

===== protocol and ports

it is also possible to specify `ports` for an ingress and egress rule.
`protocol` type can also be specified along with a protocol port. for example:

  egress:
  - to:
    - podSelector:
        matchLabels:
          app: dbserver-dev
    ports:
    - protocol: TCP
      port: 80

`ports` in ingress says that target pods can allow incoming traffic for the
specified ports and protocol. Ports in egress says that target pods can
initiate traffic to specified ports and protocol. If `ports` is not mentioned,
all ports and protocols are allowed.

===== line by line explanation

after explaining everything, you should find that the policy rules in our
example start to makes more sense, so let's look at it again in more detail:

----
  podSelector:                  <1>
    matchLabels:                <2>
      app: webserver-dev        <3>
  policyTypes:                  <4>
  - Ingress                     <5>
  - Egress                      <6>
  ingress:                      <7>
  - from:                       <8>
    - ipBlock:                  <9>
        cidr: 10.169.25.20/32   <10>
    - namespaceSelector:        <11>
        matchLabels:            <12>
          project: jtac         <13>
    - podSelector:              <14>
        matchLabels:            <15>
          app: client1-dev      <16>
    ports:                      <17>
    - protocol: TCP             <18>
      port: 80                  <19>
  egress:                       <20>
  - to:                         <21>
    - podSelector:              <22>
        matchLabels:            <23>
          app: dbserver-dev     <24>
    ports:                      <25>
    - protocol: TCP             <26>
      port: 80                  <27>
----

////
* tcp session initiated from other pods destined to target pod with
  destination port `80` will arrive the target pod. to all other pods can reach tcp port `80` at target pods
* tcp session initiate from target pods destined to other pod with destination port `8080` 
can initiate reach other pods's tcp port `8080`.  all other traffic would be blocked.
////

from this definition, we now can understand exactly what the network policy is
trying to enforce:

* line 1-3: pod `webserver-dev` is selected by the policy, so it is the "target"
  pod, all following policy
  rules will apply on it, and on it only.
* line 4-6: the policy will define rules for both `Ingress` and `Egress` traffic
* line 7-19: `ingress:` section defines the *ingress policy*
  - line 8: `from:` and line 17: `ports`, these two sections defines one *policy
    rule* in ingress policy.
    *** line 9-16: these 8 lines under `from:` section compose an ingress "whitelist":
        a. line 9-10: any incoming data with source IP being 10.169.25.20/32 can
             access the target pod `webserver-dev`
        b. line 11-13: any pods under namespace `jtac` can access target pod
            `webserver-dev`
        c. line 14-16: any pod `client1-dev` can access target pod `webserver-dev`
    *** line 17-19: `ports` section is second (and optional) part of the same
    *policy rule*.  only TCP port 80 (web service) on target pod `webserver-dev`
    is exposed and accessible. access to all other pods will be denied.
  - line 20-26: `egress:` section defines the *egress policy*
    *** line 21: `to:` and line 24: `ports`, these two sections define one *policy
        rule* in egress policy.
        a. line 21-24: these 4 lines under `to:` section compose an egress
        "whitelist", here the target pod can send egress traffic to pod
        `dbserver-dev`
    *** line 25: `ports` section is second part of the same *policy rule*. the
    target pod `webserver-pod` can only start TCP session with destination port
    of 8080 to other pods.

and that is not all. 

if you remember in the beginning of this chapter we've talked about the
kubernetes default "allow-any-any" network model and the implicit "deny-all",
"allow-all" policies, you will realize that so far we just explained the
explicit part of it (the policy `policy1` in our network policy introduction
section). after that, there are two more implicit policies:

* the `"deny all"` network policy: for the target pod `webserver-dev`, deny all
  other traffic that is other than what is explicitly allowed in the above
  whitelists, this implies at least two rules:
  - ingress: deny all incoming traffic destined to the target pod
    `webserver-dev`, other than what is defined in the ingress whitelist.
  - egress: deny all outgoing traffic sourcing from the target pod
    `webserver-dev`, other than what is defined in the egress whitelist.
* a `"allow all"` network policy: allow all traffic for other pods that are not
  target of this network policy, on both ingress and egress direction.

NOTE: in chapter 8, we'll take a deeper look at these implicit network policies
and their rules in contrail implementation.

=== create network policy

you can create and verify the network policy same way as you create other
kubernetes objects:

----
$ kubectl apply -f policy1-do.yaml
networkpolicy.networking.k8s.io/policy1-do created

$ kubectl get netpol -n dev
NAME           POD-SELECTOR        AGE
policy1        app=webserver-dev   6s

$ kubectl describe netpol policy -n dev
Name:         policy1
Namespace:    dev
Created on:   2019-10-01 11:18:19 -0400 EDT
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     app=webserver-dev
  Allowing ingress traffic:
    To Port: 80/TCP
    From:
      IPBlock:
        CIDR: 10.169.25.20/32
        Except:
    From:
      NamespaceSelector: project=jtac
    From:
      PodSelector: app=client1-dev
  Allowing egress traffic:
    To Port: 80/TCP
    To:
      PodSelector: app=dbserver-dev
  Policy Types: Ingress, Egress
----
in chapter 8, we'll setup a test environment to verify the effect of this
network policy in more detail.

== Contrail – k8s Network Policy

in chapter 4, we've given the "Kubernetes to Contrail Object Mapping" table as
shown below:

.contrail kubernetes object mapping
image::https://user-images.githubusercontent.com/2038044/63705887-1cc6fe00-c7fc-11e9-8c4f-733676cf663a.png[]

the mapping highlights contrail's implementation of kubernetes core objects:
`Namespace`, `pod`, `Service`, `Ingress` and `Network Policy`. so far from
chapter 4 through 7 we've pretty much explored everything except `Network
Policy`.  in this chapter we'll focus on the `Network Policy` implementation in
contrail. we'll first introduce the contrail firewall which is the feature we
used to implement kubernetes network policy; we'll then setup a test case to
verify how does kubernetes network policy works in contrail; based on the test
result, in the end we'll explore the contrail firewall policies and their rules
in details to understand the contrail implementation, as well as the mapping
between the two objects in the object mapping digram.

=== introducing Contrail Firewall

In chapter 3 we introduced kubernetes network policy concept. we went through
the yaml file definition in details and created the network policy based on it.
we've also mentioned that simply creating network policy object won't have any
effect, unless the kubernetes networking implementation support it. contrail as
a kubernetes CNI implements the kubernetes networking, and it supports the
kubernetes network policy through contrail firewall. that is the focus of this
chapter - we'll demonstrate how network policy work in contrail environment
through contrail firewall.

before that, let's review some of the important concept and design in contrail
to understand why we implement kubernetes network policy through contrail
firewall.

////
Contrail has various security features to control traffics between workloads.
examples are:

* security groups
* network policy
* contrail firewall security policy

Each feature has its own capabilities and more or less different design goals. 
////

.inter-VN routing
in contrail, virtual networks (VN) are isolated by default. that means by
default Workloads in `VN1` cannot communicate with workloads in another VN
`VN2`. to allow inter-VN communications between `VN1` to `VN2`, additional
configuration is required.  for example you can use a neutron router, also
called "logical router" in contrail, to connect the multiple VNs so inter-VN
traffic can be "routed".  another commonly used method is to define a "contrail
network policy" to connect VNs. contrail network policy also provides security
between two virtual networks by allowing or denying specified traffic. actually
in this respect, security group is a similar feature. next we'll talk about each
feature briefly.

.contrail network policy

A contrail network policy is used to permit inter-VN communication and to modify
intra-VN traffic. it describes which traffic is permitted or not between VNs. by
default, without a contrail network policy, intra-VN communication is allowed,
but inter-VN traffic is denied.  when you create a network policy you must
associate it with a VN to have any effect. 

////
//no need to tell these
several policies may be associated
with one VN at the same time, and each policy contains a list of rules that are
evaluated in the top-down fashion. the evaluation ends when the first match is
found, which is known as "terminal behavior"
////

NOTE: 
don't confuse "contrail network policy" with "kubernetes network policy". these
are two different security features and they work seperately.

.security group(SG)

a security group, often abbreviated as a `SG`, is a group of rules that allow a
user to specify the type of traffic that is allowed or not through a *port*.
When a VM or pod is created in a VN, a `SG` can be associated with the VM when
it is launched. unlike contrail network policy, which is configured "globally"
and associated to the VNs, the SG is configured on the per-port basis, and it
will take effect on the specific vrouter flows that is associated with the VM
port. 

////
A security group is a container for security group rules. Security groups and
security group rules allow administrators to specify the type of traffic that is
allowed to pass through a port. When a virtual machine (VM) is created in a
virtual network (VN), a security group can be associated with the VM when it is
launched. If a security group is not specified, a port is associated with a
default security group. The default security group allows both ingress and
egress traffic. Security rules can be added to the default security group to
change the traffic behavior.
////

.the limitation of SG and contrail network policy

in modern contrail cloud environments, sometimes it is hard to only use the
existing network policy and security group to achieve desired security goal. for
example: in cloud environments, workloads may move from one server to another
and so most likely the IP is changing often.  just relying on IP addresses to
identify the endpoints to be protected is painful. Instead, users must leverage
application level attributes to manipulate policies, so that the policies don't
need to be updated everytime workload moves and the associated network
environment changes.
also, in production, a user might need to group workloads based on combinations
of tags, which is hard to translate into existing language of network policy or
Security Group. 

.contrail firewall security policy

in this chapter we'll introduce another important feature: "contrail firewall
security policy".

Contrail Firewall security policy allows decoupling of routing from security
policies and provides multi dimension segmentation and policy portability, while
significantly enhancing user visibility and analytics functions.

in order to implement the multi-dimension traffic segmentation, Contrail
firewall introduces the concept of "tags". Tags are key-value pairs associated
with different entities in the deployment. Tags can be pre-defined or
custom/user defined.  contrail `tags` are pretty much the same thing as
kubernetes `labels`. both are used to identify the objects and workloads.  as
you can see, this is similar to kubernetes network policy design, and it is
natural for contrail to use its firewall security policy to implement kubernetes
network policy - in theory, contrail network policy or SG can be extended to do
the job, but the support of tags by contrail firewall make it much simpler. 

////
TODO: what?
the implementation has the following advanatages:

1. Workloads can be represented and grouped by tags.
2. Combinational tags can be used in policies.
3. Untagged workloads can be specified in policies.
4. Policies can be applied in various layers.
////

[NOTE]
====
later in this chapter, we'll sometimes refer "contrail firewall security policy"
as "contrail Security", "contrail firewall", "contrail firewall security" or
simply "contrail FW". 
====

=== contrail kubernetes Network Policy usage case

////
TODO:
* lab with 'AND', e.g. client1-support AND in support NS only
* lab to show changing tag in policy is convenient
////

//Before getting into deeper into contrail firewall, 
in this section, lets create a usage case to verify how does the kubernetes
network policy works in contrail environments. we'll start from creating a few
kubernetes namespaces and pods resources that is required in the test,
confirming every pod can talk to the DUT (Device Under Test) because of the
default "allow-any-any" networking model, then creating network policies and
observing any changes with same traffic pattern.

==== network design

suppose we have this network design:

.network policy: the test case design
image::policy-design.drawio.png[]

in this didagram, 6 nodes are distributed in 3 departments: `dev`, `qa` and
`jtac`. `dev` department is running a database server (`dbserver-dev`) holding
all valuable data collected from customer. the design requires that no one
should has direct access to this db server, instead, db server access is only
allowed through another apache frontend server in `dev` department, named
`webserver-dev`.  furthermore, for security reason, the access of customer
information should be granted only to authorized clients. for example, only
nodes in `jtac` department, one node in `dev` department named `client1-dev` and
source IP `10.169.25.20` can access the db via webserver. finally, the database
server `dbserver-dev` should not initiate any connection toward other nodes. 

==== lab preparation

this is a very ordinary, simplified network design that you will see everywhere.
if we model all these network elements in kubernetes world, it will look like
this:

.network policy: NS and pods
image::policy-pods.drawio.png[]

we need to create following resources:

* 3 namespaces: `dev`, `qa`, `jtac`
* 6 pods:
  - 2 server pods: `webserver-dev`, `dbserver-dev`
  - 2 client pods in the same namespace as of server pods: `client1-dev`, `client2-dev`
  - 2 clients pods from two different namespaces: `client-qa`, `client-jtac`
* 2 CIDRs:
  - cidr: 10.169.25.20/32, this is fabric IP of node `cent222`
  - cidr: 10.169.25.21/32, this is fabric IP of node `cent333`

.kubernetes network policy test environment

[options="header,autowidth"]
|====
|NS              |pod           |role
|dev	         |client1-dev   |web client
|dev	         |client2-dev   |web client
|qa	         |client-qa     |web client
|jtac	         |client-jtac   |web client
|dev	         |webserver-dev |webserver serving clients
|dev	         |dbserver-dev  |dbserver serving webserver
|==== 

Lets prepare the required k8s NS and pods resources.

here is the all-in-one yaml file defining `dev`, `qa` and `jtac` namespaces:

----
#policy-ns-pod.yaml
##################
# all namespaces #
##################
kind: Namespace
apiVersion: v1
metadata:
  name: dev
  labels:
    project: dev
---
kind: Namespace
apiVersion: v1
metadata:
  name: qa
  labels:
    project: qa
---
kind: Namespace
apiVersion: v1
metadata:
  name: jtac
  labels:
    project: jtac
---
##################
#   all pods     #
##################
apiVersion: v1
kind: Pod
metadata:
  name: webserver-dev
  labels:
    app: webserver-dev
    do: policy
  namespace: dev
spec:
  containers:
    - name: webserver
      image: contrailk8sdayone/contrail-webserver
      securityContext:
        privileged: true
      ports:
      - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: client1-dev
  labels:
    app: client1-dev
    do: policy
  namespace: dev
spec:
  containers:
    - name: ubuntu
      image: contrailk8sdayone/contrail-webserver
---
apiVersion: v1
kind: Pod
metadata:
  name: client2-dev
  labels:
    app: client2-dev
    do: policy
  namespace: dev
spec:
  containers:
    - name: ubuntu
      image: contrailk8sdayone/contrail-webserver
---
apiVersion: v1
kind: Pod
metadata:
  name: client-qa
  labels:
    app: client-qa
    do: policy
  namespace: qa
spec:
  containers:
    - name: ubuntu
      image: contrailk8sdayone/contrail-webserver
---
apiVersion: v1
kind: Pod
metadata:
  name: client-jtac
  labels:
    app: client-jtac
    do: policy
  namespace: jtac
spec:
  containers:
    - name: ubuntu
      image: contrailk8sdayone/contrail-webserver
----

TIP: ideally, each pods may run based on different images. TCP ports usually are
different between a webserver and a database server. in our case to make the
test easier, we use the same exact `contrail-webserver` image that we've been
using throughout the book for all pods, so clients to webserver and webserver to
dbserver all share the same port number 80 served by same HTTP server. also, we
add a label `do: policy` in all pods so that displaying all pods used in this
test is also easier.

create all resources:

----
$kubectl create -f policy-ns-pod-do.yaml
namespace/dev created
namespace/qa created
namespace/jtac created
pod/webserver-dev created
pod/dbserver-dev created
pod/client1-dev created
pod/client2-dev created
pod/client-qa created
pod/client-jtac created

$ kubectl get pod --all-namespaces -l "do=policy" -o wide
NAMESPACE   NAME            READY   STATUS    RESTARTS   AGE   IP              NODE   
dev         client1-dev     1/1     Running   0          33s   10.47.255.232   cent222
dev         client2-dev     1/1     Running   0          33s   10.47.255.231   cent333
dev         dbserver-dev    1/1     Running   0          33s   10.47.255.233   cent333
dev         webserver-dev   1/1     Running   0          33s   10.47.255.234   cent333
jtac        client-jtac     1/1     Running   0          33s   10.47.255.229   cent222
qa          client-qa       1/1     Running   0          33s   10.47.255.230   cent333
----

==== traffic mode before kubernetes network policy creation

having all of the NS and pods, before we define any network policy yet, we can
go ahead to send the traffic between clients and servers. 

of course, kubernetes networking by default follows "allow-any-any" model, so we
should expect access works between any pod, which is going to be a fully meshed
access relationships. but keep in mind that the `DUT` in this test is
`webserver-dev` and `dbserver-dev` which we are more interested to observe. to
simply the verification, according to our diagram, we'll focus on accessing the
server pods from the client pods, illustrated in below figure:
 
////
`webserver-dev` pod from all the clients pods
(`client1-dev`, `client2-dev`, `client-qa` and `client-jtac`) plus the hosts of
the two nodes, and from `webserver-dev` pod to `dbserver-dev` pod, illustrated
in below figure:
////

.network policy: pods communication before network policy creation
image::policy-b4policy.drawio.png[]

a few highlights here:

* all clients can access the servers, following the "permit-any-any" model
    - there is no restrictions between clients and `webserver-dev` pod
    - there is no restrictions between clients and `dbserver-dev` pod
* the communcation between client and servers are bi-directional and
  "symmetrical" - each end can "initiate a session" or "accept a session". these
  matches to the "egress policy" and "ingress policy" respectively in kubernetes
  network policy's term.

obviously, these do not meet our design goal, which is exactly why we need
kubernetes network policy, we'll come to that part soon. for now let's quickly
verify the allow-any-any networking model.

first let's verify the http server running at port 80 in `webserver-dev` and
`dbserver-dev` pods:

----
$kubectl exec -it webserver-dev -n dev -- netstat -antp| grep 80
tcp        0      0 0.0.0.0:80              0.0.0.0:*    LISTEN      1/python
$kubectl exec -it dbserver-dev -n dev -- netstat -antp| grep 80
tcp        0      0 0.0.0.0:80              0.0.0.0:*    LISTEN      1/python
----

TIP: as mentioned earlier, in this test all pods is with the same container
image, so all pods are running the same webserver application in their
containers. in this test we simply use the name to each pod to reflect their
different roles in the diagram.

now we can verify accessing this http server from other pods with these
commands:

.test ingress traffic
----
#from master
dbserverIP=10.47.255.233
webserverIP=10.47.255.234
kubectl exec -it client1-dev -n dev -- curl http://$webserverIP -m5
kubectl exec -it client2-dev -n dev -- curl http://$webserverIP -m5
kubectl exec -it client-qa -n qa -- curl http://$webserverIP -m5
kubectl exec -it client-jtac -n jtac -- curl http://$webserverIP -m5
kubectl exec -it dbserver-dev -n dev -- curl http://$webserverIP -m5

#from node cent222 (fabric interface IP: 10.169.25.20)
curl http://$webserverIP -m5
#from node cent333 (fabric interface IP: 10.169.25.21)
curl http://$webserverIP -m5
----

these commands triggers the HTTP requests to the `webserver-dev` pod from all
clients and the hosts of the 2 nodes. `-m5` curl command option make curl to
wait maximum 5 seconds for the response before it claims time out. as expected,
all accesses pass through and returns the same output as below:

.from `client1-dev`:
----
$ kubectl exec -it client1-dev -n dev -- \
    curl http://$webserverIP | w3m -T text/html | grep -v "^$"
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.234
                             Hostname = webserver-dev
----

in the command above, `w3m` get the output from curl which returns a webpage
HTML code and renders into readable text, then send to grep to remove the empty
lines. to make the command shorter we define an alias:

    alias webpr='w3m -T text/html | grep -v "^$"'

now the command looks shorter:

----
$ kubectl exec -it client1-dev -n dev -- curl http://$webserverIP | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.234
                           Hostname = webserver-dev
----

////
----
$ kubectl exec -it client1-dev -n dev -- curl http://10.47.255.234
<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.234<br>Hostname = webserver-dev</h3>
  </body>
  </div>
</html>
----
////

similarly, we'll get the same test results for access to `dbserver-dev` from any
other pods.

////
----
#from master
kubectl exec -it client1-dev -n dev -- curl http://10.47.255.233 -m5
kubectl exec -it client2-dev -n dev -- curl http://10.47.255.233 -m5
kubectl exec -it client-qa -n qa -- curl http://10.47.255.233 -m5
kubectl exec -it client-jtac -n jtac -- curl http://10.47.255.233 -m5
kubectl exec -it webserver-dev -n dev -- curl http://10.47.255.233 -m5

#from node cent222 (fabric interface IP: 10.169.25.20)
curl http://10.47.255.233 -m5
#from node cent333 (fabric interface IP: 10.169.25.21)
curl http://10.47.255.233 -m5
----
////

//TODO: say something about accessing pod from host?

==== create kubernetes network policy

now lets create the k8s network policy to implement our design. from out initial
design goal, these are what we wanted to achieve via network policy:

* `client1-dev` and pods under `jtac` NS (that is `jtac-dev` pod) can access
  `webserver-dev` pod
* `webserver-dev` pod (and only it) is allowed to access `dbserver-dev` pod
* all other client pods are not allowed to access the two server pods
* all other client pods can still communicate with each other

translating these requirements into language of kubernetes network policy, we'll
have this network policy yaml file:

----
#policy1-do.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy1
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: webserver-dev
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.169.25.20/32
    - namespaceSelector:
        matchLabels:
          project: jtac
    - podSelector:
        matchLabels:
          app: client1-dev
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: dbserver-dev
    ports:
    - protocol: TCP
      port: 80
----

from the network-policy definition, based on what you've learned in chapter 3,
you should easily tell what the policy is trying to enforce in our current
setup:

* according to the ingress policy, the following clients can reach the
  `webserver-dev` server pod located in `dev` namespace:

    - `client1-dev` from `dev` namespace
    - all pods from `jtac` namespace, that is `client-jtac` pod in our setup
    - clients with source IP `10.169.25.20` (`cent222` in our setup) 

* according to the egress policy, the `webserver-dev` server pod in `dev`
  namespace can initiate a TCP session towards `dbserver-dev` pod with
  destination port 80 to access the data.

* for target pod `server-dev`, all other accesses are denied.

* communication between all other pods are not affected by this network policy.

TIP: actually, this is the exact network policy yaml file that we've
demonstrated in chapter 3.

let's create the policy and verify its effect.

----
$ kubectl apply -f policy1-do.yaml
networkpolicy.networking.k8s.io/policy1 created

$ kubectl get networkpolicies --all-namespaces
NAMESPACE   NAME       POD-SELECTOR        AGE
dev         policy1    app=webserver-dev   17s
----

==== post kubernetes network policy creation

===== ingress policy on `webserver-dev`

after network policy `policy1` is created, let's test the accessing of http
server in `webserver-dev` pod from pod `client1-dev`, `client-jtac` and node
`cent222` host:

----
$ kubectl exec -it client1-dev -n dev -- curl http://$webserverIP | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.234
                           Hostname = webserver-dev
----

the access from these 2 pod to `webserver-dev` is OK and that is what we
want. now if we repeat the same test from other pods `client2-dev`, `client-qa`
and another node `cent333` now get timed out:

----
$ kubectl exec -it client2-dev -n dev -- curl http://$webserverIP -m 5
curl: (28) Connection timed out after 5000 milliseconds
command terminated with exit code 28

$ kubectl exec -it client-jtac -n jtac -- curl http://$webserverIP -m 5
curl: (28) Connection timed out after 5000 milliseconds
command terminated with exit code 28

$ curl http://$webserverIP -m 5
curl: (28) Connection timed out after 5000 milliseconds
----

the new test result after network policy applied is illustrated in this figure:

.network policy: after applying policy1
image::policy-afterpolicy.drawio.png[]

detail information of the network policy object tells the same things:

----
$ kubectl describe netpol -n dev policy1
Name:         policy1
Namespace:    dev
Created on:   2019-09-29 21:21:14 -0400 EDT
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
            {"apiVersion":"networking.k8s.io/v1","kind":"NetworkPolicy",
            "metadata":{"annotations":{},"name":"policy1","namespace":"dev"},
            "spec":{"egre...
Spec:
  PodSelector:     app=webserver-dev
  Allowing ingress traffic:     #<---
    To Port: 80/TCP
    From:
      IPBlock:
        CIDR: 10.169.25.20/32
        Except:
    From:
      NamespaceSelector: project=jtac
    From:
      PodSelector: app=client1-dev
  Allowing egress traffic:
    To Port: 80/TCP
    To:
      PodSelector: app=dbserver-dev
  Policy Types: Ingress, Egress
----

From the above exercise, we can conclude that k8s network policy works as
expected in contrail.

but out test is not done yet. in the network policy we defined both ingress and
egress policy, but so far from `webserver-dev` pod perspective we've only tested
that the ingress policy of `policy1` works successfully. secondly, we have not
apply any policy to the other server pod `dbserver-dev`. according the default
"allow any" policy, any pods can directly access it without a problem.
obviously this is not what we wanted according to our original design. another
ingress network policy is needed for `dbserver-dev` pod. and at last, we need to
apply an egress policy to `dbserver-dev` to make sure it can't connect to any
other pods. so there are at least three more test items we need to confirm:

* test egress policy of `policy1` applied to `webserver-dev` pod
* define and test ingress policy for `dbserver-dev` pod
* define and test egress policy for `dbserver-dev` pod

let's look at the egress policy of `policy1` first.

===== egress policy on `webserver-dev` pod

.test egress traffic
----
#test access to all pods
kubectl exec -it webserver-dev -n dev -- curl http://$dbserverIP -m5
kubectl exec -it webserver-dev -n dev -- curl http://<other pod IPs> -m5

#test access to all ipBlock
kubectl exec -it webserver-dev -n dev -- curl http://10.169.25.20 -m5
kubectl exec -it webserver-dev -n dev -- curl http://10.169.25.21 -m5
----

the result shows only access to `dbserver-dev` succeeds. all other egress access
is timed out.

----
$ kubectl exec -it webserver-dev -n dev -- curl $dbserverIP -m5 | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.233
                            Hostname = dbserver-dev
$ kubectl exec -it webserver-dev -n dev -- curl 10.47.255.232 -m5
curl: (28) Connection timed out after 5001 milliseconds
command terminated with exit code 28
----

===== network policy on `dbserver-dev` pod

so far so good. let's look at the second test items: ingress access to
`dbserver-dev` pod from other pods other than `webserver-dev` pod:

.test egress traffic
----
#test access to all pods
kubectl exec -it webserver-dev -n dev -- curl http://$dbserverIP -m5
kubectl exec -it client1-dev -n dev -- curl http://$dbserverIP -m5
kubectl exec -it client2-dev -n dev -- curl http://$dbserverIP -m5
kubectl exec -it client-jtac -n dev -- curl http://$dbserverIP -m5
kubectl exec -it client-qa -n dev -- curl http://$dbserverIP -m5

#test access to all ipBlock
#from node cent222 (fabric interface IP: 10.169.25.20)
curl http://10.47.255.234 -m5
#from node cent333 (fabric interface IP: 10.169.25.21)
curl http://10.47.255.234 -m5
----

all pods can access `dbserver-dev` pod directly:

----
$ kubectl exec -it client1-dev -n dev -- curl http://$dbserverIP -m5 | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.233
                            Hostname = dbserver-dev
----

our design is to block access from all pods except `webserver-dev` pod. for that
we need to apply another policy. here is the yaml file of the second policy:

----
#policy-do2.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy2
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: dbserver-dev
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webserver-dev
    ports:
    - protocol: TCP
      port: 80
----

this network policy `policy2` is pretty much like the previous network policy
`policy1`, except that it looks simpler - the `policyTypes` only has `Ingress`
in the list so it will only define an ingress policy. and that ingress policy
defines a whitelist using only a `podSelector`. in our test case, only one pod
`webserver-dev` has the matching label with it so it will be the only one
allowed to initiate the TCP connection toward target pod `dbserver-dev` on port
80. let's create the policy `policy2` now and very the result again:

----
$ kubectl exec -it webserver-dev -n dev -- curl http://$dbserverIP -m5 | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.233
                            Hostname = dbserver-dev

$ kubectl exec -it client1-dev -n dev -- curl http://$dbserverIP -m5 | webpr
command terminated with exit code 28
curl: (28) Connection timed out after 5002 milliseconds
----

now the access to `dbserver-dev` pod is secured!

===== egress policy on `dbserver-dev`

there is the one last requirement we haven't meet in our designed goal. server
`dbserver-dev` should not be able to initiate any connection toward other nodes.
which is the final item we need to test in our plan.

now when you review our policy `policy2`, you may wonder how do we make that
happen. we've highlighted in chapter 3 that network policy is "whitelist" based
only by design. so what ever you put in the whitelist means "allowed". only a
"blacklist" gives a "deny", but even with a blacklist you won't be able to list
all other pods just to get them denied. 

another thinking is to make the use of the "deny all" implicit policy. so
assuming this sequence of policies in current kubernetes network policy design:

1. policy2 on `dbserver-dev`
2. deny all for `dbserver-dev`
3. "allow all" for other pods

it looks like if we give an "empty" whitelist in egress policy of
`dbserver-dev`, then nothing will be allowed and the 'deny all' policy for
target pod will come into play. problem is how do we define an "empty"
whitelist?

----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy2-tryout
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: dbserver-dev
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webserver-dev
    ports:
    - protocol: TCP
      port: 80
  engress:      #<---
----

turn out this doesn't work as expected.

----
$ kubectl exec -it dbserver-dev -n dev -- curl http://10.47.255.232 -m5 | webpr
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.232
                            Hostname = client1-dev
----

checking into the policy object detail does not uncover anything obviously
wrong.

----
$ kubectl describe netpol policy2-tryout -n dev
Name:         policy2-tryout
Namespace:    dev
Created on:   2019-10-01 17:02:18 -0400 EDT
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"networking.k8s.io/v1","kind":"NetworkPolicy",
                "metadata":{"annotations":{},"name":"policy2-tryout",
                "namespace":"dev"},"spec"...
Spec:
  PodSelector:     app=dbserver-dev
  Allowing ingress traffic:
    To Port: 80/TCP
    From:
      PodSelector: app=webserver-dev
  Allowing egress traffic:
    <none> (Selected pods are isolated for egress connectivity)         #<---
  Policy Types: Ingress
----

the problem is on the `policyTypes` here. we haven't added the `Egress` in. that
is why in the object detail information above the "Policy Types: Ingress" is
shown. whatever configured in egress policy will be ignored. simply adding `-
Egress` in `policyTypes` will fix it. furthermore, to express an "empty"
whitelist the `egress:` keyword is optional and not required. below is the new
policy yaml file:

----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy2-egress-denyall
  namespace: dev
spec:
  podSelector:
    matchLabels:
      app: dbserver-dev
  policyTypes:
  - Ingress
  - Egress      #<---
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: webserver-dev
    ports:
    - protocol: TCP
      port: 80
----

now delete the old policy `policy2` and apply this new policy, request from
`dbserver-dev` to any other pods (for example pod `client1-dev`) will be blocked:

----
$ kubectl exec -it dbserver-dev -n dev -- curl http://10.47.255.232 | webpr
command terminated with exit code 28
curl: (7) Failed to connect to 10.47.255.232 port 80: Connection timed out
----

here is the final diagram illustrating our network policy test result:

.network policy: after applying an empty egress policy on `dbserver-dev` pod
image::policy-afterpolicy-db-egress.drawio.png[]

===== the drop action in flow table 

before we conclude the test, lets take a look at the vrouter flow table when a
traffic is dropped by the policy.

on node `cent333` where pod `dbserver-dev` is located:

----
$ docker exec -it vrouter_vrouter-agent_1 flow --match 10.47.255.232:80
Flow table(size 80609280, entries 629760)

Entries: Created 33 Added 33 Deleted 30 Changed 54Processed 33 Used Overflow entries 0
(Created Flows/CPU: 7 9 11 6)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.232]:80)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   158672<=>495824       10.47.255.232:80                                    6 (5)
                         10.47.255.233:42282
(Gen: 1, K(nh):59, Action:D(Unknown), Flags:, TCP:Sr, QOS:-1, S(nh):63,
 Stats:0/0,  SPort 54194, TTL 0, Sinfo 0.0.0.0)

   495824<=>158672       10.47.255.233:42282                                 6 (5)
                         10.47.255.232:80
(Gen: 1, K(nh):59, Action:D(FwPolicy), Flags:, TCP:S, QOS:-1, S(nh):59,
 Stats:3/222,  SPort 52162, TTL 0, Sinfo 8.0.0.0)
----

the `Action:D` is set to `D(FwPolicy)` which means DROP due to Firewall Policy.
meanwhile, in the other node `cent222` where the destination pod `client1-dev`
is located, we don't see any flow generated, indicating the packet does not
arrive at all.

----
$ docker exec -it vrouter_vrouter-agent_1 flow --match 10.47.255.233
Flow table(size 80609280, entries 629760)
......
Listing flows matching ([10.47.255.233]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
----

////
----
$ docker exec -it vrouter_vrouter-agent_1 flow --match 10.47.255.234:80
Flow table(size 80609280, entries 629760)

Entries: Created 340 Added 336 Deleted 280 Changed 331Processed 340 Used Overflow entries 0
(Created Flows/CPU: 91 84 65 100)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.234]:80)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    11148<=>242676       10.47.255.234:80                                    6 (4)
                         10.47.255.230:35778
(Gen: 1, K(nh):52, Action:D(Unknown), Flags:, TCP:Sr, QOS:-1, S(nh):52,
 Stats:0/0,  SPort 62120, TTL 0, Sinfo 0.0.0.0)

   242676<=>11148        10.47.255.230:35778                                 6 (4)
                         10.47.255.234:80
(Gen: 1, K(nh):65, Action:D(FwPolicy), Flags:, TCP:S, QOS:-1, S(nh):65,
 Stats:6/444,  SPort 61489, TTL 0, Sinfo 12.0.0.0)
----
////

=== contrail implementation details

//https://www.juniper.net/documentation/en_US/contrail5.1/topics/concept/kubernetes-cni-contrail.html#jd0e333

we've introduce that contrail implements kubernetes network policy by Contrail
firewall security Policy. you also knows that Kubernetes `labels` are exposed as
`tags` in contrail. these tags are used by contrail security policy to implement
specified Kubernetes policies. tags will be created automatically from
kubernetes objects labels or created in the UI manually. 
//The newly-defined tags can be used to create and enforce policies in Contrail
//firewall security policy.

In this section we'll take a closer look at the contrail firewall policies,
policy rules, and the tags. especially, we'll examine the mapping relationships
between the kubernetes object that we created and tested in the last section,
and the corresponding contrail objects in contrail firewall system.

Contrail Firewall is designed with a hierarchical structure:

* the top level object is named "Application Policy Set", abbreviated as `APS`
* `APS` has Firewall Policies; 
* Firewall Policy has Firewall Rules; 
* Firewall rules has the endpoints; 
* Endpoints can be identified via tags or address groups (CIDRs).  

the structure is illustrated in this figure:

.contrail firewall
image::policy-arch.drawio.png[]

==== construct mappings

Kubernetes network policy and contrail firewall policy are two different
entities in terms of the semantics in which network policy is specified in each.
in order for contrail firewall to implement kubernetes network policy, contrail
needs to implement the one to one mapping for a lot of data construct from
kubernetes to contrail firewall. these data constructs are the basic building
blocks of kubernetes network policy and the corresponding contrail firewall
policy.

Below table represents kubernetes network policy constructs and the
corresponding constructs in contrail:

[options="header,autowidth"]
|====
|K8s Network Policy Construct   |Contrail Firewall Construct
|Cluster Name	                |APS (one per k8s cluster)
|Network Policy	                |Firewall Policy (one per k8s network policy)
|Ingress and Egress policy rule |Firewall Rule (one per k8s ingress/egress policy rule)
|CIDR	                        |Address Group(one per k8s network policy CIDR )
|Label	                        |Tag (one for each k8s label)
|Namespace                      |Custom Tag (one for each namespace)
|==== 

`contrail-kube-manager`, the `KM`, as we've read many time earlier in this book,
does all of the translations between the two worlds. basically the following
will happen in the context of kubernetes network policy: 

. `KM` will create a APS with Kubernetes cluster name during its initialing
  process. typically the default kubernetes cluster name is `k8s`, so you will see
  an APS with the same name in your cluster
. `KM` registers to `kube-apiserver` to watch the network policies events.
. Whenever a kubernetes network policy is created, a corresponding contrail
  firewall policy will be created with all matching firewall rules and network
  endpoints.
. for each `label` in kubernetes object there will be a corresponding contrail
  `tag` created
. based on the `tag`, the corresponding contrail objects (VN, pods, VMI,
  projects, etc) can be located.
. contrail will then apply the contrail firewall policies and rules in the APS on
  the contrail objects, this is how the specific traffic is permited or denied.

////
. Whenever namespace is created, `KM` will associate the APS to the
   default VNs belonging to the namespace. 
. Whenever pod is created, in contrail corresponding "tag" will be created.
////

[NOTE]
====
APS can be associated to different contrail objects, e.g.

* VMI(virtual machine interface)
* VM (virtual machine) or pods
* VN (virtual network)
* project

In contrail kubernetes cluster, it is associated to virtual network. Whenever
traffic goes on those networks, firewall policies associated on the APS would be
evaluated and respective action would be taken for the traffic.
====

in the previous section, we have created two kubernetes network policies in our
usage case.  now lets explore the contrail objects that are created for these
kubernetes network policies.

==== Application Policy Set (APS)

As mentioned above, `contrail-kube-manager` will create an Application Policy
Set(APS) using the kubernetes cluster name during the initialization stage. in
chapter 3 when we introduce "Contrail Namespaces and Isolation", we've learned
the cluster name is `k8s` by default in contrail. therefore the APS name will
also be `k8s` in the contrail UI.

.contrail UI: APS: configure -> Security -> Global Policies -> "Application Policy Sets"
//image::https://user-images.githubusercontent.com/2038044/65438012-d1037680-ddf2-11e9-9e21-fb1498b95310.png[]
image::https://user-images.githubusercontent.com/2038044/65655412-22298b00-dfea-11e9-850d-ee6a71833d45.png[]

//TODO: what is it
There is one more APS `default-application-policy-set` which is created by default. 

==== policies

now clicking on the "Firewall Policies" to display all firewall polices in the
cluster. in our test environment, you will find the following policies
available:

* k8s-dev-policy1
* k8s-dev-policy2
* k8s-denyall
* k8s-allowall
* k8s-Ingress

.contrail UI:"Firewall Policies"
//image::https://user-images.githubusercontent.com/2038044/65438014-d1037680-ddf2-11e9-8c0b-3c6220cce87c.png[]
//image::https://user-images.githubusercontent.com/2038044/65655766-505b9a80-dfeb-11e9-9499-b8d9b67bbcc9.png[]
image::https://user-images.githubusercontent.com/2038044/65892424-48e21b80-e374-11e9-9ed3-2a1cf9a8e07d.png[]

===== contrail firewall policy naming convention

the `k8s-dev-policy1` and `k8s-dev-policy2` policies are what we've created.
although they looks different from the object name we gave in our yaml file, it
is easy to tell which one is which. when `KM` creates the contrail firewall
policies based on the kubernetes network policies, it prefixes the firewall
policy name with the cluster name, plus the namespace, in front of our network
policy name:

    <cluster name>-<namespace-name>-<kubernetes network policy name>

this sounds familiar. earlier we've showed how `KM` name the VN in contrail UI
after the kubernetes VN objects name we created in yaml file. 

the `K8s-ingress` firewall policy is created for the ingress loadbalancer.
basically this is to ensure the Ingress to work properly in contrail. the detail
explanation is out of the scope of this book, so we can ignore it here.

===== the `k8s-allowall` and `k8s-denyall` firewall policy

but the bigger question is that why we still see 2 more firewall policies here,
because we had never created any network policies like `allowall`, or `denyall`?

remember when we introduce kubernetes network policy in chapter 3, we've
mentioned kubernetes network policy uses "whitelist" method and the implicit
"deny all" and "allow all" policies. the nature of "whitelist" method indicates
"deny all" action for all traffic other than what is added in the whitelist,
while the implicit "allow all" behavior make sure a pod that is not involved in
any network policies can continue its "allow-any-any" traffic model. the problem
with contrail firewall regarding these implicitness is that, by default it
follows a "deny all" model - anything that is not explicitly defined will be
blocked.  that is why in contrail implementation, these two corresponding
implicit network policies are honored by two explicitly policies generated by
the `KM` module.

//TODO: add a diagram, to show mapping between k8s implicit policies and
//contrail explicit policies
////
YUVA:
By default firewall policy `k8s-denyall` and `k8s-allowall` and `K8s-ingress`
will be created.  Since k8s by default "allows all" and contrail firewall by
default "denies all", contrail has to have rules to match k8s default bahaviour.
So by default firewalls rules are added in k8s-allowall firewall policy. Below
contrail UI snapshots shows it.
////

one question may be raised at this point. with multiple firewall policies, which
one should be applied and evaluated first and which ones afterward? in another
word, in what "sequence" shall contrail apply and evaluate each policy?
firewall policies evaluation with a different sequence will lead to completely
different result. just imagine these two sequences "denyall - allowall" vs
"allowall- denyall", the former give a pass to all other pods, while the latter
give a stop.

the anwer is the "sequence number".

===== sequence number

When firewall polices in an APS are evaluated, it has to be evaluated in a
certain sequence. all firewall polices and all firewall rules (will come to this
soon) in each of the policy has a `sequence number`.  When there is a matching
policy, it will be executed, and the evaluation will stop. it is again
`contrail-Kube-manager` that allocates the right sequence number for all
firewall policies and firewall rules, so that everythings works in correct
order.  the process is automatically done without manual intervention.  we don't
have to worry about these things when we create the kubernetes network policies.

we'll visit sequence number again later, now let's look at the rules defined in
the firewall policy.

==== firewall policy rules

in the same view of "Firewall policies" list, in the right side we see number of
"Rules" for each policy:

.contrail UI:"Firewall Policy rules"
//image::https://user-images.githubusercontent.com/2038044/65438015-d1037680-ddf2-11e9-8a40-628b2c60a087.png[]
//image::https://user-images.githubusercontent.com/2038044/65655828-8e58be80-dfeb-11e9-867e-5545f9192713.png[]
image::https://user-images.githubusercontent.com/2038044/65898408-813b2700-e37f-11e9-840a-d111a69f1d88.png[]

===== rules in `k8s-dev-policy1` firewall policy

there are 4 rules for the `k8s-dev-policy1` policy. Clicking at it we will see
the rules in detail:

.contrail UI:"k8s-dev-policy1" rules
//image::https://user-images.githubusercontent.com/2038044/65438016-d1037680-ddf2-11e9-8418-9755d3a763bc.png[]
//image::https://user-images.githubusercontent.com/2038044/65656030-53a35600-dfec-11e9-95f8-1c65a5ed22e8.png[]
image::https://user-images.githubusercontent.com/2038044/65898936-9c5a6680-e380-11e9-959c-b65df8924735.png[]

doesn't it look familiar with our kubernetes network policy `policy1` that
we've tested? let's put the rules displayed in the screnshot into a table:

[options="header,autowidth"]
|====
|rule#|Action |Services |End Point1                         |Dir |End Point2                         |Match Tags
|1    |pass   |tcp:80   |project=jtac                       |>   |app=webserver-dev && namespace=dev |-
|2    |pass   |tcp:80   |app=client1-dev && namespace=dev   |>   |app=webserver-dev && namespace=dev |-
|3    |pass   |tcp:80   |app=webserver-dev && namespace=dev |>   |app=dbserver-dev  && namespace=dev |-
|4    |pass   |tcp:80   |Address Group: 10.169.25.20/32     |>   |app=webserver-dev && namespace=dev |-
|====

the first column is the rule number that we added, all other columns are
imported from the UI screenshot. now compare it with the kubernetes object
information:

----
$ kubectl get netpol --all-namespaces -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: NetworkPolicy
  metadata:
    ......
  spec:
    egress:
    - ports:
      - port: 80
        protocol: TCP
      to:
      - podSelector:            #<---rule#3
          matchLabels:
            app: dbserver-dev
    ingress:
    - from:
      - ipBlock:                #<---rule#4
          cidr: 10.169.25.20/32
      - namespaceSelector:      #<---rule#1
          matchLabels:
            project: jtac
      - podSelector:            #<---rule#2
          matchLabels:
            app: client1-dev
      ports:
      - port: 80
        protocol: TCP
    podSelector:
      matchLabels:
        app: webserver-dev
    policyTypes:
    - Ingress
    - Egress
----

all rules we see in firewall policy `k8s-dev-policy1` matches with rules in
kubernetes network policy `policy1`.

////
before we start, make sure you delete all network policies you created 
manually. in our environment we just need to remove the `policy1` and
`policy2` when we created for the previous test.

----
$ kubectl delete netpol policy2 -n dev
networkpolicy.extensions "policy2" deleted
$ kubectl delete netpol policy1 -n dev
networkpolicy.extensions "policy1" deleted
----

After the network policy creation,  k8s APS will have the new firewall policy
for the k8s netwok policy. 
////

===== rules in `k8s-denyall` firewall policy 

now let's go back and exmine the rules in `k8s-denyall` policy that `KM`
generated for our kubernetes network policies.

.contrail UI:"k8s-denyall" rules
//image::https://user-images.githubusercontent.com/2038044/65438018-d19c0d00-ddf2-11e9-8fc4-26fa2a06667f.png[]
//image::https://user-images.githubusercontent.com/2038044/65656361-9580cc00-dfed-11e9-8616-c979614054d1.png[]
image::https://user-images.githubusercontent.com/2038044/65900170-5a7eef80-e383-11e9-9d71-da107f89af5f.png[]

again we convert that into a table:

[options="header,autowidth"]
|====
|rule#|Action |Services |End Point1                         |Dir |End Point2                         |Match Tags
|1    |deny   |any:any  |app=webserver-dev && namespace=dev |>   |any                                |-
|2    |deny   |any:any  |any                                |>   |app=dbserver-dev && namespace=dev  |-
|3    |deny   |any:any  |any                                |>   |app=webserver-dev && namespace=dev |-
|====

the `k8s-alldeny` rules are simple. it just tell contrail to deny communication
with all other pods that is not in the whitelist. one thing worth to mention is
that there is a rule in the direction from `app=webserver-dev && namespace=dev`
to `any`, so that egress traffic is denied for `webserver-dev` pod, while there
is no such a rule from `app=dbserver-dev && namespace=dev` to `any`. if you
review our test in last section, in the original policy `policy2`, we did not
define an `Egress` option in `policyTypes` to deny egress traffic of
`dbserver-dev`, that is why when translated into contrail firewall there is no
such a rule either. if we change `policy2` to the new policy
`policy2-egress-denyall` and examine the same, we'll see the "missing" rule now:

.contrail UI:"k8s-denyall" rules
image::https://user-images.githubusercontent.com/2038044/66015096-3d3f4380-e49f-11e9-86ba-4707f19447bf.png[]

please pay attention that the "k8s-denyall" policy only apply to those "target"
pods - pods that are selected by the network policies. in our case it only
applies to pods `webserver-dev` and `dbserver-dev`. other pods like
`client-jtac` or `client-qa` will not be affected. instead, those pods will be
applied by `k8s-allowany` policy, which we will examine the next.


//TODO: why there is no egress policy defined in denyall, mostly because there
//is no Egress type defined, what will be behavior, say sth.
//the first and third rules say `webserver-dev` pod can NOT talk to `any` other pods.

===== rules in `k8s-allowall` firewall policy 

the `k8s-allowall` policy seems to have more rules than other policies:

.contrail UI:"k8s-allowall" rules
image::https://user-images.githubusercontent.com/2038044/65902980-8ef5aa00-e389-11e9-83c7-17ea5064f1c9.png[]

despite the number of rules, in fact `k8s-allowall` is the simplest one. it
works at the NS level and simply has two rules for each NS. in UI within the
"search" box, apply a namespace as the filter e.g. "jtac" or "qa", we'll see
these results:

.contrail UI:"k8s-allowall" rules filtered by NS "jtac"
image::https://user-images.githubusercontent.com/2038044/65903133-daa85380-e389-11e9-9f5a-70185741a2e0.png[]

.contrail UI:"k8s-allowall" rules filtered by NS "qa"
image::https://user-images.githubusercontent.com/2038044/65903184-f57ac800-e389-11e9-86aa-6db3050c39a7.png[]

basically what this policy says is: for those pods that does not has any network
policy applied yet, let's continue the kubernetes default "allow-any-any"
networking model and allow everything!

==== sequence number

after having explored the contrail firewall policy rules, let's come back to the
`sequence number` and see how it works exactly.

////
firewall-policies referred by the application-policy-set objects are ordered
using sequence number. We put a sequence number on the link from firewall-policy to
firewall-policy-rule objects. It is a string object, instead of integer. It
decides the order in which the rules are applied.
////

`sequence number` is a number attached in all firewall-policies and their rules.
it decides the order in which all policies are applied and evaluated, similarly,
in one particular policy, it decides the order in which all rules are applied
and evaluated. the lower the sequence number the higher the priority. to find
the `sequence number` we have to look into the firewall policy and policy rule
object attributes in contrail configuration database.  first let's explore the
firewall policy object in APS to check their `sequence number`.

TIP: in chapter 5 we've used `curl` command to pull the
loadbalancer object data when we introduce `service`. here we use "Config
Editor" to do the same. 

===== sequence number in firewall policies
.contrail UI:sequence number for policies: "setting" -> "Config Editor"
//image::https://user-images.githubusercontent.com/2038044/65438021-d19c0d00-ddf2-11e9-92a8-76998b831776.png[]
//image::https://user-images.githubusercontent.com/2038044/65656872-5ce1f200-dfef-11e9-8c09-bfadec3b5f82.png[]
image::https://user-images.githubusercontent.com/2038044/65905817-4fca5780-e38f-11e9-819d-05578602d5d0.png[]

.contrail UI:sequence number for policies: (continue)
image::https://user-images.githubusercontent.com/2038044/65907018-886b3080-e391-11e9-9323-afce02512667.png[]

in these screenshots, under APS `k8s` there are all the 5 policies that we've
saw.  for example, the policy `k8s-dev-policy1` which maps to the kubernetes
network policy `policy1` that we explicitly defined, and the policy
`k8s-denyall` which is what the system automatically generated. in the figure it
shows `k8s-dev-policy1` and `k8s-denyall` has a sequence number as "00038.0" and
"00042.0" respectively. therefore, `k8s-dev-policy1` has a higher priority and
it will be applied and also evaluated first. that means the traffic types we
defined in whitelist will be allowed first, then all other traffic to or from
the target pod will be denied. this is the exact goal that we want to achieve.

all `sequence number` for all firewall policies are listed in below table, from
highest priority to the lowest:

[options="header,autowidth"]
|====
| seq#      | firewall policy
| `00002.0` |`k8s-Ingress`
| `00038.0` |`k8s-dev-policy1`
| `00040.0` |`k8s-dev-policy2`
| `00042.0` |`k8s-denyall`
| `00043.0` |`k8s-allowall`
|====

based on the sequence number, the application and evaluation order is the
explicit policies first, followed by the "deny all" policy and then the "allow
all" policy at the last. the same order as in kubernetes is honored.  next let's
check the sequence number in policy rules.

===== sequence number in firewall policy rules

as mentioned, in the same firewall policy, policy rules will also have to be
applied and evaluated in a certain order. in contrail firewall that is again
ensured by the sequence number. the sequence number in rules of firewall policy
`k8s-dev-policy1` is displayed in below figures:

.contrail UI:sequence number for rules: "setting" -> "Config Editor"
image::https://user-images.githubusercontent.com/2038044/65919094-5d8cd680-e3a9-11e9-872d-44619dda2d29.png[]

.contrail UI:sequence number for rules: (continue)
image::https://user-images.githubusercontent.com/2038044/65919172-857c3a00-e3a9-11e9-9c7a-8fe32cdba6a2.png[]

below table shows `sequence number` of all rules of firewall policy
`k8s-dev-policy1`, from highest priority to the lowest:

[options="header,autowidth"]
|====
| seq#      | firewall rule
|`00000.0`  |`dev-ingress-policy1-0-ipBlock-0-cidr-10.169.25.20/32-0`
|`00001.0`  |`dev-ingress-policy1-0-namespaceSelector-1-0`
|`00002.0`  |`dev-ingress-policy1-0-podSelector-2-0`
|`00003.0`  |`dev-egress-policy1-podSelector-0-0`
|====

comparing with our network policy yaml file configuration:

----
  ingress:
  - from:
    - ipBlock:
        cidr: 10.169.25.20/32           #<---seq# 00000.0
    - namespaceSelector:                #<---seq# 00001.0
        matchLabels:
          project: jtac
    - podSelector:                      #<---seq# 00002.0
        matchLabels:
          app: client1-dev
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:                      #<---seq# 00003.0
        matchLabels:
          app: dbserver-dev
    ports:
    - protocol: TCP
      port: 80
----

we can find that the rules `sequence number` is consistent with the sequence
they appear in the yaml file. in another word, rules will be applied and
evaluated in the same order as they are defined.

==== tag

//TODO: what? why? how?
we've been talking about the contrail `tags` and we already know that
`contrail-kube-manager` will translate each kubernetes label into a contrail
tag, which is attached to the respective port of a pod.

//image::https://user-images.githubusercontent.com/2038044/65438024-d19c0d00-ddf2-11e9-86a6-0f4c1acd7c8a.png[]
image::https://user-images.githubusercontent.com/2038044/65657044-e5f92900-dfef-11e9-9a6a-727ef93fb4e7.png[]

==== UI visualization

//TODO: don't quite understand it

Contrail UI provides nice visualization for security. It is self explanatory
if you know how contrail security works.

.Sample traffic visualization for the above policy with workload 
//image::https://user-images.githubusercontent.com/2038044/65438025-d19c0d00-ddf2-11e9-9855-2ab52e229a65.png[]
image::https://user-images.githubusercontent.com/2038044/65741634-d1359780-e0ba-11e9-87b0-f62a1952f76b.png[]

.Sample traffic visualization with more network policies
image::https://user-images.githubusercontent.com/2038044/65438027-d234a380-ddf2-11e9-87f8-6883bb66118c.png[]

