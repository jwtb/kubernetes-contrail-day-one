// vim:set ft=asciidoc cc=80 tw=80:
= Chapter 4: Kubernetes and Contrail integration 
:toc: right
:toclevels: 3
//:toc-placement: preamble
:source-highlighter: pygments
:source-highlighter: coderay
:source-highlighter: prettify
:highlightjs-theme: googlecode
:coderay-linenums-mode: table
:coderay-linenums-mode: inline
:numbered:

This chapter demonstrates the how kubernetes objects works in contrail setup.

== Contrail-Kubernetes architecture 
=== Why contrail with Kubernetes ?

Now after we have seen the main concepts of Kubernetes in chapter 2 and 3, what
could be the gain in adding Contrail to standard Kubernetes deployment ?

in brief, Contrail offers common deployment for multiple environments
(OpenStack, Kubernetes, etc) as well it enriches Kubernetes networking and
security capabilities.

.multiple environment integration

When it comes to deployment for multiple environments, Yes containers is the
current trend to build applications, but don’t expect everyone to migrate
everything from VM to containers that fast (This is not to mention the nested
approach where containers are hosted in VM). if we add to the picture
workload fully or partially run in the public cloud, we end up feeling the
misery for network and security administrators where Kubernetes becomes just
one thing to manage Network and security. 

administrator in many organization manage individual orchestrator/manager for
each environment. OpenStack or VMware NSX for VM, Kubernetes or Mesos for
Containers, AWS console.  and here what contrail could put the network and
security administrators out of their misery is it provides dynamic end-to-end
networking policy and control for any cloud, any workload, and any deployment.

from a single user interface contrail translates abstract workflows into
specific policies, simplifying the orchestration of virtual overlay connectivity
across all environments by building and securing virtual networks that connect
BMS, VM and Containers located in private or public cloud. 

A very common way to deploy Kubernetes is to launch its POD in VMs orchestrated
by OpenStack. this is one of the many use cases of contrail doing its magic.  in
this book we won’t cover contrail integration with other environments as we
focus only in Kubernetes. But any feature that we explain in here could be
extended for other environments.

.kubernetes enrichment

Then what we mean by contrail enriching standard Kubernetes deployment?

kubernetes by itself does not implement the networking, networking is handled in
a plugin called "Container Network Interface" - `CNI`. there are a lot of
opensourced CNI plugin by different providers in the industry. on the other
hand, kubernetes does impose some very fundamental requirements for any CNI
implementations. the most important part of the requirement is that pods on a
node must be able to communicate with all pods on all nodes **without** NAT.
that indicates a "flat" pod network. 

contrail is one of such a CNI plugin. you can refer to
https://kubernetes.io/docs/concepts/cluster-administration/networking/
for more available CNI plugins.

with CNI plugins, 
Kubernetes offers flat network connectivity with some security feature confined
in a cluster, but Contrail could offer on top of that:

. namespaces and services customized isolations for segmentations and
  multi-tenancy
. service chaining
. distributed loadbalancing and firewall with extensive centralized flow and
  logs insight 
. rich security policy using tags that can extend to other environment
  (OpenStack, VMWare, BMS, AWS ,..,etc) 

In this chapter we will cover some of these aspects, but first let’s talk about
Kubernetes/contrail architecture and the object mapping 

=== contrail-kube-manager

A new components of contrail has been added called `contrail-Kube-manager`. what
it does basically is to watch kubernetes apiserver events, and translate
kubernetes objects into Contrail controller object. the following figure
illustratesthe basic work flow:

.contrail kubernetes architecture

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/kubemanager.png[]
//image::https://raw.githubusercontent.com/pinggit/kubernetes-contrail-day-one/master/diagrams/kubemanager.png?token=AAPRSHE5SF522ETPA6NAUDK5D7PHS[]

image::https://user-images.githubusercontent.com/2038044/60748771-5f3c2f00-9f5f-11e9-9f72-1f28794c8d5b.png[]

=== kubernetes to contrail object mapping

So not much of change of the regular contrail that we have seen before and all
of that is happening behind the scene.
what we have to be aware of it before dealing with Kubernetes/contrail is the
object mapping. because contrail is single interface managing multiple
environments - as explained before – each environment has its own acronym and
terms hence the need for this mapping
 
For example, Namespace in Kubernetes are intended for segmentation between
multiple teams, or projects as if we are creating virtual cluster. In contrail
the similar concept would be named as project so when you create a namespace in
Kubernetes it will automatically create an equivalent project in contrail. more
on that will come later on for now kindly make yourself familiar with this list
of object mapping 

.contrail kubernetes object mapping

//image::https://github.com/pinggit/kubernetes-contrail-day-one/blob/master/diagrams/chapter%204%20contrail%20-%20k8s%20mapping.png[]

image::https://user-images.githubusercontent.com/2038044/60748774-6bc08780-9f5f-11e9-91ae-2ec496cab987.png[]

////

=== contrail-kube-manager

.contrail

image::https://user-images.githubusercontent.com/2038044/59642949-fb2f0380-9134-11e9-86d2-1035e5b901b7.png[]

.kubernetes
image::https://user-images.githubusercontent.com/2038044/59642835-94a9e580-9134-11e9-9053-80505cb1ba75.png[]

.contrail kubernetes
image::https://user-images.githubusercontent.com/2038044/59642699-1a796100-9134-11e9-8a58-fb529b329cba.png[]

////


== contrail namespaces and isolation

=== namespace introduction

In chapter3 you`ve read about `namespace` or `NS` in kubernetes, and how to use
a `quota` to apply some constraints to the resource utilization by a NS. in the
beginning of this chapter we've mentioned object mappings between kubernetes and
contrail. in this section we'll see how NS works in contrail environments and
how contrail extends the feature.

one analogy we`ve given when introducing `namespace` concept is openstack
`project`, or `tenant`. that is exactly how contrail is looking at it. whenever
a new `namespace` object is created, `contrail-kube-manager` gets noticed about
the object creation event and it will create the corresponding `project` in
contrail api database. to differiciate between multiple kubernetes NS project in
contrail, a kubernetes cluster name will be added to the kubernetes NS or
project name. the default kubernetes cluster name is 'k8s'.  so if you create a
kubernetes NS `ns-user-1`, what you will end up to see in contrail GUI will be:
`k8s-ns-user-1`:

.contrail command: projects
image::https://user-images.githubusercontent.com/2038044/60316467-8fb91300-9938-11e9-9de6-429b56429868.png[]

****
the kubernetes `cluster name` is configurable, during deployment process. if you
don't configure it `k8s` will be the default. once the cluster is created, the
name can not be changed anymore. to view the `cluster name`, go to
`contrail-kube-manager` and it can be found in the configuration file.

----
$ docker exec -it kubemanager_kubemanager_1 grep cluster /etc/contrail/contrail-kubernetes.conf
cluster_name=k8s        #<---
cluster_project={}
cluster_network={}
----

****

NOTE: in the rest part of this book we will refer all these terms `namespace`,
`NS`, `tenant`, `project` interchangeably.

=== Non-isolated NS


after previous section now you are aware that kubernetes basic networking
requirement is a "flat"/"NATless" network - any pod can talk to any pod in any
namespace, any cni providers should ensure that. consequently in kubernetes by
default all namespaces are simply **not** isolated. It is just like a "shared
tenant" in openstack.

.k8s-default-pod-network and k8s-default-service-network

To provide networking for all non-isolated namespace, there should be a
**common** VRF (virtual routing and forwarding table) or RI (routing instance).
in contrail kubernetes environment, two "default" VNs are pre-configured in k8s
default NS project, for pod and service respectively. correspondingly there are
2 VRFs each with same names as their correspondingly VN. 

the name of the two VNs/VRFs are in this format:

    <k8s-cluster-name>-<namespace name>-<pod-network|service-network>

so for default NS the name will become:

* `k8s-default-pod-network`: pod VN/VRF, with the default subnet `10.32.0.0/12`
* `k8s-default-service-network`: service VN/VRF, with a default subnet `10.96.0.0/12`

NOTE: the default subnet for pod or service is configurable.

it is important to know that these 2 default VNs are **shared** between all of
the "non-isolated" namespaces. what that means is, they will be available for
any new non-isolated NS that you create, implicitly.  that is why in any
non-isolated NS and `default` NS, the communication between pods in the default
pod network works fine. 

NOTE: this only applies to the "default" VNs. all new VNs that you create will
be isolated with other VNs, regardless of same or different NS. additional
configuration is needed to make pods belonging two different VNs to communicate
with each other.

for the isolated NS, however, it will be a different story.

=== Isolated NS 

in contrast, "isolated" namespace, will have its own default pod-network and
service-network, accordingly two new VRFs are also created for each "isolated"
namspace. The same flat-subnets `10.32.0.0/12` and `10.96.0.0/12` are shared by
the pod and service networks in the isolated namespaces. however since the
networks are with a different VRF, by default it is isolated with other NS.
therefore, by default pods launched in isolated NS can only talk to service and
pods on the same namespace. Additional configurations, e.g. policy, is required
to make the pod being able to reach the network outside of current namespace.

////
TODO: give a diagram, otherwise confusing

default-domain:k8s-default:k8s-default-pod-network:k8s-default-pod-network
default-domain:k8s-default:k8s-default-service-network:k8s-default-service-network
default-domain:k8s-default:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network

default-domain:k8s-ns-non-isolated:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network

default-domain:k8s-ns-user-1:k8s-ns-user-1-pod-network:k8s-ns-user-1-pod-network
default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
default-domain:k8s-ns-user-1:k8s-vn-left-1-pod-network:k8s-vn-left-1-pod-network
////

here is the yaml file to create an isolated namespace:

----
$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation" : "true"
  name: ns-isolated
----

to create the NS:

----
kubectl create -f ns-isolated.yaml

$ kubectl get ns
NAME          STATUS    AGE
contrail      Active    8d
default       Active    8d
development   Active    2d
ns-isolated   Active    1d  #<---
kube-public   Active    8d
kube-system   Active    8d
----

the annotations under metadata are something additional comparing to standard
(non-isolated) k8s namespace, the value of `true` indicates this is an isolated
NS:

  annotations:
    "opencontrail.org/isolation" : "true"

this part of the definition is Juniper's extension. `contrail-kube-manager`
reads the namespace `metadata` from `kube-apiserver`, parses the information
defined in the `annotations` object, and sees that the `isolation` flag is set
to `true`. it then creates the tenant with the correponding routing
instances(one for pod and one for service) instead of using the default ns
routing instances for the isolated namespace. fundamentally that is how the
"isolation" is implemented. 

in the following sections we'll verify how the routing isolation works.

=== communication between pod in different namespaces

create a non-isolated namespace and an isolated namespace:

----
$ cat ns-non-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ns-non-isolated

$ cat ns-isolated.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    "opencontrail.org/isolation": "true"
  name: ns-isolated

$ kubectl apply -f ns-non-isolated.yaml
namespace/ns-non-isolated created

$ kubectl apply -f ns-isolated.yaml
namespace/ns-isolated created

$ kubectl get ns | grep isolate
ns-isolated       Active   79s
ns-non-isolated   Active   73s
----

in both NS and the default NS, create a deployment to launch a pod:

----
$ kubectl apply -f deployment-cirros.yaml -n default
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-non-isolated
deployment.extensions/cirros created

$ kubectl apply -f deployment-cirros.yaml -n ns-isolated
deployment.extensions/cirros created

$ kubectl get pod -o wide -n default
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-tjfn6  1/1    Running  0         13s  10.47.255.242  cent333  <none>

$ kubectl get pod -o wide -n ns-non-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-nrxq6  1/1    Running  0         23s  10.47.255.248  cent222  <none>

$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
----

ping between all pods in 3 namespaces

----
#default ns to non-isolated new ns: succeed
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.248
PING 10.47.255.248 (10.47.255.248): 56 data bytes
64 bytes from 10.47.255.248: seq=0 ttl=63 time=1.600 ms
^C
--- 10.47.255.248 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.600/1.600/1.600 ms

#default ns to isolated new ns: fail
$ kubectl -n default exec -it cirros1-85fc7dd848-tjfn6 -- ping 10.47.255.239
PING 10.47.255.239 (10.47.255.239): 56 data bytes
^C
--- 10.47.255.239 ping statistics ---
3 packets transmitted, 0 packets received, 100% packet loss
----

the test result shows that, bidirectional communication between two non-isolated
namespaces (namespace `ns-non-isolated` and `default` in this case) works, but
traffic from non-isolated NS (`default` NS) toward isolated NS does not pass
through. what about traffic within the same isolated NS? 

with the power of the `deployment` we can quickly test it out: in isolated NS
`ns-isolated`, clone one more pod by `scale` the deployment with `replicas=2`
and ping between the 2 pods:

----
$ kubectl scale deployment cirros --replicas=2
$ kubectl get pod -o wide -n ns-isolated
NAME                     READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros-85fc7dd848-6l7j2  1/1    Running  0         8s   10.47.255.239  cent222  <none>
cirros-85fc7dd848-215k8  1/1    Running  0         8s   10.47.255.238  cent333  <none>

$ kubectl -n ns-isolated exec -it cirros-85fc7dd848-6l7j2 -- ping 10.47.255.238
PING 10.47.255.238 (10.47.255.238): 56 data bytes
64 bytes from 10.47.255.238: seq=0 ttl=63 time=1.470 ms
^C
--- 10.47.255.238 ping statistics ---
1 packets transmitted, 1 packets received, 0% packet loss
round-trip min/avg/max = 1.470/1.470/1.470 ms
----

the ping packet passes through now. the isolation is between isolated NS and all
other tenant in the cluster, but not between pods in same NS!

NOTE: pod-level isolation can be archived via kubernetes network policy, or
security groups in contrail. this is covered later in this chapter.

== contrail floating IP

=== floating IP introduction

`floating IP`, or `FIP` for short, is a "traditional" concept that contrail
supports since very early releases. Essentially it is an openstack utility to
"map" a VM IP, which is typically a private IP address, to a public IP (the
"floating IP" in this context) that is reachable from the outside of the
cluster. Internally the one to one mapping is implemented by NAT. whenever a
vrouter receives packets from outside of the cluster destined to the floating
IP, it will translate it to the VM's private IP and forward the packet to the
VM. similarly it will do the translation on reverse direction. Eventually both
VM and Internet host can talk to each other, and both can initiate the
communication.

the figure below illustrates the basic work flow of FIP:

.Floating IP
//image::https://user-images.githubusercontent.com/2038044/60388331-be8cd180-9a7d-11e9-8ff7-c202ed9f7349.png[]
image::https://user-images.githubusercontent.com/2038044/60556767-b8faea00-9d10-11e9-84bb-0e40e3edcc3d.png[]

//image::https://user-images.githubusercontent.com/2038044/60357106-b448d580-99a0-11e9-8ad2-31e15102b6bd.png[]

here are some highlights regarding FIP and FIP pool configuration:

* a FIP is allocated from a `FIP pool`
* a FIP is associated with a VM's `port`, or a `VMI` (Virtual Machine
  Interface).
* a FIP pool is created based on a virtual network(`FIP-VN`)
* the `FIP-VN` may mark itself as "public", by setting `route-target` (`RT`)
  attributes. 
* when a `FIP` is advertised to the outside of the cluster, typically through a
  gateway router, the router will examine the RT. if it sees a match with its
  route import policy, it will load the route into its VRF table. all remote
  clients connected to the VRF will be able to communicate with the FIP.

Regarding the FIP concept and role, there is nothing new in contrail
kubernetes environment. But the usage of floating IP has been extended in
`service` and `ingress` implementation, and it plays an important role for
access toward kubernetes `service` and `ingress` from external. in later
sections you will learn more details about how kubernetes `service` and
`ingress` works in contrail kubernetes setup.

=== creating FIP pool

creating a FIP pool is a 3 steps process:

* create a public VN, 
* set `RT` (route-target) for the VN so it can be advertised and imported into
  the gateway router's VRF.
* create a FIP pool based on the public VN

again this is nothing new but the same steps as with other contrail environment
without kubernetes. however, as you've learned in previous section, with
kubernetes integration a VN can now be created in a "kubernetes style":

.create a public VN named `vn-ns-default`

----
$ cat vn-ns-default.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "101.101.101.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-ns-default
spec:
  config: '{
    "cniVersion": "0.3.0",
    "type": "contrail-k8s-cni"
  }'

$ kubectl apply -f vn-ns-default.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-ns-default unchanged

$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-ns-default   22d
----

.set the `RT`

if you need the FIP to be reachable from Internet through gateway router, you'll
need to set a route-target to make the VN prefix getting imported in the gateway
router's VRF table. this step is necessary whenever Internet access is required.

.contrail command: setting RT
image::https://user-images.githubusercontent.com/2038044/60751261-b43c6d00-9f80-11e9-93c5-b06aeb642eb0.png[]

NOTE: the UI navigation path to set RT is:
CC: main-menu > Overlay > "Virtual Networks" > k8s-vn-ns-default-pod-network >
Edit > "Routing, Bridging and Policies"

NOTE: in the later lab demo of `service` or `ingress`, you always need to set the
RT to the public VN whenever they need to be accessed from Internet host, 

.create a FIP pool based on the public VN

from contrail command UI, Create a floating IP pool based on the public VN:

.contrail command: create a FIP pool
image::https://user-images.githubusercontent.com/2038044/60357727-6d5bdf80-99a2-11e9-90c1-98b037cb0c98.png[]

NOTE: the UI navigation path for this setting is: CC: main-menu > Overlay >
Floating IP > Create

TIP: in contrail UI, you can also set the "external" flag in VN "Advanced"
options so that a FIP pool named "public" will automatically be created.

=== FIP pool scope

there are different ways you can define an floating IP pool, and correspondingly
the scope of the pools will also be different. here are 3 possible scopes an IP
pool may have:

* object specific
* NS default
* global default

.object specific

this is the most specific level of scope. object sepecific FIP pool binds itself
only to the object that you specified, it does not affact any other objects in
the same NS or the cluster. E.g. you can specify a service `web` to get FIP from
FIP pool `pool1`, a service `dns` to get FIP from another FIP pool `pool2`, etc.
This give the most granular control of where the FIP comes from for an object,
the cost is you need to explicitly specify it in your yaml file for every object

.NS default

in a NS, a "lazy" way to give FIP is to define a "NS level" FIP pool, so that
all objects created in that NS will "by default" get FIP assignment from that
pool. with NS default pool defined (e.g. `pool-ns-default`), there is no need to
specify the same pool name in each object's yaml file any more. you can still
give a different pool name, say `my-webservie-pool` in an object `webservice` ,
in that case object `webservice` will get the FIP from `my-webservice-pool`
instead of from the NS level pool `poo-ns-default`, because the former is more
specific.

.global default

a "even lazier" method is to define a "global" level default pool, which means
the scope will be the whole cluster, including all namespaces. 

you can combine all 3 methods to take advantages of the flexibility. here is a
practical example:

* define a global pool `pool-global-default`, so any objects in a NS that has no
  NS-level or object-level pool defined, will still get a FIP from this pool
* for NS `dev`, define a FIP pool `pool-dev`, so all objects in `dev`
  will by default get FIP from `poo-dev`
* for NS `sales`, define a FIP pool `pool-sales`, so all objects in `sales` 
  will by default get FIP from `poo-dev`
* for NS `test-only`, do NOT define any NS level pool, so by default all objects
  in it will get FIP from the `pool-global-default`
* when a service `dev-websevice` in `dev` needs a FIP from `pool-sales`, specify
  `pool-sales` in `dev-webservice` object will archive this goal.

NOTE: Just keep in mind the rule of thumb - the most specific scope will always
prevail.

==== global level default FIP pool

to define the global level default FIP pool, you need to specify the full
qualified pool name (domain > project > network) in configuration file of
`contrail-kube-manager` docker container. 

----
$ docker ps -a | grep  kubemanager
2260c7845964  ...snipped...  ago  Up  2  minutes  kubemanager_kubemanager_1
----

the configuration file is `/etc/contrail/common_kubemanager.env`:

----
$ cat /etc/contrail/common_kubemanager.env
VROUTER_GATEWAY=10.169.25.1
CONTROLLER_NODES=10.85.188.19
KUBERNETES_API_NODES=10.85.188.19
RABBITMQ_NODE_PORT=5673
CLOUD_ORCHESTRATOR=kubernetes
KUBEMANAGER_NODES=10.85.188.19
CONTRAIL_VERSION=master-latest
KUBERNETES_API_SERVER=10.85.188.19
TTY=True
ANALYTICS_SNMP_ENABLE=True
STDIN_OPEN=True
ANALYTICS_ALARM_ENABLE=True
ANALYTICSDB_ENABLE=True
CONTROL_NODES=10.169.25.19
----

as you can see, this `.env` file contains important environmental parameters
about the setup. to specify a `global FIP pool`, add following line in it:

----
KUBERNETES_PUBLIC_FIP_POOL={'domain': 'default-domain','name': 'pool-global-default','network': 'vn-global-default','project': 'k8s-ns-user-1'}
----

it reads: the global default FIP pool is called `pool-global-default`, and it
is defined based on a VN `vn-global-default` under project `k8s-ns-user-1`.
the project name indicates the corresponding kubernetes namespace is `ns-user-1`.

now with that piece of configuration placed, you can "re-compose" the
`contrail-kube-manager` docker container to make the change take effect.
essentially you need to tear it down and then bring it back up:

----
$ cd /etc/contrail/kubemanager/
$ docker-compose down;docker-compose up -d
Stopping kubemanager_kubemanager_1 ... done
Removing kubemanager_kubemanager_1 ... done
Removing kubemanager_node-init_1   ... done
Creating kubemanager_node-init_1 ... done
Creating kubemanager_kubemanager_1 ... done
----

now the global default FIP pool is defined.

==== NS level default FIP pool

the next FIP pool scope is in NS level. each NS can define its own default
pool.  same way as kubernetes annotations object is used to give a subnet to a
VN, it is also used to specify a FIP pool. the yaml file looks:

----
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    opencontrail.org/isolation: "true"
    opencontrail.org/fip-pool: "{'domain': 'default-domain', 'name': 'pool-ns-default', 'network': 'vn-ns-default', 'project': 'k8s-ns-user-1'}"
  name: ns-user-1
----

in this example, NS `ns-user-1` is given a NS level default FIP pool named
`pool-ns-default`, and the corresponding VN is `vn-ns-default`. once the NS
`ns-user-1` is created with this yaml file, any new service which requires an
FIP, if not created with the object-specific pool name in its yaml file, will
get a FIP allocated from this pool. In practice, most NS (especially
those isolated NS) will need its own NS default pool so you will see this
type of configuration very often in field.

==== object level FIP pool

the last one is object-specific pool. here is an example:

----
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb-pool-public-1
  annotations:
    "opencontrail.org/fip-pool": "{'domain': 'default-domain','name': 'pool-public-1','network': 'vn-public-1','project': 'k8s-ns-user-1'}"
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer
----

in this example, service `service-web-lb-pool-public-1` will get an FIP from
pool `pool-public-1`, which is created based on VN `vn-public-1` under current
project `k8s-ns-user-1`. the corresponding kubernetes NS is `ns-user-1`.

=== creating FIP

once FIP pool is created and available, an FIP can be allocated from the FIP
pool for the objects that requires one. this can be done either manually, by
associating an FIP to a VMI (VM or pod interface), or automatically by contrail
system when it sees the need.

.automatically assignment

later in this book, you will see examples when contrail automatically assign a
FIP and associate it to objects:

* LoadBalancer service
* Ingress objects

in fact, ClusterIP by itself is also implemented by FIP. we'll check out more
details later.

.manually association

// TODO: <<SCREENSHOT>>

=== advertising FIP

//TODO


== contrail services

=== kubernetes service introduction

service is the core object in kubernetes. in chapter 3 you've learned what is
kubernetes service and how to create a `service` object with yaml file.
functional-wise, a service is running as a layer 4 (transport layer) load balancer that is
sitting between clients and servers. client can be anything "requesting" a
service. server in our context is the backend pods "providing" the
response. the client only sees the "frontend" - a service IP and service port
exposed by a service, it does not (and no need to) care about which backend
pods (and with what "pod IP") actually responds the service request. inside of
the cluster, that `service IP` is a kind of virtual IP (`VIP`) that is also
called a `cluster IP`.

This design model is very powerful and efficient in one sense that, it covers
the fragility of the possible single point failure that may be caused by
failure of any individual pod providing the service, therefore making a
`service` much more robust from client's perspective.

////
`pod` is the one doing the real work, and in kubernetes it is very "cheap" to
launch pods as needed. in chapter 3 you'll learned how fast it is to scale a rc
and deployment to control numbers of running pods dynamically. However, the
nature of a kubernetes pod is "mortal". to understand that just think of if a
screw of a chair breaks for whatever reason, you won't bother to "repair" it but
instead you just grab a new one.
////

=== contrail service: ECMP loadbalancer

in contrail kubernetes integration environment, all 3 types of services are
supported:

* clusterIP
* nodePort
* loadbalancer

in contrail all services is implemented by contrail's `loadbalancer` (`LB`).
before we dive into the details of a service loadbalancer, it will be good to
review the legacy neutron based loadbalancer concept in contrail. for brevity
we'll sometimes also refer `loadbalancer` as `LB`.

contrail LB is an relatively "old" feature that is supported since version 1.x.
it enables the creation of a pool of VMs serving applications, sharing one
virtual-ip (`VIP`) as the frontend IP towards clients.

==== contrail neutron loadbalancer

.contrail neutron loadbalancer
image::https://user-images.githubusercontent.com/2038044/60641740-1f5c3700-9dfb-11e9-962f-ed67836d8115.png[]

some highlights of this figure:

* the LB is created with VIP `20.1.1.1`. for each layber 4 port that the LB is
  listening, a `LB listener` is created
* all backend VMs, called a `pool`, is allocated an IP from subnet 30.1.1.0/24. 
* each backend VM is called a `member` of the `pool`
* a client only sees one frontend VIP representing the whole service
* when LB sees a request coming, it does TCP connection proxying. what that
  means is it establishes the TCP connection with the client, extracts the
  clients' HTTP/HTTPS requests, creates a new TCP connection towards one of the
  backend VMs from the pool, and send the request in the new TCP connection.
* when LB gets its response from the VM, it close the connection towards the
  backend VM and deliver the response to the original TCP connection towards the
  client.

.what is "proxy"

the implementation uses open sourced haproxy to do the "proxy" job, which is
typically an application layer forwarding. a proxy sitting between machine A and
B, works at the application layer so it is "aware of" the application layer
protocols (HTTP/HTTPS) and so it not transparent in its nature. It creates two
connections each with source and destination. Machine A does not even know about
the existence of machine B at all. For machine A Proxy is the only thing its
talking to and does not care how and where the proxy gets its data.

you see that this loadbalancer model is very similar to kubernetes service
concept:

* VIP is the "service IP" 
* backend VM becomes backend pods
* members are added by kubernetes instead of neutron

in fact, contrail re-uses a good part of this model in kubernetes service
implementation. to support service loadbalancing, contrail extends the
loadbalancer with a new driver, with it service will be implemented as "equal
cost multiple path" (ECMP) loadbalancer working in layer 4(transport layer) . this is the primary
difference comparing with the "proxy" mode that the old neutron loadbalancer
type does.

NOTE: ingress, on the other hand, is conceptually even closer with the old
neutron loadbalancer in the sense that both are layer 7 (application
layer) "proxy" based. more about ingress will be discussed in later section.

==== contrail sevice loadbalancer

let's use another figure to illustrate service loadbalancer and the related
objects.

.service loadbalancer
//image::https://user-images.githubusercontent.com/2038044/60640833-0f425880-9df7-11e9-91e1-9b0830394aaa.png[]
//image::https://user-images.githubusercontent.com/2038044/60677600-f87c2000-9e4f-11e9-8032-7cffd5f35da7.png[]
image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]

highlights in this figure:

* Each service is represented by a `loadbalancer` object. 
* the loadbalancer object comes with a `loadbalancer_provider` property. for
  service implementation a new `loadbalancer_provider` type called `native` is
  implemented.  
* for each sevice port a `listener` object is created for the same service `loadbalancer`
* for each `listener` there will be a `pool` object
* the `pool` contains `members`, depending on number of backend pod one pool may
  has multiple `members`
* each member object in the pool will map to one of pod backend
* `contrail-kube-manager` listens `kube-apiserver` for any changes, based on
  kubernetes `service` and `pod` label mapping it will knows the most current
  backend pods, and update them as members in the pool .
* `loadbalancer` will have a "virtual IP" `VIP`, which is same as the `service
  IP` 
* The `service-ip`/`VIP` will be linked to each backend pod's interface
* the linkage from service-ip to multiple backend pods interface creates an ECMP
  next-hop in contrail, traffic will be loadbalanced from the source pod towards
  one of the backend pod directly. later we'll show the ECMP prefix in the pod's
  VRF table

the most important thing to understand in this diagram, as we've mentioned, is
that in contrast to the legancy neutron loadbalancer, and also the ingress
loadbalancer which we'll discussed later, there is no application layer "proxy"
in this process. contrail service implementation is based on layer 4 (transport
layer) ECMP based
loadbalancing. 

////
detail discussions of the LB and all surrounding objects are out
of the scope of this book.
////

NOTE: technically, the LB has `VIP` only, but it also has a reference toward VMI
object which again has a reference to the `instance-ip`. the `instance-ip` is
the same IP as `service-ip`. to avoid confusions we won't cover these level of
implementation details in this book.

////
# k8s-5.md
Till 4.1, service ip is allocated from cluster-network even for isolated
namespaces. So, service from one isolated namespaces can reach service from
another isolated namespace. Security groups in isolated namespace prevents
reachability from other namespaces which also prevents reachablity from outside
of the cluster. In order to provide reachablity to external entity, the security
group would be changed to allow all which defeats the isolation. 

To address this, two virtual-networks would be created in the isolated
namespaces. One is for pods(pod-network) and another one is for
services(service-network). Contrail network-policy would be created between
pod-network and service-network for the reachablity between pods and services.
Service uses the same service-ipam which will be a flat-subnet like pod-ipam. It
is applicable for default namespace as well. Since virtual-networks are isolated
by default in contrail, services from one isolated namespace can not reach
service from another isolated namespace.
////

////

=== contrail clusterIP service

the `clusterIP` type of service is the most simple one. it is the default mode
if the `ServiceType` is not given. 

clusterIP service is exposed on a `clusterIP` and a service port. when client
pods need to access the service it sends request toward this clusterIP and
service port. service "binds" itself to certain backend pods via label mapping
between the two objects. `endpoint` is created for each service as long as there
is at least one matching pod available to be its backend. this model works great
if all requests are coming from the same cluster. the nature of the clusterIP
limits the scope of this service to be only within the same cluster. overall by
default the clusterIP is not reachable from external. 

////

==== navigating the service loadbalancer objects

we've talked a lot about it the contrail "loadbalancer object" and you may
wonder what it looks like. now we'll dig a little big deeper to look at the
loadbalancers and the supporting objects: listener, pool, members.

in contrail setup you can pull the object data either from contrail UI, CLI
(`curl`) or third party UI tools based on restapi. in production depending on
which one is available and handy you can select your favorite. with curl you
just need a FQDN of the URL pointing to the object, e.g.: to find the
loadbalancer object URL for the service `service-web-clusterip` from all
loadbalancers list:

----
$ curl http://10.85.188.19:8082/loadbalancers | python -mjson.tool | grep -C4 web
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   984  100   984    0     0   163k      0 --:--:-- --:--:-- --:--:--  192k
        {
            "fq_name": [
                "default-domain",
                "k8s-ns-user-1",
                "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
            ],
            "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
            "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        },
----

with the loadbalancer URL, you can pull the specific LB object details:

----
curl http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc | python -mjson.tool

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2439  100  2439    0     0   517k      0 --:--:-- --:--:-- --:--:--  595k
{
    "loadbalancer": {
        "annotations": {
            "key_value_pair": [
                {
                    "key": "namespace",
                    "value": "ns-user-1"
                },
                {
                    "key": "cluster",
                    "value": "k8s"
                },
                {
                    "key": "kind",
                    "value": "Service"
                },
                {
                    "key": "project",
                    "value": "k8s-ns-user-1"
                },
                {
                    "key": "name",
                    "value": "service-web-clusterip"
                },
                {
                    "key": "owner",
                    "value": "k8s"
                }
            ]
        },
        "display_name": "ns-user-1__service-web-clusterip",
        "fq_name": [
            "default-domain",
            "k8s-ns-user-1",
            "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
        ],
        "href": "http://10.85.188.19:8082/loadbalancer/99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "id_perms": {
            ...<snipped>...
        },
        "loadbalancer_listener_back_refs": [    #<---
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/loadbalancer-listener/3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc-TCP-8888-3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
                ],
                "uuid": "3702fa49-f1ca-4bbb-87d4-22e1a0dc7e67"
            }
        ],
        "loadbalancer_properties": {
            "admin_state": true,
            "operating_status": "ONLINE",
            "provisioning_status": "ACTIVE",
            "status": null,
            "vip_address": "10.105.139.153",    #<---
            "vip_subnet_id": null
        },
        "loadbalancer_provider": "native",      #<---
        "name": "service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "parent_href": "http://10.85.188.19:8082/project/86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "parent_type": "project",
        "parent_uuid": "86bf8810-ad4d-45d1-aa6b-15c74d5f7809",
        "perms2": {
            ...<snipped>...
        },
        "service_appliance_set_refs": [
            ...<snipped>...
        ],
        "uuid": "99fe8ce7-9e75-11e9-b485-0050569e6cfc",
        "virtual_machine_interface_refs": [
            {
                "attr": null,
                "href": "http://10.85.188.19:8082/virtual-machine-interface/8d64176c-9fc7-491a-a44d-430e187d6b52",
                "to": [
                    "default-domain",
                    "k8s-ns-user-1",
                    "k8s__Service__service-web-clusterip__99fe8ce7-9e75-11e9-b485-0050569e6cfc"
                ],
                "uuid": "8d64176c-9fc7-491a-a44d-430e187d6b52"
            }
        ]
    }
}
----

the output is very extensive and includes a whole buntch of details that is not
of our interests at this moment. but it does tell something interesting:

* LB use service IP as its VIP
* LB is connected to a listener by a reference
* `loadbalancer_provider` is `native`, this is a new extension to implement
  layer 4 (transport layer)  ECMP for kubernetes service, different than the neutron LB
  implemented by haproxy

in the rest part of the exploration to LB and its related objects, we'll use the
legacy contrail UI, but the same is also supported in the new contrail command
UI. 

for each service there is a LB object, in the below capture it shows 2 LB
objects:

* ns-user-1-service-web-clusterip
* ns-user-1-service-web-clusterip-mp

this indicates 2 services were created. the service loadbalancer object's name
is composed by connecting `<NS name>-<service name>` hence we can tell the
2 service's name.

.loadbalancer objects
image::https://user-images.githubusercontent.com/2038044/60685179-a0edac80-9e6f-11e9-98c1-e2db001df543.png[]

click on the small triangle icon in left of the first loadbalancer object
`ns-user-1-service-web-clusterip` to expand it, then click on `advanced json
view` icon on the right, you will see the similar detail information as what
you've seen in `curl` capture. for example the `VIP`, `loadbalancer_provider`,
`loadbalancer_listener` object that refers it, etc. 

from here you can keep expanding the `loadbalancer_listener` object by clicking
the `-` character to see the detail information of it. you then see a
`loadbalancer_pool`, expand it again you will see `member`. by the reference all
of these objects are connected to each other and work together.

.loadbalancer
image::https://user-images.githubusercontent.com/2038044/60685370-bca58280-9e70-11e9-8030-2746766082c8.png[]

click on the LB name and select "listener", then expand it and click on
`advanced json view` icon on the right, you will get the listener details. the
listener is listening on service port 8888, and it is referenced by a `pool`.

.listener:
image::https://user-images.githubusercontent.com/2038044/60685556-b368e580-9e71-11e9-820f-47fb25aacee4.png[]

just repeat the exploring process we will get down to the pool and two
`members` in it. the member is with a port of `80`, which maps to the container
targetPort in pod.

.pool:
image::https://user-images.githubusercontent.com/2038044/60685626-15c1e600-9e72-11e9-8539-a24ea28b0bf3.png[]

.members:
image::https://user-images.githubusercontent.com/2038044/60685682-6fc2ab80-9e72-11e9-804d-5eccd8e055df.png[]

next we'll examine the vrouter VRF table for the pod to show contrail service
loadbalancer ECMP operation details. in order to better understand the "1 to N"
mapping between loadbalancer and listener shown in the loadbalancer object
figure, we'll also give an example of configuring a "multiple port service" in
our setup.  we'll conclude the ClusterIP service section by inspecting the
vrouter flow table to illustrate the service packet workflow.

=== service testbed

before starting our investigation, let's look at our testbed. in this book we
build a setup including the following devices, most of our case studies are
based on it:

* one cenos server running as k8s `master` and contrail controllers
* two cenos servers, each running as a k8s `node` and contrail vrouter
* one Juniper QFX switch running as the underlay "leaf"
* one Juniper MX router running as a gateway router, or a "spine"
* one centos server runs as an Internet host machine

the digaram is here:

image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]

NOTE: To minimize the resource utilization, all "servers" are actually centos
virtual machines created by vmware ESXI hypervisor running in one physical HP
server. this is also the same testbed for ingress.

in appendix you will find all of the details about the setup. the prerequisites,
software/hardware specifications, sample configuration files, and installation
steps. following the steps you will be able to build a same setup in your lab.

=== contrail ClusterIP service

in chapter 3 we've demonstrated how to create and verify a clusterIP service. in
this section we'll revisit the lab and look at some important details about
contrail specific implementations. we'll continue and add a few more tests to
illustrate the contrail service loadbalancer implementation details.

==== ClusterIP as FIP

let's review what we got from service lab in chapter3:

----
$ kubectl get svc -o wide
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service-web-clusterip   ClusterIP   10.105.139.153   <none>        8888/TCP   45m   app=webserver

$ kubectl get pod -o wide --show-labels
NAME                 READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   LABELS
cirros               1/1     Running   1          5d1h   10.47.255.237   cent222   <none>           app=cirros
rc-webserver-g27kg   1/1     Running   0          45m    10.47.255.238   cent333   <none>           app=webserver
----

here we see one service is created, with one pod running as its backend. the
label in the pod matches to the SELECTOR in service. the pod name also indicates
this is a RC-generated pod. later we can scale the RC for ECMP case study, for
now we'll stick to one pod and examine the ClusterIP implementation details.

in contrail, a `ClusterIP` is essentially implemented in the form of a FIP. once
a service is created, a FIP will be created from the service subnet and
associated to the backend pod VMI and IP. this FIP/clusterIP is also acting as a
"VIP" to the client pods inside of the cluster. this holds true for all type of
contrail kubernetes service, for LoadBalancer type of service contrail will
allocate a second FIP "external-ip" as the VIP, but in that case the external
VIP is advertised outside of the cluster through gateway router. you will get
more details about that later.

from UI we'll see the automatically allocated FIP as ClusterIP.

.ClusterIP as FIP
image::https://user-images.githubusercontent.com/2038044/60973473-57c9ac80-a2f6-11e9-81a7-df74349e9877.png[]

the FIP is also associated with the pod VMI and podIP, in the case the VMI is
representing the pod interface.

.pod interface
image::https://user-images.githubusercontent.com/2038044/60975990-df191f00-a2fa-11e9-9f81-e635c141c7e6.png[]

expand the `fip_list`, we'll see the `port_map` in it:

----
fip_list:  {
    list:  {
        FloatingIpSandeshList:  {
            ip_addr: 10.105.139.153
            vrf_name: default-domain:k8s-ns-user-1:k8s-ns-user-1-service-network:k8s-ns-user-1-service-network
            installed: Y
            fixed_ip: 10.47.255.238
            direction: ingress
            port_map_enabled: true
            port_map:  {
                list:  {
                    SandeshPortMapping:  {
                    protocol: 6
                    port: 80
                    nat_port: 8888
                    }
                }
            }
        }
    }
}
----

the `port_map` tells that port `8888` is a `nat_port`, `6` is the protocol
number so it means protocol TCP. TCP port 8888 will be translated to container
port `80` and vice versa.

now you understand with FIP representing ClusterIP, NAT will happen in service.
later we'll examine NAT again in the flow table.

==== scale backend pods
in chapter 3 clusterIP service example, we have created a sevice and a backend
pod. to verify the ECMP, let's increase the replica to 2 to generate a second
backend pod. this is a more realistic and rebost model: each pod will now be
backing up each other to avoid a single point failure.

instead of using yaml file to manually create a new webserver pod, with the
"kubernetes spirit" in mind you should think of to `scale` a RC or deployment,
as what you`ve seen earlier in this book. in our service example we`ve been
using `RC` object to spawn our webserver pod on purpose:

----
$ kubectl scale rc rc-webserver --replicas=2
replicationcontroller/rc-webserver scaled

$ kubectl get pod -o wide --show-labels
NAME                 READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   LABELS
cirros               1/1     Running   1          5d1h   10.47.255.237   cent222   <none>           app=cirros
rc-webserver-7btnj   1/1     Running   0          27m    10.47.255.236   cent222   <none>           app=webserver
rc-webserver-g27kg   1/1     Running   0          45m    10.47.255.238   cent333   <none>           app=webserver

$ kubectl get svc -o wide
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
service-web-clusterip   ClusterIP   10.105.139.153   <none>        8888/TCP   45m   app=webserver
----

immediately after you create a new webserver pod by scaling the RC with
`replicas 2`, a new pod is launched.  we end up having 2 backend pods now, one
is running in same node `cent222` as the client cirros pod, or a "local" node
for cirros pod; the other one is running in the other node `cent333` - the
"remote" node from client pod's perspective.  and the `endpoint` objects get
updated to reflect the current set of backend pods behind the `service`.

----
$ kubectl get ep -o wide
NAME             ENDPOINTS                           AGE
service-web-lb   10.47.255.236:80,10.47.255.238:80   20m
----

NOTE: without `-o wide` option, only first endpoint will be displayed.

we go ahead and check the FIP again.

.ClusterIP as FIP (ECMP)
image::https://user-images.githubusercontent.com/2038044/60973157-b2163d80-a2f5-11e9-957a-438642355391.png[]

we see the same FIP, but now it is associated with two podIP and VMIes. 

==== ECMP routing table: control node perspective

first, to examine the ECMP, let's take a look at the routing table in the
controller's routing instance.

.control node routing instance table
image::https://user-images.githubusercontent.com/2038044/60966312-ee41a200-a2e5-11e9-8966-053f0bbc20ea.png[]

the routing instance (RI) has a full name with the following format:

    <DOMAIN>:<PROJECT>:<VN>:<RI>

in most cases RI inheritate the same name from it's VN, so in our case the
full IPv4 routing table has this name:
`default-domain:k8s-ns-user-1:k8s-ns-user-1-pod-network:k8s-ns-user-1-pod-network.inet.0`
the `.inet.0` indicate the routing table type is unicast IPv4. there are many
other tables which is not of our interests right now.

two routing entries with the same exact prefixes of the ClusterIP show up in the
routing table, with two different next hops, each pointing to a different node.
what happens is each node advertises the presence of the backend pods in itself
to the control node via XMPP, which then reflect the routes to the other compute
node.

==== ECMP routing table: compute node perspective

next, on the client pod node `cent222`, we'll look at the the pod's VRF table to
understand how the packets are forwarded towards the backend pods

.vrouter vrf table
image::https://user-images.githubusercontent.com/2038044/60680116-18174680-9e58-11e9-9235-48c152959df7.png[]

the most important part of the screenshot is the routing entry `Prefix:
10.105.139.153 / 32 (1 Route)`, it is our ClusterIP address. underneath the
prefix there is a statement `ECMP Composite sub nh count: 2`. this indicates the
prefix has multiple possible next hop to reach. now expand it by clicking the
small triangle icon in the left, you will be given a lot more details about this
prefix.

.vrouter ECMP nexthop
image::https://user-images.githubusercontent.com/2038044/60680345-ece12700-9e58-11e9-9793-2b609918e146.png[]

we won't cover all additional outputs of it. the most important thing that is of
our focus is `nh_index: 87`, which is the next hop ID (`NHID`) for the service
IP prefix. from vrouter agent docker, we can further resolve the NHID to
disclose more details:

----
[2019-07-04 12:42:06]root@cent222:~
$ docker exec -it vrouter_vrouter-agent_1 nh --get 87
Id:87         Type:Composite      Fmly: AF_INET  Rid:0  Ref_cnt:2          Vrf:2
              Flags:Valid, Policy, Ecmp, Etree Root,
              Valid Hash Key Parameters: Proto,SrcIP,SrcPort,DstIp,DstPort
              Sub NH(label): 51(25) 37(59)              #<---

Id:51         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:18         Vrf:0
              Flags:Valid, MPLSoUDP, Etree Root,        #<---
              Oif:0 Len:14 Data:00 50 56 9e e6 66 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:10.169.25.21

Id:37         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:5          Vrf:2
              Flags:Valid, Etree Root,
              EncapFmly:0806 Oif:8 Len:14               #<---
              Encap Data: 02 30 51 c0 fc 9e 00 00 5e 00 01 00 08 00

$ vif --get 8
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

vif0/8      OS: tapeth0-304431
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.236  #<---
            Vrf:2 Mcast Vrf:2 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:455  bytes:19110 errors:0
            TX packets:710  bytes:29820 errors:0
            Drops:455
----

TIP: don't forget to execute the vrouter command from within the vrouter docker.
doing it from the host directly won't work.

some important information to highlight from this capture:

* NHID 87 is an "ECMP composite nexthop"
* the ECMP nexthop contains 2 "sub" nexthops: nexthop 51 and nexthop 37, each representing a
  seperate path towards the backend pods
* nexthop 51 represents a MPLSoUDP tunnel, from current node `cent222`, with
  source IP being local fabric IP `10.169.25.20`, to the other node `cent333`
  whose fabric IP is `10.169.25.21`. if you recall where our two backend pods
  are located, this is the path to the "remote" node.
* nexthop 37 represents a "local" path, towards vif 0/8 (`Oif:8`), which is the
  local backend pod's interface. the `vif --get 8` proves this.

the loadbalancer ECMP workflow is illustrated in this figure:

.contrail service loadbalancer ECMP
//image::https://user-images.githubusercontent.com/2038044/60762382-97f60a00-a02c-11e9-81ad-b1f05d815571.png[]
image::https://user-images.githubusercontent.com/2038044/60762413-1ce12380-a02d-11e9-8cec-41d5e177bfb9.png[]

==== multiple port service

we've understood how the service layber 4 ECMP works and explored the LB
objects in lab. remember in the figure showing the LB and all relevant objects,
we showed that one LB may having 2 or more LB listeners. each listener has an
individual backend pool which has one or multiple member(s). 

.service loadbalancer
image::https://user-images.githubusercontent.com/2038044/60762277-e1912580-a029-11e9-92f1-93d8410f4eeb.png[]

to understand the 1:N mapping between loadbalancer and listeners, we can use the
`multiple port service` as an example. let's look at the yaml file of it:

----
$ cat svc/service-web-clusterip-mp.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-clusterip-mp
spec:
  ports:
  - name: port1
    port: 8888
    targetPort: 80
  - name: port2
    port: 9999
    targetPort: 90
  selector:
    app: webserver
----

what we've added is another item in the `ports` list: a new service port `9999`
that maps to container's `targetPort` `90`. now with two port mappings we have
to give each mapping a name, `port1` and `port2` respectively.

NOTE: without a port `name` the multiple ports yaml file won't work.

now we apply the yaml file and a new service `service-web-clusterip-mp` with 2
ports is created:

----
$ kubectl apply -f svc/service-web-clusterip-mp.yaml
service/service-web-clusterip-mp created

$ kubectl get svc
NAME                      TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)            AGE
service-web-clusterip     ClusterIP  10.105.139.153  <none>       8888/TCP           3h8m
service-web-clusterip-mp  ClusterIP  10.101.102.27   <none>       8888/TCP,9999/TCP  4s

$ kubectl get ep
NAME                       ENDPOINTS                           AGE
service-web-clusterip      10.47.255.238:80                    4h18m
service-web-clusterip-mp   10.47.255.238:80,10.47.255.238:90   69m
----

NOTE: to simply the case study we've scaled down the backend RC to one pod

it looks everything is ok, isn't it? the new service comes up with 2 service
ports exposed, `8888` is the old one we've tested in previous examples, and the
new `9999` port should work equally well.

turns out that is not the case.

service port 8888 works:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:8888 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.238
                         Hostname = rc-webserver-g27kg
                                    [giphy]
----

service port 9999 doesn't work:

----
$ kubectl exec -it cirros -- curl 10.101.102.27:9999 | w3m -T text/html | cat
command terminated with exit code 7
curl: (7) Failed to connect to 10.101.102.27 port 9999: Connection refused
----

the request towards port 9999 is rejected. reason is the `targetPort` is not
running in pod container, so there is no way you will get a response from it.

----
$ kubectl exec -it rc-webserver-g27kg -- netstat -lnap
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/python
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node   PID/Program name    Path
----

`readinessProbe` introduced in chater 3 is the official kubernetes tool to
detect this situation, so in case the pod is not "ready", it will be restarted
and you will catch the events.

to resolve this let's start a server in pod to listen on port `90`. 
one of the easiest way today to start a HTTP server is to use the
`SimpleHTTPServer` module coming with `python`. in our test we only need to
set the port to `90`

----
$ kubectl exec -it rc-webserver-g27kg -- python -m SimpleHTTPServer 90 
Serving HTTP on 0.0.0.0 port 90 ...                                    
----

NOTE: by default, python SimpleHTTPServer listens on port 8080.

now the `targetPort` is on, we can start the request towards service port `9999`
again from the cirros pod. this time it succeed and get the returned webpage
from python SimpleHTTPServer.

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999 | w3m -T text/html | cat
Directory listing for /

 ━━━━━━━━━━━━━━━━━━━━━

  • app.py
  • Dockerfile
  • file.txt
  • requirements.txt
  • static/

 ━━━━━━━━━━━━━━━━━━━━━
----

for each incoming request the `SimpleHTTPServer` logs one line output, with an
IP address showing where the request came from. in our case cirros client pod is
with the IP `10.47.255.237`.

----
10.47.255.237 - - [04/Jul/2019 23:49:44] "GET / HTTP/1.1" 200 -
----

==== the flow table

so far we've tested clusterIP service, and we see client request is sent towards
the service IP. in contrail environment `vrouter` is the module that does all of
the packet forwarding job. when the `vrouter` in client pod get the packet, it
looks up the corresponding `vrouter` VRF table for the pod, get the nexthop and
resolves to the correct egress interface and proper encapsulation. in our test
so far, the client and backend pods are in 2 different nodes, so the source
`vrouter` decides the packets need to be send in MPLSoUDP tunnel, towards the
node where backend pod is running. what interests us the most is:

* how the service IP and podIP is translated to each other? 
* is there a way to "capture and see" the two IPs in a flow, "before" and
  "after" the translations for comparison purpose?

the most "straightforward" method you would think of is to capture the packets,
then decode and see. doing that however, is not as easy as what you've expected.
first you need to capture the packet at different places:

* at the pod interface, this is after the address is translated, that part is
  easy
* the fabric interface, this is before packet is translated and reaches the pod
  interface. here the packets are with MPLSoUDP encapsulation since data plane
  packets are "tunneled" between nodes.

then you need to copy the pcap file out and load with wireshark to decode. you
probably also need to set up wireshark to recognize the MPLSoUDP encapsulation.

the easier way is to check the vrouter flow table which records IP and port
details about a traffic flow. in this test we will prepare a big file `file.txt`
in server pod and try to download it from the client pod. 

[TIP]
====
you may wonder why we don't simply use same curl test to pull the webpage, as
what we've done in early test. in theory that is fine.  the only problem is that
the TCP flow follows the TCP session. in our previous test with `curl`, the TCP
session starts and stops immediately after the webpage is retrieved, then the
vrouter clears the flow too. you won't be fast enough to capture the flow table
at the right moment. instead, downloading a big file will hold the TCP session -
as long as the file transfer is ongoing the session will remain, and we can take
time to investigate the flow. later on in `ingress` example we will demonstrate
a different method with a one-liner shell script.  

====

now in the cirros pod curl URL, instead of just give root path `/` to list the
files in folder, we try to pull the file: `file.txt`

----
$ kubectl exec -it cirros -- curl 10.103.87.232:9999/file.txt
----

in server pod we see the log indicating the file transfer starts:

----
10.47.255.237 - - [05/Jul/2019 00:41:21] "GET /file.txt HTTP/1.1" 200 -
----

now we have enough time to collect the flow table from both client and server
node, in the vrouter docker.

.client node flow table

----
(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1361 Added 1361 Deleted 442 Changed 443Processed 1361 Used Overflow entries 0
(Created Flows/CPU: 305 342 371 343)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    40100<=>340544       10.47.255.237:42332                                 6 (3)
                         10.103.87.232:9999
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):59,  Stats:7878/520046,
 SPort 65053, TTL 0, Sinfo 6.0.0.0)

   340544<=>40100        10.103.87.232:9999                                  6 (3)
                         10.47.255.237:42332
(Gen: 1, K(nh):59, Action:F, Flags:, TCP:SSrEEr, QOS:-1, S(nh):68,  Stats:142894/205180194,
 SPort 63010, TTL 0, Sinfo 10.169.25.21)
----

highlights in this output:

* cirros client starts TCP connection from its pod IP `10.47.255.237` and a
  rondom port, towards the service IP `10.103.87.232` and server port `9999`
* the flow TCP flag `SSrEEr` indicates the session is established bidirectionally.
* Action `F` means "forwarding". note that there is no special processing like
  `NAT` happening here. 

we can conclude, from client's perspective, it only see the service IP. it is
not aware of any backend pod IP at all.

.server node flow table

now look at flow table in server node vrouter docker:

----
(vrouter-agent)[root@cent333 /]$ flow --match 10.47.255.237
Flow table(size 80609280, entries 629760)

Entries: Created 1116 Added 1116 Deleted 422 Changed 422Processed 1116 Used Overflow entries 0
(Created Flows/CPU: 377 319 76 344)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.237]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   238980<=>424192       10.47.255.238:90                                    6 (2->3)
                         10.47.255.237:42332
(Gen: 1, K(nh):24, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):24,
 Stats:8448/202185290,  SPort 62581, TTL 0, Sinfo 3.0.0.0)

   424192<=>238980       10.47.255.237:42332                                 6 (2->2)
                         10.103.87.232:9999
(Gen: 1, K(nh):24, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:8067/419582,  SPort 51018, TTL 0, Sinfo 10.169.25.20)
----

the second flow entry looks same as the one we just saw in client side capture.
traffic lands vrouter fabric interface from remote cirros client node, across
MPLSoUDP tunnel. destination IP and port are service IP and service port
respectively. it seems nothing special here.

however, the flow `Action` now is set to `N(DPd)`, not `F`. according to the
header lines in the `flow` command output, this means NAT, or specifically,
`DNAT` (Destination address translation) with `DPAT` (Destination port
translation). so both the service IP and service port will be translated, to
backend pod IP and port.

now look at the first flow entry. source IP `10.47.255.238` is the backend pod
IP and source port is python server port `90` opened in backend container .
obviously this is the returning traffic indicating the file downloading is still
ongoing. the `Action` is also NAT(`N`), but this time it is the reverse
operation - source NAT (`SNAT`) and source PAT(`SPAT`) will happen. vrouter will
translate backend's source IP source port to the service IP and port, before
putting it into the MPLSoUDP tunnel and return back to client node.

the complete end to end traffic flow is illustrated here:

//image::https://user-images.githubusercontent.com/2038044/60388198-f7c44200-9a7b-11e9-9b08-f34167b0a2b8.png[]
//image::https://user-images.githubusercontent.com/2038044/60762300-96c3dd80-a02a-11e9-8933-452d3ee074a4.png[]
image::https://user-images.githubusercontent.com/2038044/60763424-32147d00-a042-11e9-813a-a6aa3989c09d.png[]

=== contrail LoadBalancer service

in chapter 3 we've briefly talked about LoadBalancer service. in there we
mentioned if the goal is to expose the service to the external world outside of
the cluster, we just specify `ServiceType` as `LoadBalancer` in the service yaml
file. 

whenever a service of `type: LoadBalancer` get created, in contrail environment
what will happen is , not only a `clusterIP` will be allocated and exposed to
other pods within the cluster, but also a `floating ip` will be assigned to the
loadbalancer instance and exposed to the public world outside of the cluster. 

while the `clusterIP` is still acting as a `VIP` to the client **inside** of the
cluster, the `floating ip` will essentially act as a `VIP` facing those client
sitting **outside** of the cluster, for example, a remote Internet host which
sends request to the service across the gateway router. 

in this section we'll demonstrate how does the `LoadBalancer` type of service
works in our end to end lab setup, including the kubernetes cluster, fabric
switch, gateway router, and Internet host.

==== create loadbalancer service

let's look at the yaml file of a `LoadBalancer` service. it is same as ClusterIP
service except just one more line declaring the service `type`:

----
$ cat service-web-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: service-web-lb
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver
  type: LoadBalancer    #<---
----

create and verify the service:

----
$ kubectl apply -f service-web-lb.yaml
service/service-web-lb created

$ kubectl get svc -o wide
NAME            TYPE          CLUSTER-IP   EXTERNAL-IP      PORT(S)         AGE    SELECTOR
service-web-lb  LoadBalancer  10.96.89.48  101.101.101.252  8888:32653/TCP  10s    app=webserver
----

comparing with the `clusterIP` service type, this time in the "EXTERNAL-IP"
column there is an IP allocated. if you remember what we've covered in the
"floating IP pool" section, you should understand this "EXTERNAL-IP" is actually
another `FIP`, allocated from the `NS FIP pool` - we did not give any specific
FIP pool information in the service object yaml file, so based on the algorithm
`NS default FIP pool` will be used automatically. 

==== examine VRF table in gateway router

the `route-target` community setting in the FIP VN makes it reachable by the
Internet host, so effectively our service is now also exposed to the Internet
,instead of only to pods inside of the cluster. Examining the gateway router's
VRF table reveals this:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24
Jun 19 03:56:11

k8s-test.inet.0: 23 destinations, 40 routes (23 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:01:11, MED 100, localpref 200, from 10.169.25.19
                      AS path: ?, validation-state: unverified
                    > via gr-2/2/0.32771, Push 40
----

the FIP host route is learned by gateway router, from contrail controller - more
specifically, contrail control node, which acts as a standard MP-BGP VPN `RR`
reflects routes between compute nodes and the gateway router. A further look
at the detail version of the same route displays more information about this
process:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101/24 detail
Jun 20 11:45:42

k8s-test.inet.0: 23 destinations, 41 routes (23 active, 0 holddown, 0 hidden)
101.101.101.252/32 (2 entries, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:9
                ......
                Source: 10.169.25.19                    #<---
                Next hop type: Router, Next hop index: 1266
                Next hop: via gr-2/2/0.32771, selected  #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 44
                Label TTL action: prop-ttl
                Load balance label: Label 44: None;
                Indirect next hop: 0x900c660 1048574 INH Session ID: 0x690
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 10:15:38   Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1020006
                    unknown type 8084 value eac4:1030001
                Import Accepted
                VPN Label: 44
                Localpref: 200
                Router ID: 10.169.25.19
                Primary Routing Table bgp.l3vpn.0
----

* the `source` indicates from which BGP peer the route is learned,
  `10.169.25.19` is the contrail controller (and kubernetes master) in our lab
* `protocol next hop` tells who generates the route. `10.169.25.20` is node
`cent222` where the backend webserver pod is running
* `gr-2/2/0.32771` represents the (MPLS over) GRE tunnel between node `cent222`
and the gateway router.

to summarize, the FIP given to the service as its external ip is advertised to
gateway router, and get loaded in the router's VRF table. when Internet host
sends a request to the FIP, through MPLSoGRE tunnel the gateway router will
forward it to the compute node where backend pod is locating.

the packet flow is illustrated in this figure:

.service workflow
image::https://user-images.githubusercontent.com/2038044/60563159-a7254100-9d28-11e9-94ca-934b8f870b1e.png[]

////
* you create a `FIP pool` from a public VN, with route-target the VN is
  advertised to the remote gateway router via MP-BGP 
* you create a pod with a label `app: webserver`, kubernetes decides the pod
  will be created in node `cent222`. via XMPP the node publish the pod IP
* you create a loadbalancer type of service with `service port` and label
  selector `app=webserver`.  kubernetes allocates a service IP.
* kubernetes finds the pod with the matching label and update the `endpoint`
  with the pod IP and port information. 
* contrail create a loadbalancer instance and assign a FIP to it. contrail also
  associate that FIP with the pod interface, so there will be one to one NAT
  operation between the FIP and podIP.
* via XMPP, node `cent222` advertises the podIP and FIP to contrail controller
  `cent111`, which then advertises only the FIP to the gateway router. at this
  moment the gateway learns the nexthop of the FIP is `cent222`, so it generate
  a soft GRE tunnel toward `cent222`.
* when gateway router see a request coming from Internet toward the FIP, through
  the MPLS over GRE tunnel it will send the request to the node `cent222`
* vrouter in the node sees the packets destined to the FIP, it will perform NAT
  so the packets will be sent to the right backend pod.
////

==== verify the loadbalancer service

To verify the end to end service access from Internet host to the backend pod, 
we will login to the Internet host desktop and launch a browser, with URL
pointing to `http://101.101.101.252:8888`. 

TIP: just to keep in mind that the request has to be sent to the public **FIP**,
not the **service IP**(**clusterIP**) or backend **podIP** which is only
reachable from inside of the cluster!

this is the returned web page:

image::https://user-images.githubusercontent.com/2038044/60388669-ea5e8600-9a82-11e9-87b9-30a98572f7bb.png[]

****
in our testbed we installed a centos server as an Internet host. as with any
linux distribution, if you need to login the "GUI", you need to install Xwindow
or linux desktop applications and set it up properly. also you need a web
browser if it does not come with the server.
****

To simplify the test, you can also ssh into the Internet host and test it with
`curl` tool:

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | w3m -T text/html | cat
             Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = rc-webserver-vl6zs
   [giphy.gif]
----

the kubernetes service is available from Internet!

==== loadbalancer service ECMP

so far you've seen how loadbalancer type of service is exposed to the Internet
and how the FIP did the "trick". in ClusterIP service section, you've also seen
how the service loadbalancer ECMP works. what you haven't seen yet is how does
the "ECMP" processing works under loadbalancer type of service. To demonstrate
this again we scale the RC to generate one more backend pod behind the
`service`. 

----
$ kubectl scale rc rc-webserver --replicas=2
replicationcontroller/rc-webserver scaled

$ kubectl get pod -l app=webserver -o wide
NAME                READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
rc-webserver-r9zdt  1/1    Running  0         25m  10.47.255.238  cent333  <none>
rc-webserver-xkjpw  1/1    Running  0         23s  10.47.255.236  cent222  <none>
----

here is the question: with 2 pods on different node as backend now, from the
gatway router's perspective when it get the service request, which node it will
choose to forward the traffic to? let`s check the gateway router`s VRF table
again:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32
Jun 30 00:27:03

k8s-test.inet.0: 24 destinations, 46 routes (24 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 *[BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/3/0.32771, Push 26
                    [BGP/170] 00:00:25, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 26
----

the same FIP prefix is imported as we've seen in previous example, except that
now the same route is learned twice and an additional MPLSoGRE tunnel is
created. previously in ClusterIP service example we use `detail` option in `show
route` command to find the tunnel endpoints, this time we examine the soft GRE
`gr-` interface to find the same:

----
labroot@camaro> show interfaces gr-2/2/0.32771
Jun 30 00:56:01
  Logical interface gr-2/2/0.32771 (Index 392) (SNMP ifIndex 1801)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.21:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 0
    Output packets: 0
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None

labroot@camaro> show interfaces gr-2/3/0.32771
  Logical interface gr-2/3/0.32771 (Index 393) (SNMP ifIndex 1703)
    Flags: Up Point-To-Point SNMP-Traps 0x4000 
    IP-Header 10.169.25.20:192.168.0.204:47:df:64:0000000800000000      #<---
    Encapsulation: GRE-NULL
    Copy-tos-to-outer-ip-header: Off, Copy-tos-to-outer-ip-header-transit: Off
    Gre keepalives configured: Off, Gre keepalives adjacency state: down
    Input packets : 11
    Output packets: 11
    Protocol inet, MTU: 9142
    Max nh cache: 0, New hold nh limit: 0, Curr nh cnt: 0, Curr new hold cnt: 0, NH drop cnt: 0
      Flags: None
    Protocol mpls, MTU: 9130, Maximum labels: 3
      Flags: None
----

the `IP-Header` of `gr-` interface indicates the two end points of a GRE tunnel:

* `10.169.25.20:192.168.0.204`: tunnel between node `cent222` and gateway router
* `10.169.25.21:192.168.0.204`: tunnel between node `cent333` and gateway router

We end up to have 2 tunnels in the gateway router, each pointing to a different
node where a backend pod is running. now we believe the router will perform
ECMP load balancing between the two GRE tunnel, whenever it got service request
toward the same FIP. let's check it out.

==== verify the loadbalancer service ECMP

to verify the ECMP we'll just pull the webpage a few more time and we expect to
see both podIP displayed eventually.

turns out this never happens!

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = rc-webserver-xkjpw
----

the only webpage we got is from the first backend pod `10.47.255.236`,
`rc-webserver-xkjpw`, running in node `cent222`. the other one never show up.
so the expected ECMP does not happen yet. when we examine the route again with
`detail` or `extensive` keyword we find the root cause:

----
labroot@camaro> show route table k8s-test.inet.0 101.101.101.252/32 detail | match state
Jun 30 00:48:29
                State: <Secondary Active Int Ext ProtectionCand>
                Validation State: unverified
                State: <Secondary NotBest Int Ext ProtectionCand>
                Validation State: unverified
----

from that we realize that, even if the router learned the same prefix from both
node, only one is `Active` and the other one won't take effect because it is
`NotBest`. therefore, the second route and the corresponding GRE interface
`gr-2/2/0.32771` will never get loaded into the forwarding table:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 00:53:12
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination         Type  RtRef  Next  hop      Type  Index  NhRef  Netif
101.101.101.252/32  user  0      indr  1048597  2
                                Push 26     1272     2 gr-2/3/0.32771
----

this is the default Junos BGP path selection behavior and detail discussion of
it is out of the scope of this book. 

NOTE: for Junos BGP path selection algorithm, check this link:
https://www.juniper.net/documentation/en_US/junos/topics/topic-map/bgp-path-selection.html

the solution is to enable the `multipath vpn-unequal-cost` knob under the VRF:

----
labroot@camaro# set routing-instances k8s-test routing-options multipath vpn-unequal-cost
----

now check the VRF table again:

----
labroot@camaro# run show route table k8s-test.inet.0 101.101.101.252/32
Jun 26 20:09:21

k8s-test.inet.0: 27 destinations, 54 routes (27 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.252/32 @[BGP/170] 00:00:04, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/1/0.32771, Push 72
                    [BGP/170] 00:00:52, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 52
                   #[Multipath/255] 00:00:04, metric 100, metric2 0
                       via gr-2/1/0.32771, Push 72
                     > via gr-2/2/0.32771, Push 52
----

a `Multipath` with both GRE interface will be added under the FIP prefix, the
forwarding table reflects the same:

----
labroot@camaro> show route forwarding-table table k8s-test destination 101.101.101.252
Jun 30 01:12:36
Routing table: k8s-test.inet
Internet:
Enabled protocols: Bridging, All VLANs,
Destination        Type RtRef Next hop    Type Index    NhRef Netif
101.101.101.252/32 user     0             ulst  1048601     2
                                          indr  1048597     2
                                         Push 26     1272     2 gr-2/3/0.32771
                                          indr  1048600     2
                                         Push 26     1277     2 gr-2/2/0.32771
----

now try to pull the webpage from Internet host multiple times with `curl` or web
browser, we see the random result - both backend pod get the request and
responses back.

----
[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.236
  Hostname = rc-webserver-xkjpw

[root@cent-client ~]# curl http://101.101.101.252:8888 | lynx -stdin --dump
                                     Hello
This page is served by a Contrail pod
  IP address = 10.47.255.238
  Hostname = rc-webserver-r9zdt
----

the end to end packet flow is illustrated here:

.loadbalancer service ECMP
image::https://user-images.githubusercontent.com/2038044/60763675-8e2dd000-a047-11e9-91a6-5fb1319517dc.png[]

== contrail ingress

=== kubernetes Ingress introduction

.Ingress
In previous section, you've learned how to expose service to the outside of the
cluster. Besides `LoadBalancer` type of `service`, `ingress` is another ways
to archive the same effect.

////
* 'nodePort' service
* 'LoadBalancer' service
* Ingress
////

in chapter 3, we`ve learned the basic concept of `ingress` object. to recap the
basics: comparing with `service`, Basically Ingress provides "Layer 7"
(application layer) load-balancing whereas the `service` provide "Layer 4"
(transport layer) load-balancing. 

.Ingress vs service
in "service" example we've demonstrated the end to end work flow when an
Internet host requests service provided by the backend pods inside of the
kubernetes, where FIP plays an important role. in `curl` or web browser you
input the FIP and service port, the request will reaches the backend pod. the
FIP is like a "gateway" between service inside of the cluster and outside world.

in reality, however, the "raw" IP is rarely used today - typically you access
all services via URLs. In the background there is "mapping" or "resolution" from
URL to IP and that is normally when DNS comes into picture. with DNS the URL is
resolved to an IP address. in practice, to ensures the availability, uptime and
performance, a public domain name is typically bound to a group of public IP
addresses and load sharing happens between them. that is why DNS sometimes is
also used to do loadbalancing.

kubernetes Ingress, works in a similar-looking but essentially totally different
way. with Ingress, instead of map URL to the backend server's IP directly, it is
mapped to another abstract we already know, the "service", and behind service
there are backends. since we've learned a lot about kubernetes `service` so far,
you understand what will happen basically after Ingress dispatch the URL request
to service.

that makes this Ingress section a little bit easier. we don't need to explain
everything that happens in Ingress. instead, we can focus on the Ingress FIP
exposure and service mapping. the second half of the story is all about service
to backend mapping which we've read a lot. we'll also introduce how does
contrail integrate with ingress and then demonstrate with an end to end lab
about how ingress works in contrail.

////

multiple ingress controller

----
metadata:
  name: foo
  annotations:
    kubernetes.io/ingress.class: "gce"
----
////

=== contrail ingress loadbalancer

like contrail's service implementation, contrail Ingress, is also implemented
through loadbalancer, but with a different `loadbalancer_provider` attribute,
accordingly `contrail-svc-monitor` component takes different actions to
implement `Ingress` in contrail environment.

Remember in "Contrail-Kubernetes architecture" section we gave the "object
mapping" between kubernetes and contrail. in there you see kubernetes `service`
maps to `ECMP loadbalancer (native)` and `Ingress` maps to `Haproxy
loadbalancer`. 

in `service` section when we were exploring the loadbalancer and
the relevant objects (listener, pool, and member), we noticed the loadbalancer's
`loadbalancer_provider` attribute is with a type `native`. 

        "loadbalancer_provider": "native",

in this section we'll look at contrail Ingress's `loadbalancer` and we'll
understand the similiarities and difficulties comaring with `service`
loadbalancer

=== contrail ingress workflow

Whenever an ingress is configured in kubernetes, `contrail-kube-manager` that is
watching the kube-apiserver get the events and creates an loadbalancer object in
contrail-controller.  `contrail-svc-monitor` component listens for the load
balancer objects and launches two haproxy processes, each in a seperate compute
node. both `haproxy` processes are programmed with appropriate configuration
based on the ingress rules you defined. the two haproxy processes work in
"active-standby" mode.  service request, as a type of `overlay` traffic,
traverses through gateway router and lands the "active" haproxy running in one
of the compute node, from there the traffic is "proxied" to the node where
backend pods are located and reaches the pods. In the case that the backend pods
are running in a different compute node, a MPLS over UDP tunnel is created
between the two compute node.

////
Contrail has its built-in implementation of Kubernetes Ingress loadbalancer -
the `HAProxy` load-balancer. 
////

////
this is how it works:

* `contrail-kube-manager` keeps listening to the `kube-apiserver` events
* user configures an `ingress` (rules)
* contrail `contrail-kuber-manager` gets the event from `kube-apiserver`
* contrail `contrail-kuber-manager` creates a `load-balancer object` in
  contrail DB.
* contrail `service monitor` component launches the HAProxy process, with
  appropriate configuration, based on the ingress rules defined in the yaml file.
////

contrail supports all 3 types of ingress:

* http-based single-service ingress, 
* simple-fanout ingress
* name-based virtual hosting ingress.

In this book we'll focus on the third type: name-based virtual hosting ingress.

=== Ingress testbed

in our lab we use the same testbed as what we use for `service` test:

image::https://user-images.githubusercontent.com/2038044/60372220-e28edb00-99c9-11e9-8918-1f0935a913ed.png[]

=== ingress: single service

=== ingress: simple fan-out

=== ingress: virtual host

Name-based virtual hosts support routing HTTP traffic to multiple host names at
the same IP address. based on the URL and rules, an ingress loadbalancer directs
traffic to different backend services, and each service direct traffic to its
backend pod. 

    www.juniper.net --|                 |-> service-1
                      |  101.101.101.1  |
    www.cisco.com   --|                 |-> service-2

to demonstrate `virtual host` type of Ingress, the objects that we need to
create are:

* an `Ingress` object: the rules, mapping 2 URLs to 2 backend services
* 2 backend services objects: `service-1`, `service-2`
* each service requires at least one pod as backend: `pod-1` for `service-1`,
  `pod-2` for `service-2`.

additionally, a "client" pod is needed to test the ingress from inside of the
cluster. we can use the same `cirros` pod we've used in earlier examples.

////
besides that, there are 2 components running in the background:

* an Ingress controller: in contrail environment it is `contrail-kube-manager`,
  running as a docker container in one of the kubernetes node.
* the loadbalancer: in contrail environment it is the `HAproxy` process,
  launched by `contrail-svc-monitor`

these are created automatically by contrail system so we don't need to worry
about them basically, but we need to understand their fundamental roles so
whenever things go wrong, these are the components we need to examine as part of
the troubleshooting flow
////

==== `ingress` definition

in our virtual host ingress test lab, we define the following rules:

* request toward URL `www.juniper.net` will be directed to a service `service-1`
  with `servicePort` 8888
* request toward URL `www.cisco.com` will be directed to a service `service-2`
  with `servicePort` 8888
* request toward any URLs other than these 2, will be directed to `service-1`
  with `servicePort` 8888

here is the corresponding yaml definition file:

----
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: service-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: service-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: service-2
              servicePort: 8888
            path: /
----

that's it. `Ingress` object itself is just about the `rules`, or more
specifically, mappings from a URL to the backend service. next we'll look at the
backend service.

==== `service` definition

we can use exactly the same service as introduced in `service` example. since we
defined 2 rules each for a URL, we need two services also. we just need to
"clone" the first service and then change the service's name and selector to
generate the second service. e.g.: this is definition of `service-1` service.

----
apiVersion: v1
kind: Service
metadata:
  name: service-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
  #type: LoadBalancer
----

NOTE: the service `type` is optional. with `Ingress`, `service` does not need to
be exposed to external directly anymore. therefore `LoadBalancer` type of
service is not required. 

==== backend `pod` definition

same as in `service` example, we can use exactly the same `webserver` rc. we
just need to clone it into 2 for the 2 services. this is the definition of one
of the RCs.

.service `service-1`

----
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

=== an "all in one" yaml file

as usual, we can create an individual yaml file for each of the objects. but
considering in `Ingress`, these objects always need to be created and removed
together, it is better to "merge" all yaml files into one. yaml syntax supports
this by using a "document delimitor", a `---` line between each object
definition. the benefits are:

* you can create all objects in the yaml file in one go, using just one `kubectl apply`
  command with the yaml file name
* similarly, if anything goes wrong and you need to clean up, you can delete
  all objects created with the yaml file in one `kubectl delete` command
* whenever needed, you can still delete/update each individual objects
  independently, by giving the object name

NOTE: imaging during test process you may need to create and delete all objects
as a whole very often, grouping multiple objects in one yaml file is a very
convenient and recommended method.

----
$ cat ingress/ingress-test.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-vh
spec:
  backend:
    serviceName: service-1
    servicePort: 8888
  rules:
    - host: www.juniper.net
      http:
        paths:
          - backend:
              serviceName: service-1
              servicePort: 8888
            path: /
    - host: www.cisco.com
      http:
        paths:
          - backend:
              serviceName: service-2
              servicePort: 8888
            path: /
---
apiVersion: v1
kind: Service
metadata:
  name: service-1
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-1
---
apiVersion: v1
kind: Service
metadata:
  name: service-2
spec:
  ports:
  - port: 8888
    targetPort: 80
  selector:
    app: webserver-2
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver-1
  labels:
    app: webserver-1
spec:
  replicas: 1
  selector:
    app: webserver-1
  template:
    metadata:
      name: webserver-1
      labels:
        app: webserver-1
    spec:
      containers:
      - name: webserver-1
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: ReplicationController
metadata:
  name: rc-webserver-2
  labels:
    app: webserver-2
spec:
  replicas: 1
  selector:
    app: webserver-2
  template:
    metadata:
      name: webserver-2
      labels:
        app: webserver-2
    spec:
      containers:
      - name: webserver-2
        image: savvythru/contrail-frontend-app
        securityContext:
           privileged: true
        ports:
        - containerPort: 80
----

////
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
////

=== `apply` the all-in-one yaml file

before applying the yaml file to get all objects created, let's take a quick
look at our two nodes, see if there is any `haproxy` process running already:

----
$ ps aux | grep haproxy
$ 
----

So the answer is no. haproxy will be created only after we created `Ingress` and
the corresponding loadbalancer object is seen by `contrail-service-monitor`.
we'll check this again after we create an `Ingress`.

----
$ kubectl apply -f ingress/ingress-test.yaml
ingress.extensions/ingress-vh created
service/service-1 created
service/service-2 created
replicationcontroller/rc-webserver-1 created
replicationcontroller/rc-webserver-2 created
----

the Ingress, two services and two RC objects are now created.

=== examine ingress objects

let's start to look at the Ingress object.

----
$ kubectl get ingresses.extensions -o wide
NAME        HOSTS                          ADDRESS                      PORTS  AGE
ingress-vh  www.juniper.net,www.cisco.com  10.47.255.248,101.101.101.1  80     8m27s
----

----
$ kubectl get ingresses.extensions -o yaml
apiVersion: v1
items:
- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    ......
    generation: 1
    name: ingress-vh
    namespace: ns-user-1
    resourceVersion: "830991"
    selfLink: /apis/extensions/v1beta1/namespaces/ns-user-1/ingresses/ingress-vh
    uid: 8fd3e8ea-9539-11e9-9e54-0050569e6cfc
  spec:
    backend:
      serviceName: service-1
      servicePort: 8888
    rules:
    - host: www.juniper.net
      http:
        paths:
        - backend:
            serviceName: service-1
            servicePort: 8888
          path: /
    - host: www.cisco.net
      http:
        paths:
        - backend:
            serviceName: service-2
            servicePort: 8888
          path: /
  status:
    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----

as expected, the rules are defined properly, within each rule there is a mapping
from URL to the corresponding service. what may confuse you is the two IP
addresses shown here: 

    loadBalancer:
      ingress:
      - ip: 101.101.101.1
      - ip: 10.47.255.248

//in `namespace` section we've known `10.32.0.0/12` is the default pod subnet.

we've seen these two subnets in service examples:

* 10.47.255.x is IP allocated from the pod's default subnet
* 101.101.101.x is the FIP associated with the VM IP.

but the question is why an Ingress even needs an pod IP and FIP?

////
this is the IP addresses allocated to the haproxy "virtual machine". 
what is a haproxy "virtual machine" anyway? isn't it just a process running in
the compute node? or, does compute node spawned some hidden VMs behind the
scene? 
////

let's hold the answer for now and continue to check service and pod
objects created from the all-in-one yaml file. we'll come back to this shortly.

.service objects
----
$          kubectl    get            svc          -o        wide
NAME       TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)   AGE   SELECTOR
service-1  ClusterIP  10.99.225.17   <none>       8888/TCP  24h   app=webserver-1
service-2  ClusterIP  10.105.134.79  <none>       8888/TCP  24h   app=webserver-2
----

the two services are also created, each is allocated a clusterIP. we've seen
this before and it looks nothing special. now look at the pods:

.backend and client pod
----
$ kubectl get pod -o wide
NAME                      READY  STATUS   RESTARTS  AGE    IP             NODE     NOMINATED  NODE
rc-webserver-1-djt9t      1/1    Running  0         7m44s  10.47.255.244  cent222  <none>
rc-webserver-2-t6fhs      1/1    Running  0         3m57s  10.47.255.242  cent222  <none>
cirros1-85fc7dd848-fvnc5  1/1    Running  0         16s    10.47.255.239  cent333  <none>

$ kubectl get pod -o wide -l app=webserver-1
NAME                   READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE
rc-webserver-1-djt9t   1/1     Running   0          153m   10.47.255.244   cent222   <none>

$ kubectl get pod -o wide -l app=webserver-2
NAME                   READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE
rc-webserver-2-t6fhs   1/1     Running   0          149m   10.47.255.242   cent222   <none>
----

everything looks fine. for each service there is a backend pod. we have learned
how selector and label works in service-pod associations so there is nothing new
here. next we'll examine the haproxy and try to make some sense out of the 2 IPs
allocated to Ingress object.

=== examine haproxy process again

earlier before the Ingress is created, we were looking for haproxy process in
node but could not see anything. let's check it again and see if any magic
happens:

.node `cent222`

----
$ ps aux | grep haproxy
188   7934  0.0  0.0  55440   852  ?      Ss  18:04  0:00  haproxy  
  -f /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.conf 
  -p /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.pid
----

.node `cent333`

----
$ ps aux | grep haproxy
188       4380  0.0  0.0  55584   928 ?        Ss   18:04   0:00 haproxy 
  -f /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.conf 
  -p /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.pid -sf 4348
----

right after ingress got created, we see a haproxy process created in each node!


remember earlier when we talk about ingress contrail implementation, we've said
contrail Ingress is also implemented through loadbalancer (like service), but
Ingress's loadbalancer is with a different `loadbalancer_provider` attribute,
which makes `contrail-svc-monitor` takes a different action than what it does
for service. now it is the time to tell that the `loadbalancer_provider` is
`opencontrail`, and accordingly the `contrail-svc-monitor` action is to launch a
haproxy process running with ingress rules in its configuration file. this
basically explains what we see now.

that is only a high level overview about the contrail's implementation of
ingress. in fact the for each loadbalancer with `loadbalancer_provider` being
`opencontrail`, `contrail-svc-monitor` will a service-instance (SI). next we'll
explore the objects in a little bit more details.

=== exploring Ingress service-instance objects

other important details we've not talked yet are:

* the contrail-svc-monitor creates a `service instance` (SI) with
  `haproxy-loadbalancer` type of template applied. 
* the SI will link to a VM via a "port tuple"
* in Ingress ,the VM is not a "real" VM but a linux namespace, the namespace VM
  will also has its VMI, and a reference to an instance-ip


image::https://user-images.githubusercontent.com/2038044/60989518-3bd50380-a314-11e9-8bee-abfc5cbc400f.png[]

now we can explain the IP `10.47.255.248` seen in ingress: it is an
cluster-internal pod IP address allocated to the SI, which is the frontend IP
that the loadbalancer will listen. it is also the one that the ingress FIP maps
to with NAT. whenever node receives traffic destined to the ingress FIP, vrouter
will translate the "public" FIP into this "private" loadbalancer IP address.
loadbalancer, through haproxy, will do proxy and forward the traffic to the
service and eventually backend pod.

//LB, LB-listeners, LB pools, LB members, etc
//https://github.com/Juniper/contrail-web-controller/blob/master/specs/load_balancing.md

now we see the haproxy process is running, we can check further on its
configuration file and confirm the ingress rules are programmed properly.

=== examine `haproxy.conf` file

in each (compute) node, under `/var/lib/contrail/loadbalancer/` folder there
will be a `haproxy` subfolder. the file structure looks like this:

----
.
├── ba92b465-9ae9-11e9-9b3e-0050569e6cfc.conf
└── haproxy
    └── ba92b465-9ae9-11e9-9b3e-0050569e6cfc
        ├── haproxy.conf
        ├── haproxy.pid
        └── haproxy.sock
----

you can check either `ba92b465-9ae9-11e9-9b3e-0050569e6cfc.conf` or
`haproxy/haproxy.conf` file for the haproxy configuration:

----
$ cd /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/
$ cat haproxy.conf
global
        daemon
        user haproxy
        group haproxy
        log /var/log/contrail/lbaas/haproxy.log.sock local0
        log /var/log/contrail/lbaas/haproxy.log.sock local1 notice
        tune.ssl.default-dh-param 2048
        ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
        ulimit-n 200000
        maxconn 65000
        stats socket /var/lib/contrail/loadbalancer/haproxy/8fd3e8ea-9539-11e9-9e54-0050569e6cfc/haproxy.sock mode 0666 level user

defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 300000
        timeout server 300000

frontend acf8b96d-b322-4bc2-aa8e-0611baa43b9f

        option tcplog
        bind 10.47.255.248:80                   #<---loadbalancer IP
        mode http
        option forwardfor

        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host hdr(host) -i www.juniper.net
        acl 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path path /
        use_backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6 if 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_host 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6_path

        default_backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46

        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host hdr(host) -i www.cisco.net
        acl 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path path /
        use_backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f if 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_host 1e1e9596-85b5-4b10-8e14-44d1ca50a92f_path

backend 77c6ad05-e3cc-4be4-97b2-4e6a681ec8e6    #<---service-1
        mode http
        balance roundrobin
        option forwardfor
        server 33339e1c-5011-4f2e-a276-f8dd37c2cc51 10.101.158.92:8888 weight 1

backend cd7a7a5b-6c49-4c23-b656-e23493cf7f46    #<---default
        mode http
        balance roundrobin
        option forwardfor
        server e8384ee4-7270-4272-b765-61488e1d3e9c 10.101.158.92:8888 weight 1

backend 1e1e9596-85b5-4b10-8e14-44d1ca50a92f    #<---service-2
        mode http
        balance roundrobin
        option forwardfor
        server aa0cde60-2526-4437-b943-6f4eaa04bb05 10.104.4.232:8888 weight 1
----

the configuration seems like a little bit complicated, but the most important
part of it is relatively easy to understand:

* the haproxy `frontend` section defines each URL, or `host`, and its path. here the
  2 hosts are `www.juniper.net` and `www.cisco.com`. `path` is what follows the
  host part in the URL string, in our case both are `/`. 
* the haproxy `backend` section defines the `server`, which is `service` in our
  case. it has a format of `serviceIP:servicePort`, which is the exact `service`
  object we've created using the all-in-one yaml file.
* `use_backend` command in `frontend` section declares the ingress rules: `if` the
  request includes specific URL and path, direct it toward the corresponding
  service.
* `default_backend` defines which backend is the "default": it will be used when
  a haproxy receives a URL request that is other than the two defined one

through this configuration, the haproxy implemented our ingress:

////
.haproxy frontend:

* 10.47.255.248:80 is the frontend IP and port facing clients

.haproxy backend:
////

* `www.juniper.net` and `/` composes the full URL, request will be dispatched to
  `service-1` (`10.101.158.92:8888`)
* `www.cisco.net` and `/` composes the full URL, request will be dispatched to
   `service-2` (`10.104.4.232:8888`)
* other URLs goes to default backend which is service `service-1`

=== examine gateway router VRF table

we've explored a lot inside of the cluster. now let's look at the gateway
router's VRF table. 

----
labroot@camaro> show route table k8s-test protocol bgp

k8s-test 7 destinations, 7 routes (7 active, 0 holddown, 0 hidden)
@ = Routing Use Only, # = Forwarding Use Only
+ = Active Route, - = Last Active, * = Both

101.101.101.1/32   *[BGP/170] 02:46:13, MED 100, localpref 200, from 10.169.25.19
                       AS path: ?
                    validation-state: unverified, > via gr-2/2/0.32771, Push 61

----

Same as in service example, from outside of the cluster, only FIP is visible.
`detail` version of it conveys more information:

----
labroot@camaro> show route table k8s-test 101.101.101.1 detail

k8s-test 24 destinations, 49 routes (24 active, 0 holddown, 0 hidden)
101.101.101.1/32 (1 entry, 1 announced)
        *BGP    Preference: 170/-201
                Route Distinguisher: 10.169.25.20:5     #<---
                ......
                Source: 10.169.25.19
                Next hop: via gr-2/2/0.32771, selected
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                ......
                Protocol next hop: 10.169.25.20         #<---
                Label operation: Push 61
                Label TTL action: prop-ttl
                Load balance label: Label 61: None;
                Indirect next hop: 0x900d320 1048597 INH Session ID: 0x6f9
                State: <Secondary Active Int Ext ProtectionCand>
                Local AS: 13979 Peer AS: 60100
                Age: 34         Metric: 100     Metric2: 0
                Validation State: unverified
                Task: BGP_60100_60100.10.169.25.19
                Announcement bits (1): 1-KRT
                AS path: ?
                Communities: target:500:500 target:64512:8000016
                    encapsulation:unknown(0x2) encapsulation:mpls-in-udp(0xd)
                    unknown type 8004 value eac4:7a1207 unknown type 8071 value
                    eac4:b unknown type 8084 value eac4:10000 unknown type 8084
                    value eac4:ff0004 unknown type 8084 value eac4:1040000
                Import Accepted
                VPN Label: 61
                Localpref: 200                          #<---
                Router ID: 10.169.25.19
----
 
* through XMPP, vrouter advertises the FIP prefix to contrail controller.
  at least 2 pieces of information from the output indicates who originates the
  FIP - node `cent222`:
  - `Protocol next hop` being `10.169.25.20`
  - `Route Distinguisher` being `10.169.25.20:5`
* through MP-BGP, contrail controller "reflects" the FIP prefix to the gateway
  router, `Source: 10.169.25.19` indicates this fact.

so it looks `cent222` is selected to be the active haproxy node, and so the
other node `cent333` will be the standby one. therefore you should expect
service request coming from Internet host goes to node `cent222`. of course, the
overlay traffic will be carried in MPLS over GRE tunnel, same as what you've
seen from service example.

NOTE: both nodes have loadbalancer and haproxy running so both will advertise the
FIP prefix `101.101.101.1` to gateway router. however, they are advertised with
different local preference value. the "Active" node has a value of `200` and the
"standby" node's value is `100`. that is why the "other" BGP route is dropped
and only one is displayed. `Localpref` being `200` proves it is coming from the
active compute node.

=== verify ingress: from internal

we've explored a lot about ingress configuration, now it is time to verify the
result. since the Ingress serves both inside and outside of the cluster, our
verification work will start from the cirros pod inside of cluster, then from
the Internet host outside of it.

.from inside of cluster

----
$ kubectl exec -it cirros -- curl -H 'Host:www.juniper.net' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- curl -H 'Host:www.cisco.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = rc-webserver-2-m2272

$ kubectl exec -it cirros -- curl -H 'Host:www.google.com' 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]

$ kubectl exec -it cirros -- curl 10.47.255.238:80 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]
----

we still use the `curl` command to trigger HTTP requests towards the ingress's
loadbalancer IP. the return proves the test works: the 2 requests towards
"juniper" and "cisco" URL is proxied to 2 different backend pods, through 2
backend services `service-1` and `service-2` respectively. the third request
towards "google" is a "unknown" URL which does not have a matching service, so
it goes to the default backend service - `service-1`. 

The `-H` option plays an important role in Ingress test with `curl`. it carries
the full URL in HTTP payload that the loadbalancer is waiting for. without it
the HTTP header will carry `Host: 10.47.255.238`, which has no matching rule,
so it will be treated same as with a unknown URL.

=== verify ingress: from external (Internet host)

to make sure the URL resolves to the FIP address, we need to update `/etc/hosts`
file by adding one line in the end:

----
# echo "101.101.101.1  www.juniper.net www.cisco.com www.google.com" >> /etc/hosts
# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
101.101.101.1  www.juniper.net www.cisco.com www.google.com
----

now, from internet host's "desktop", we launch two chrome page side by side, and
input URLs `www.juniper.net` and `www.cisco.com`. By keep refreshing the 2 pages
we can confirm "juniper" page is always returned by RC `rc-webserver-1` pod
`10.47.255.236`, "cisco" page is always returned by RC `rc-webserver-2` pod
`10.47.255.235`. we launch a third chrome page and input `www.google.com`, we
see "google" page is returned by the same pod serving "cisco" URL.

image::https://user-images.githubusercontent.com/2038044/60478459-c6e93600-9c50-11e9-848b-a73e9c6d010f.png[]

`curl` command test result is exactly the same as what we've seen when testing
from a pod, except this time we send requests to FIP, instead of the
loadbalancer internal IP.

----
$ curl -H 'Host:www.juniper.net' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]

$ curl -H 'Host:www.cisco.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.235
                        Hostname = rc-webserver-2-m2272

$ curl -H 'Host:www.google.com' 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]

$ curl 101.101.101.1 | w3m -T text/html | cat
                                     Hello
                     This page is served by a Contrail pod
                          IP address = 10.47.255.236
                        Hostname = rc-webserver-1-g65dg
                                    [giphy]
----

=== ingress work flow

to conclude the ingress section, let's examine the traffic flow step by step in
our lab setup, starting from Internet host, through gateway router, haproxy to
the destination pod. earlier we uses gateway router's VRF routing table and
forwarding table to show the packet next hops, this time we'll introduce
more useful tools available in node and client host. after understanding these
tools you will be able to troubleshoot both the routing and forwarding plane
problems in contrail kubernetes environment. the tools are:

* curl debug option
* tcpdump packet capture on tap interface and fabric interface
* vrouter flow table with prefix filter
* vrouter VRF routing table
* shell script

after this section you will get a clear picture about the "whole story". we'll
then conclude the ingress section by a figure showing the end to end flow.

==== Internet Host -> gateway router

----
[root@cent-client ~]# curl -vH 'Host:www.juniper.net' 101.101.101.1
* About to connect() to 101.101.101.1 port 80 (#0)
*   Trying 101.101.101.1...
* Connected to 101.101.101.1 (101.101.101.1) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Accept: */*
> Host:www.juniper.net
>
* HTTP 1.0, assume close after body
< HTTP/1.0 200 OK
< Content-Type: text/html; charset=utf-8
< Content-Length: 359
< Server: Werkzeug/0.12.1 Python/2.7.12
< Date: Tue, 02 Jul 2019 16:50:46 GMT
* HTTP/1.0 connection set to keep alive!
< Connection: keep-alive
<

<html>
<style>
  h1   {color:green}
  h2   {color:red}
</style>
  <div align="center">
  <head>
    <title>Contrail Pod</title>
  </head>
  <body>
    <h1>Hello</h1><br><h2>This page is served by a <b>Contrail</b>
    pod</h2><br><h3>IP address = 10.47.255.236<br>Hostname =
    rc-webserver-1-g65dg</h3>
    <img src="/static/giphy.gif">
  </body>
  </div>
</html>
* Connection #0 to host 101.101.101.1 left intact
----

first we use `-v` option in `curl` command. with this option it prints more
verbose information about the HTTP interaction. `>` lines are what `curl` sent
out, and `<` lines are what it receives from remote. from the interaction we see
`curl` sent a HTTP `GET` to path `/` to the FIP 101.101.101.1, with `Host`
filled with "juniper" URL. it gets the response with code `200 OK`, indicating
the request has succeeded. there are a bunch of other headers in the response
which is not important for our test so we can skip. the rest part of the
response is the HTML source code of a returned web page. the connection is closed
immediately afterward.

NOTE: the `curl` tool implementation will always close the TCP session right
after the HTTP request is responded. although this is a safe and clean behavior
in practice, it may bring some difficulties in our test. we prefer the
connection to remain open for a while so we can take time to investigate some
details during our analysis. there are some methods to workaround that.  e.g.
you can install a large file in the webserver and try to pull it from curl, that
way as long as the file transfer is ongoing the session remains. in our test
below we will show how to use a small shell script to workaround this problem.

==== gateway router -> active haproxy node: MPLS over GRE

we've seen gateway router's routing and forwarding table earlier, so we already
know the packet will be sent to node `cent222` via MPLSoGRE tunnel. to
demonstrate the forwarding flow, this time we collect the flow table in node
`cent222`. 

NOTE: with a filter `--match 15.15.15.2`. only flow entries with Internet Host
IP is printed.

////
----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
......
Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)

   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)
----
* a user from Internet Host sends a http request by typing the URL
  "http://www.juniper.net" and hit enter
* DNS resolves the host to FIP address
* via default route Internet Host send HTTP request to gateway router's VRF
* gateway router learns the VIP prefix in VRF with next-hop pointing to the
  compute node running active haproxy, in this case node `cent222`

////

----
(vrouter-agent)[root@cent222 /]$ flow --match 15.15.15.2
Flow table(size 80609280, entries 629760)

Entries: Created 586803 Added 586861 Deleted 1308 Changed 1367Processed 586803 Used Overflow entries 0
(Created Flows/CPU: 147731 149458 144549 145065)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([15.15.15.2]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   114272<=>459264       15.15.15.2:58282                                    6 (2->2)
                         101.101.101.1:80
(Gen: 3, K(nh):89, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):61,
 Stats:2/112,  SPort 50985, TTL 0, Sinfo 192.168.0.204)

   459264<=>114272       10.47.255.238:80                                    6 (2->5)
                         15.15.15.2:58282
(Gen: 1, K(nh):89, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):89,
 Stats:1/74,  SPort 60289, TTL 0, Sinfo 8.0.0.0)

(vrouter-agent)[root@cent222 /]$ nh --get 61
Id:61         Type:Tunnel         Fmly: AF_INET  Rid:0  Ref_cnt:3316       Vrf:0
              Flags:Valid, MPLSoGRE, Etree Root,
              Oif:0 Len:14 Data:f0 1c 2d 41 90 00 00 50 56 9e 62 25 08 00
              Sip:10.169.25.20 Dip:192.168.0.204
----
////
(vrouter-agent)[root@cent222 /]$ nh --get 89
Id:89         Type:Encap          Fmly: AF_INET  Rid:0  Ref_cnt:7          Vrf:2
              Flags:Valid, Policy, Etree Root,
              EncapFmly:0806 Oif:8 Len:14
              Encap Data: 02 c0 0a c1 e6 6c 00 00 5e 00 01 00 08 00
////

this rephrases the same fact as what we've seen from gateway router's VRF table:

* the first flow entry displays the source and destination of the http request,
  it is coming from Internet host and lands the FIP in current node `cent222`

* `S(nh):61` in is the next hop to the source of the request - the Internet host
  this is similiar concept like the reverse path forwarding(RPF). vrouter
  maintains the path toward the source of the packet in the flow.

* `nh --get` command resolves the nexthop 61 with more details, we see a
  `MPLSoGRE` flag is set, `Sip` and `Dip` is the two end of the GRE tunnel,
  they are current node and gateway router's loopback IP respectively.


==== NAT: FIP -> loadbalancer IP: NAT
////
----
(vrouter-agent)[root@cent222 /]$ flow --match 101.101.101.1
Flow table(size 80609280, entries 629760)

Entries: Created 1856648 Added 1856785 Deleted 3015 Changed 3234Processed 1856648 Used Overflow entries 0
(Created Flows/CPU: 467916 472342 457241 459149)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead
Listing flows matching ([101.101.101.1]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   290848<=>13004        15.15.15.2:56186                                    6 (3->3)
                         101.101.101.1:80
(Gen: 1, K(nh):58, Action:N(D), Flags:, TCP:SSrEEr, QOS:-1, S(nh):42,
 Stats:5/309,  SPort 52637, TTL 0, Sinfo 192.168.0.204)

(vrouter-agent)[root@cent222 /]$ flow --match 10.47.255.238
......
Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
    13004<=>290848       10.47.255.238:80                                    6 (3->4)
                         15.15.15.2:56186
(Gen: 1, K(nh):58, Action:N(S), Flags:, TCP:SSrEEr, QOS:-1, S(nh):58,
 Stats:4/272,  SPort 61571, TTL 0, Sinfo 3.0.0.0)
----
////

to verify the NAT operation, we only need to dig a little bit more out of the
same flow output.

* the `Action` flag, `N(D)` in the first entry and `N(S)` in the second,
  indicate the two type of NAT operations:
  - destination NAT - `DNAT`, destination IP 101.101.101.1 will be translated to
    the other IP (the loadbalancer IP)
  - source NAT - `SNAT`, source IP `10.47.255.238` will be translated to the
    other IP (the FIP)
  
* the second flow entry is also called a "reverse flow" of the first one. it is
  the flow entry vrouter uses to send packet towards Internet host. from
  loadbalancer's perspective it only uses `10.47.255.248` assigned from the
  default pod network as its source IP, it does not knows anything about the
  FIP. same thing for the external host, it only knows how to reach the FIP and
  has no clues about the private loadbalancer IP. it is vrouter that is doing
  the two way NAT translations in between.

overall, what the flow table tells is that on receiving the packet destined to
the FIP, vrouter on node `cent222` performs NAT and translates destination FIP
IP to the loadbalancer's IP (10.47.255.238). after that the packet lands the
loadbalancer's VRF, where haproxy is watching and doing the proxy work.

==== active haproxy -> service IP: MPLS over UDP

now the packet lands in loadbalancer VRF and it is in the frontend of the proxy.
what the haproxy supposes to do is:

* haproxy listening on the frontend IP (loadbalancer's IP) see the packet
* haproxy checks the ingress rule programmed in its config file, decides that
  the requests need to be proxied to service IP of `service-1`. 
* loadbalancer checks its VRF table and sees the prefix of `service-1` IP 
  is learned from destination node `cent333`, it pushes a MPLS label and
  send it through MPLS over UDP tunnel.

to verify haproxy process packet processing details, we captured the packet on
the physical interface of node `cent222`, where the active haproxy process is
running.

//image::https://user-images.githubusercontent.com/2038044/60518123-e1ea9300-9cae-11e9-82ec-d341e32e42c8.png[]
image::https://user-images.githubusercontent.com/2038044/60539848-aadea680-9cdb-11e9-8896-c4824d17dd9d.png[]

from the wireshark screenshot, we see clearly that:

* the HTTP request packet is "forwarded" to the service IP, which is the other
  node `cent333`. that is why we see underlay destination IP of the request is
  `10.169.25.21`
* sending overlay packets between compute node requirs MPLSoUDP tunnel. 

everything is working as expected.

now if you are observant enough, you should have noticed something "weird" in
this capture. questions are:

* shouldn't the source IP address be the Internet host's IP `15.15.15.2`,
  instead of loadbalancer's frontend IP? 
* is the packet "forwarded"?
* is the transaction within the same TCP session from Internet host, accrossing
  gateway router and loadbalancer node `cent222`, all the way up to the backend
  pod sitting in node 'cent333`?

The answer is NO. the haproxy in this test is doing layer4-7 level
loadbalancing. what it does is to establish TCP connection with Internet host
and keep monitoring the HTTP request. whenever it see an request coming in, it
checks its rule and initiates a brand new TCP connection to the corresponding
backend, it "copies" the original HTTP request from Internet host and "paste"
into the new TCP connection with its backend. precisely speaking the http
request is "proxied", not "forwarded".

//capture on haproxy interface
//image::https://user-images.githubusercontent.com/2038044/60540296-d1e9a800-9cdc-11e9-8914-fbe4fc59ae60.png[]

==== service IP -> backend pod IP: NAT

at the moment we know the http request is "proxied" to haproxy's backend. that
backend is a kubernetes `service`, and to reach the `service` the request is
sent to node `cent333` where all backend pod is sitting.

on destination node `cent333`, when packet comes in from loadbalancer IP
`10.47.255.238` toward the service IP `10.99.225.17` of `service-1`, vrouter
needs to translate the service IP to the backend podIP `10.47.255.236`. the
translation is again a NAT operation, pretty much the same as what we've seen
many times earlier, whenever FIP is involved. 

packet capture on the pod interface also reveals the packet interaction between
the loadbalancer IP and backend podID.

----
$ tcpdump -ni tapeth0-baa392 -v
12:01:07.701956 IP (tos 0x0, ttl 63, id 32663, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [S], cksum 0xd88d (correct), seq 2129282145, win 29200, options [mss 1420,sackOK,TS val 515783670 ecr 0,nop,wscale 7], length 0
12:01:07.702012 IP (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto TCP (6), length 60)
    10.47.255.236.http > 10.47.255.238.51968: Flags [S.], cksum 0x1468 (incorrect -> 0x8050), seq 3925744891, ack 2129282146, win 28960, options [mss 1460,sackOK,TS val 515781436 ecr 515783670,nop,wscale 7], length 0
12:01:07.702300 IP (tos 0x0, ttl 63, id 32664, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1f57 (correct), ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 0
12:01:07.702304 IP (tos 0x0, ttl 63, id 32665, offset 0, flags [DF], proto TCP (6), length 159)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [P.], cksum 0x6fac (correct), seq 1:108, ack 1, win 229, options [nop,nop,TS val 515783671 ecr 515781436], length 107: HTTP, length: 107
        GET / HTTP/1.1
        User-Agent: curl/7.29.0
        Accept: */*
        Host:www.juniper.net
        X-Forwarded-For: 15.15.15.2

12:01:07.702336 IP (tos 0x0, ttl 64, id 12224, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1eee), ack 108, win 227, options [nop,nop,TS val 515781436 ecr 515783671], length 0
12:01:07.711882 IP (tos 0x0, ttl 64, id 12225, offset 0, flags [DF], proto TCP (6), length 69)
    10.47.255.236.http > 10.47.255.238.51968: Flags [P.], cksum 0x1471 (incorrect -> 0x5f06), seq 1:18, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 17: HTTP, length: 17
        HTTP/1.0 200 OK
12:01:07.712032 IP (tos 0x0, ttl 64, id 12226, offset 0, flags [DF], proto TCP (6), length 550)
    10.47.255.236.http > 10.47.255.238.51968: Flags [FP.], cksum 0x1652 (incorrect -> 0x1964), seq 18:516, ack 108, win 227, options [nop,nop,TS val 515781446 ecr 515783671], length 498: HTTP
12:01:07.712152 IP (tos 0x0, ttl 63, id 32666, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [.], cksum 0x1ec7 (correct), ack 18, win 229, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712192 IP (tos 0x0, ttl 63, id 32667, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.238.51968 > 10.47.255.236.http: Flags [F.], cksum 0x1ccb (correct), seq 108, ack 517, win 237, options [nop,nop,TS val 515783681 ecr 515781446], length 0
12:01:07.712202 IP (tos 0x0, ttl 64, id 12227, offset 0, flags [DF], proto TCP (6), length 52)
    10.47.255.236.http > 10.47.255.238.51968: Flags [.], cksum 0x1460 (incorrect -> 0x1cd5), ack 109, win 227, options [nop,nop,TS val 515781446 ecr 515783681], length 0
----

this may not be very convincing in one aspect. the packet capture shows the
communication is between loadbalancer IP and podIP, that part is fine. problem
is this is solely from the pod's perspective "after" the NAT operation. it does
not shows what happens right before NAT.

we can rely on the flow table again, as what we've seen on node `cent222` where
vrouter does NAT between FIP and loadbalancer IP. the problem is, as we
mentioned earlier, `curl` get its job done pretty fast, it open the session,
send request, get response, then close it. in fast this process is too fast to
be captured in the flow table. as soon as you hit "enter" to curl command,
everything is done in less than 2 or even 1 second. by the time you type in
flow command, everything is done and you end up with empty table. we could
change the server's behavior - let it send a big file to hold the session, or
we can repeat the test to take some chance. in the client terminal with a
one-liner script we can repeat the test over and over, then in compute terminal
using same script we can keep dumping the flow table. over the time we will
have the chance to capture the flow table at the right moment. then we stop the script
and investigate the table.

compute side script:

----
while :; do flow --match 10.47.255.238; sleep 0.2; done
----

Internet host side script:

----
while :; do curl -H 'Host:www.juniper.net' 101.101.101.1 | lynx -stdin --dump | cat; sleep 3; done
----

here is the flow table we captured before its gone.

----
evrouter-agent)[root@cent333 /]$ flow --match 10.47.255.238
Flow table(size 80609280, entries 629760)
Entries: Created 482 Added 482 Deleted 10 Changed 10Processed 482 Used Overflow entries 0
(Created Flows/CPU: 163 146 18 155)(oflows 0)

Action:F=Forward, D=Drop N=NAT(S=SNAT, D=DNAT, Ps=SPAT, Pd=DPAT, L=Link Local Port)
 Other:K(nh)=Key_Nexthop, S(nh)=RPF_Nexthop
 Flags:E=Evicted, Ec=Evict Candidate, N=New Flow, M=Modified Dm=Delete Marked
TCP(r=reverse):S=SYN, F=FIN, R=RST, C=HalfClose, E=Established, D=Dead

Listing flows matching ([10.47.255.238]:*)

    Index                Source:Port/Destination:Port                      Proto(V)
 ----------------------------------------------------------------------------------
   403188<=>462132       10.47.255.236:80                                    6 (2->4)
                         10.47.255.238:57760
(Gen: 1, K(nh):23, Action:N(SPs), Flags:, TCP:SSrEEr, QOS:-1, S(nh):23,
 Stats:2/140,  SPort 52190, TTL 0, Sinfo 4.0.0.0)

   462132<=>403188       10.47.255.238:57760                                 6 (2->2)
                         10.99.225.17:8888
(Gen: 1, K(nh):23, Action:N(DPd), Flags:, TCP:SSrEEr, QOS:-1, S(nh):26,
 Stats:3/271,  SPort 65421, TTL 0, Sinfo 10.169.25.20)
----

obviously the second entry is triggered by the incoming request. haproxy follows
its rules inheritated from our Ingress definition and dispatchs the request of
"juniper" URL to `service-1`, whose IP:port is `10.99.225.17:8888`. vrouter see
the service IP and knows that supposes to go to backend podIP `10.47.255.236`.
it does NAT between the two IPs. 

finally, pod sees the HTTP request and responds back with a web page.

////
----
$ tcpdump -ni tapeth0-baa392
23:37:29.754864 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [S], seq 1528773587, win 29200, options [mss 1420,sackOK,TS val 384765722 ecr 0,nop,wscale 7], length 0
23:37:29.754922 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [S.], seq 953745157, ack 1528773588, win 28960, options [mss 1460,sackOK,TS val 384763489 ecr 384765722,nop,wscale 7], length 0
23:37:29.755247 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 0
23:37:29.755253 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [P.], seq 1:71, ack 1, win 229, options [nop,nop,TS val 384765724 ecr 384763489], length 70: HTTP: GET / HTTP/1.1
23:37:29.755291 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 71, win 227, options [nop,nop,TS val 384763489 ecr 384765724], length 0
23:37:29.766886 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [P.], seq 1:18, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 17: HTTP: HTTP/1.0 200 OK
23:37:29.767032 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [FP.], seq 18:516, ack 71, win 227, options [nop,nop,TS val 384763501 ecr 384765724], length 498: HTTP
23:37:29.767188 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [.], ack 18, win 229, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767210 IP 10.47.255.238.57554 > 10.47.255.236.http: Flags [F.], seq 71, ack 517, win 237, options [nop,nop,TS val 384765736 ecr 384763501], length 0
23:37:29.767218 IP 10.47.255.236.http > 10.47.255.238.57554: Flags [.], ack 72, win 227, options [nop,nop,TS val 384763501 ecr 384765736], length 0
----
////

==== returning traffic

on the reverse direction, podIP runs webserver and responds with it's web page.
the response follows the reverse path of the request:

* pod responds to loadbalancer frontend IP, across MPLSoUDP tunnel
* vrouter on node `cent333` perform source NAT, translating podIP into service IP
* respond reaches to active haproxy running on node `cent222`
* haproxy terminate the tcp connection with backend pod, "copies" the http
  response, and "paste" into its connection with the remote Internet host
* vrouter on node `cent222` perform source NAT, translating loadbalancer
  frontend IP to FIP
* response is sent to gateway router, which forwards to Internet host
* Internet host gets the response.

==== the whole story

this figure visualizes the basic service request path:

image::https://user-images.githubusercontent.com/2038044/60410376-09017180-9b96-11e9-927e-4cf1d98f2cef.png[]

== contrail multiple interface pod

=== multiple interface pod introduction

in Kubernetes cluster, typically each pod only has one network interface (except
the `loopback` interface). In reality, there are scenarios where multiple
interfaces are required. e.g. in contrail solution service chain model, a
service instance typically needs a "left", "right" and optionally a "management"
interface to manipulate the service traffic. Service Providers also tend to keep the
management and tenant networks independent for isolation, and management
purpose. Multiple interfaces provide a way for containers to be connected to
multiple devices in multiple networks simultaneously.

//a pod may requires a "data interface" to carry the
//service traffic, and a "management interface" for the reachability detection. 

=== contrail as a CNI

////
As you probably already know containers use namespaces to isolate resources and
rate limit their use. Linux’s network namespaces are used to glue container
processes and the host networking stack. Docker spawns a container in the
containers own network namespace and later on runs a veth pair (a cable with two
ends) between the container namespace and the host network stack
////

in container technology, A virtual network device pair abstraction (the `veth`)
is functioning pretty much like a virtual "cabel", that can be used to create
tunnels between network namespaces. one end of it is "plugged" in the container
and the other end is in the host. it can also be used to create a bridge to a
physical network device in another namespace.

A "CNI plugin" is the one who is responsible for inserting the network
interface, as one end of the veth pair, into the container network namespace.
it will also makes all necessary changes on the host. e.g. attaching the other
end of the veth into a bridge, assigning IP, configuring routes, and so on.

.container and veth pair
image::https://user-images.githubusercontent.com/2038044/60554760-ee9ad580-9d06-11e9-9628-f01af759f6e1.png[]

contrail is one of such "CNI plugin" implementations.  there are many publicly
available CNI plugin implementation today. for a comprehensive list you can
check https://github.com/containernetworking/cni (where contrail is also
listed). for example, `multus-cni`, is another CNI plugin "enables attaching
multiple network interfaces to pods". multus-cni's multipe-network support is
accomplished by Multus calling multiple other CNI plugins. because each plugin
will create its own network, so overall a pod can have multiple networks.  one
of the main advantages that contrail provides, comparing with `mutus-cni` and
all other current implementations in the industry, is that contrail by itself
provides the ability to attach multiple network interfaces to a kubernetes pod,
without the need to call other plugins. this brings support to a truly
"multi-homed" pod.

contrail CNI follows the Kubernetes Network `CRD` (Custom Resource Definition)
Standard to provide a standardized method to specify the configurations for
additional network interfaces. there is no change to the standard kubernetes
upstream APIs, making the implementation coming with the most compatibilities.

=== CRD and contrail-kube-manager

////
Kubernetes supports a custom extension to represent networks in its object
model, through its `CustomResourceDefinition(CRD)` feature. This extension adds
support for a new kind of object called `NetworkAttachmentDefinition`, which
represents a network in Kubernetes data model.
////

a CRD object defines the template for a network object
`NetworkAttachmentDefinition`, which contains all information about each
network's specification, and tells Kubernetes API how to understand and expose
it. in contrail setup the CRD is created by a component named
`contrail-Kube-Manager`, abbreviated as `KM`, running as a docker container
typically. KM interfaces with Kubernetes API server and converts objects from
kube-apiserver to Contrail config-api server. when bootup, `KM` validates if
a network CRD `network-attachment-definitions.k8s.cni.cncf.io` is found in the
Kubernetes API server, and creates one if not yet.

here is how a `CRD` object template looks like:

----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: network-attachment-definitions.k8s.cni.cncf.io
spec:
  group: k8s.cni.cncf.io
  version: v1
  scope: Namespaced
  names:
    plural: network-attachment-definitions
    singular: network-attachment-definition
    kind: NetworkAttachmentDefinition
    shortNames:
    - net-attach-def
  validation:
    openAPIV3Schema:
      properties:
        spec:
          properties:
            config:
             type: string
----

in contrail kubernetes setup, the CRD has been created and can be displayed:

----
$ kubectl get crd
NAME                                             CREATED AT
network-attachment-definitions.k8s.cni.cncf.io   2019-06-07T03:43:52Z
----

////
----
$ kubectl get crd -o yaml
apiVersion: v1
items:
- apiVersion: apiextensions.k8s.io/v1beta1
  kind: CustomResourceDefinition
  metadata:
    creationTimestamp: 2019-06-07T03:43:52Z
    generation: 1
    name: network-attachment-definitions.k8s.cni.cncf.io
    resourceVersion: "1170"
    selfLink: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/network-attachment-definitions.k8s.cni.cncf.io
    uid: 77f15393-88d6-11e9-a8b1-0050569e6cfc
  spec:
    additionalPrinterColumns:
    - JSONPath: .metadata.creationTimestamp
      description: |-
        CreationTimestamp is a timestamp representing the server time when this object was created. It is not guaranteed to be set in happens-before order across separate operations. Clients may not set this value. It is represented in RFC3339 form and is in UTC.

        Populated by the system. Read-only. Null for lists. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
      name: Age
      type: date
    group: k8s.cni.cncf.io
    names:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    scope: Namespaced
    version: v1
    versions:
    - name: v1
      served: true
      storage: true
  status:
    acceptedNames:
      kind: NetworkAttachmentDefinition
      listKind: NetworkAttachmentDefinitionList
      plural: network-attachment-definitions
      shortNames:
      - net-attach-def
      singular: network-attachment-definition
    conditions:
    - lastTransitionTime: 2019-06-07T03:43:52Z
      message: no conflicts found
      reason: NoConflicts
      status: "True"
      type: NamesAccepted
    - lastTransitionTime: null
      message: the initial names have been accepted
      reason: InitialNamesAccepted
      status: "True"
      type: Established
    storedVersions:
    - v1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
----
////

with this CRD object present, we have the ability to create a
`NetworkAttachmentDefinition` object as our virtual-network.

=== NetworkAttachmentDefinition object

to create a virtual-network from kubernetes, use a yaml template like this:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: <network-name>
  namespace: <namespace-name>
  annotations:
    "opencontrail.org/cidr" : [<ip-subnet>]
    "opencontrail.org/ip_fabric_snat" : <True/False>
    "opencontrail.org/ip_fabric_forwarding" : <True/False>
spec:
  config: '{
    “cniVersion”: “0.3.0”,
    "type": "contrail-k8s-cni"
}'
----

through `NetworkAttachmentDefinition` object which is created by CRD, we can
define new VNs. like many other standard kubernetes object, basically you
specify the VN name, namespace under `metadata`, and `annotations` which
is used to carry additional information about a network. in contrail
`NetworkAttachmentDefinition` implementation, the `annotations` bring:

* `opencontrail.org/cidr`: gives CIDR, which defines the subnet for a VN
* `opencontrail.org/ip_fabric_forwarding`: a flag to enable/disable `ip fabric
  forwarding` feature
* `opencontrail.org/ip_fabric_snat`: a flag to enable/disable `ip fabric snat`
  feature

****
with the contrail `ip-fabric-forwarding` feature, A VN can be marked for IP
fabric based forwarding without tunneling. When two virtual networks with this
type of configuration communicate with each other, overlay traffic will be
forwarded directly using the underlay. 

With the Contrail `ip-fabric-snat` feature, pods that are in the overlay can
reach the Internet without floating IPs or a logical-router. The
`ip-fabric-snat` feature uses compute node IP for creating a source NAT to reach
the required services and is applicable only to pod networks. 

none of `ip fabric forwarding` or `ip fabric snap` features are in the scope of
this book.
****

alternatively, you can define a new VN by referring an existing VN:

----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: extns-network
  annotations:
    "opencontrail.org/network" : '{"domain":"default-domain", "project": "k8s-extns", "name":"k8s-extns-pod-network"}'
spec:
  config: '{
    “cniVersion”: “0.3.1”,
    "type": "contrail-k8s-cni"
}'
----

throughout this book we'll use the first template to define our VNs in all
examples.

=== multiple-interface pod

with multiple VNs created, we can "attach" (you may also say "plug", or
"insert") any of them into a pod, with a pod yaml file like this:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      { "name": "VN-a" },
      { "name": "VN-b" },
      { "name": "other-ns/VN-c" }
    ]'
spec:
  containers:
----

another valid format:

----
kind: Pod
metadata:
  name: my-pod
  namespace: my-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: 'VN-a,VN-b,other-ns/VN-c'
spec:
  containers:
----

you probably notice, pods in a namespace not only can refer to the networks
defined in local NS, but also can refer networks created on other namespaces
using their fully scoped name. this is very useful - the same network does not
has to be duplicated again and again in every NS that needs it, it can be
defined just one time and then referred anywhere else.

=== lab demo: multi-interface pod

we've understood the basic theories and explored the various templates. now it's
the time to look at a "working example" in the real world. we'll start from
creating two VNs, examining the VN objects, then create a pod and attach the 2
VNs into it. we'll conclude the test and this section by examining the pod
interfaces and connectivity with other pods sharing the same VNs.

////
now you may want to test these theories in your setup
starting from creating your own yaml files based on these templates. if this is
the first time you work on this, you will most likely run into all kinds of
small issues here and there.
////

here is a yaml file of two VNs: `vn-left-1` and `vn-right-1`

----
$ cat vn-left-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "10.10.10.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-left-1
spec:
  config: '{ 
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

----
$ cat vn-right-1.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    "opencontrail.org/cidr": "20.20.20.0/24"
    "opencontrail.org/ip_fabric_forwarding": "false"
    "opencontrail.org/ip_fabric_snat": "false"
  name: vn-right-1
  #namespace: default
spec:
  config: '{
    "cniVersion": "0.3.0", 
    "type": "contrail-k8s-cni" 
  }'
----

create both VNs:

----
$ kubectl apply -f vn-left-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-left-1 created

$ kubectl apply -f vn-right-1.yaml
networkattachmentdefinition.k8s.cni.cncf.io/vn-right-1 created
----

examine the VNs:

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io
NAME            AGE
vn-left-1       3s
vn-right-1      10s
----

----
$ kubectl get network-attachment-definitions.k8s.cni.cncf.io vn-left-1 -o yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"k8s.cni.cncf.io/v1","kind":"NetworkAttachmentDefinition","metadata":{"annotations":{"opencontrail.org/cidr":"10.10.10.0/24","opencontrail.org/ip_fabric_forwarding":"false"},"name":"vn-left-1","namespace":"ns-user-1"},"spec":{"config":"{ \"cniVersion\": \"0.3.0\", \"type\": \"contrail-k8s-cni\" }"}}
    opencontrail.org/cidr: 10.10.10.0/24
    opencontrail.org/ip_fabric_forwarding: "false"
  creationTimestamp: 2019-06-13T14:17:42Z
  generation: 1
  name: vn-left-1
  namespace: ns-user-1
  resourceVersion: "777874"
  selfLink: /apis/k8s.cni.cncf.io/v1/namespaces/ns-user-1/network-attachment-definitions/vn-left-1
  uid: 01f167ad-8de6-11e9-bbbf-0050569e6cfc
spec:
  config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

the VNs are created, as expected. it seems nothing much exciting here. However,
if you login to the contrail UI, you will see something "unexpected". 


//old GUI:
//image::https://user-images.githubusercontent.com/2038044/59985880-f5886080-9601-11e9-98c9-791fec2fbe55.png[]

.contrail command: "main-menu" -> "virtual networks"

image::https://user-images.githubusercontent.com/2038044/60283772-f78b4180-98d7-11e9-9358-1ed47aeeef57.png[]

****
.CC
contrail command (CC) is the new user interface (UI) coming with the
contrail 5.0.1. throughout this book we use both CC and old UI to demonstrate
most of lab studies. just keep in mind that in the future CC will be the only UI
and the "legacy" one will be deprecated. 

.CC "main-menu":
in CC, the functions and settings are groups in a a "main menu". it is also the
entry point from where you can navigate through different functions. 

.CC "main-menu":
image::https://user-images.githubusercontent.com/2038044/60282872-ed684380-98d5-11e9-92f7-e1df07c5fecf.png[]

in order to get this menu, click on group name right next to the "contrail
command" logo on the upper left of the UI. in the above screen capture that
group is "Infrastructure", but regardless it can be any group, just click it and
you will get the main menu, then from there you can select and jump into all
other settings.
****

make sure you select a correct "project", in this case it is `k8s-default`.

you won't be able to find any VN with the exact name `vn-left-1` or `vn-right-1`
in the command UI. instead, what you will find are two VNs named
`k8s-vn-left-1-pod-network` and `k8s-vn-right-1-pod-network` got created. 

there is nothing wrong here. What happened is whenever a VN get created from
kubernetes, contrail automatically adds a prefix `k8s-` to the VN name that you
give in the network yaml file, and a suffix `-pod-network` in the end. This
makes sense because we know a VN can be created by different methods. with these
extra keywords embeded in the name, it is easier to tell how the VN was created
(from kubernetes or from the GUI manually), what will it be used for, and also
potential VN name confliction is avoided.

here is yaml file of the `cirros` pod - a mirror container version of the
classis cirros VM.

----
apiVersion: v1
kind: Pod
metadata:
  name: cirros
  labels:
    app: cirros
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left-1" },
       { "name": "vn-right-1" }
   ]'
spec:
  containers:
  - name: cirros
    image: cirros
    imagePullPolicy: Always
  restartPolicy: Always
----

in pod annotations under metadata, we insert 2 VNs: `vn-left-1` and
`vn-right-1`. Now guess how many interfaces will the pod has on bootup?  you may
think it will be two because that is what we gave in the file. let's create the
pod and verify:

----
$ kubectl get pod -o wide
NAME    READY  STATUS   RESTARTS  AGE  IP             NODE     NOMINATED  NODE
cirros  1/1    Running  0         20s  10.47.255.238  cent222  <none>

$ kubectl describe pod cirros
Name:               cirros
Namespace:          ns-user-1
Priority:           0
PriorityClassName:  <none>
Node:               cent222/10.85.188.20
Start Time:         Wed, 26 Jun 2019 12:51:30 -0400
Labels:             app=cirros
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.250",
                              "mac": "02:87:cf:6c:9a:98",
                              "name": "vn-left-1"
                          },
                          {
                              "ips": "10.47.255.238",
                              "mac": "02:87:98:cc:4e:98",
                              "name": "cluster-wide-default"
                          },
                          {
                              "ips": "20.20.20.1",
                              "mac": "02:87:f9:f9:88:98",
                              "name": "vn-right-1"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ 
                        { "name": "vn-left-1" }, { "name": "vn-right-1" } ]
                    kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":
                      {"annotations":{"k8s.v1.cni.cncf.io/networks":"[
                      { \"name\": \"vn-left-1\" }, { \"name\": \"vn-...
Status:             Running
IP:                 10.47.255.238
...<snipped>...
----

in `Annotations`, under `k8s.v1.cni.cncf.io/network-status` we see a list
`[...]`, which has 3 items each represented by a block `{}` of key-value
mappings. each block includes the interface allocated IP, MAC and the VN it
belongs to. so you will end up to have 3 interfaces created in the pod instead
of 2.  please notice the 2nd item which gives IP address `10.47.255.238`, that
is the interface attached to the "default pod network" named
"cluster-wide-default", which is created by the sytem. you can treat the default
pod network as a "managment" network because it is always "up and running" in
every pod's network namespace, but funtionally it is no much different with the
VN you create - except that you can't delete it.

we can "login to" the pod, list the interfaces and verify the IP and MAC.

----
$ kubectl exec -it cirros sh
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
37: eth0@if38: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:47:06:d8:98 brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.238/12 scope global eth0
       valid_lft forever preferred_lft forever
39: eth1@if40: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:6b:a0:e2:98 brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.250/24 scope global eth1
       valid_lft forever preferred_lft forever
41: eth2@if42: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:53:8e:8a:80:98 brd ff:ff:ff:ff:ff:ff
    inet 20.20.20.1/24 scope global eth2
       valid_lft forever preferred_lft forever
----

we see 1 lo interface and 3 interfaces plugged by contrail CNI, each with the IP
allocated from the corresponding VN. also you will notice the MAC addresses
match what we've seen in `kubectl describe` command output. 

you will see multiple-interfaces pod again in sevice-chaining example later on.
in that example the pod will be based on Juniper CSRX image instead of a general
docker image. but the basic idea remains the same.

NOTE: having the MAC address in the annotations may be very important under
certain cases - imaging you login a pod and for some reason you lose the track
of interface to VN mapping (e.g., you manually changed/removed the IPs, or the
pod's application reset the IP, etc) you can count on the MAC address! later In
"service chaining" section you will run into a scenario when you need to use the
MAC address to locate the proper interface, before you can even tell which
interface should be configured with which podIP that kubernetes allocated from a
VN. check "service chaining" section for more details.


== service chaining with CSRX

service chaining is the idea of forwarding traffic through multiple network
entity in a certain order, each network entity do specific function such as
firewall, IPS , NAT , LB , …,etc the legacy way of doing service chaining would
use standalone HW appliances which made service chaining inflexible, expensive
and takes a long time to setup Dynamic service chaining is where network
functions deployed as VM or Container and could be chained automatically in a
logical way.  in the next example we use contrail for services chaining between
two PODs in two different networking using CSRX container L4-L7 firewall to
secure the traffic between these two networks as shown in the figure 

.service chaining
image::https://user-images.githubusercontent.com/2038044/60268925-85a4ff00-98bb-11e9-94c3-219d41038642.png[]

[NOTE]
====
- left and right networks are just a common name used for simplicity and
  expected the traffic to follow from left to right but you can use your own
  names 
- make sure to configure the network before you attached a POD to it otherwise
  POD would fail to be created 
====

so let’s start create two networks using this YAML files 

----
[root@cent11]# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.10.10.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-left
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'

[root@cent11]# cat vn-left.yaml
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    opencontrail.org/cidr: “10.20.20.0/24"
    opencontrail.org/ip_fabric_forwarding: "false"
    opencontrail.org/ip_fabric_snat: "false"
  name: vn-right
  namespace: default
spec:
 config: '{ "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }'
----

----
[root@cent11]# kubectl create -f vn-left.yaml
[root@cent11]# kubectl create -f vn-right.yaml
----

Verify using Kubectl 
 
----
[root@cent11 ~]# kubectl get network-attachment-definition
NAME       AGE
vn-left    19d
vn-right   17d

[root@cent11 ~]# kubectl describe network-attachment-definition
Name:         vn-left
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.10.10.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-25T20:28:22Z
  Generation:          1
  Resource Version:    83111
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-left
  UID:                 a44fe276-7f2b-11e9-9ff0-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>


Name:         vn-right
Namespace:    default
Labels:       <none>
Annotations:  opencontrail.org/cidr: 10.20.20.0/24
              opencontrail.org/ip_fabric_forwarding: false
              opencontrail.org/ip_fabric_snat: false
API Version:  k8s.cni.cncf.io/v1
Kind:         NetworkAttachmentDefinition
Metadata:
  Creation Timestamp:  2019-05-28T07:14:02Z
  Generation:          1
  Resource Version:    380427
  Self Link:           /apis/k8s.cni.cncf.io/v1/namespaces/default/network-attachment-definitions/vn-right
  UID:                 2b8d394f-8118-11e9-b36d-0050569e2171
Spec:
  Config:  { "cniVersion": "0.3.0", "type": "contrail-k8s-cni" }
Events:    <none>
----

It’s a good practice to confirm these two networks are seen now in contrail
before proceeding. 
From the Contrail Controller module control node (http://10.85.188.16:8143 in
our setup), select Configure > Networking > Networks > default-domain > k8s-default
As shown in the figure which focus on left network

NOTE: using namespace: default object in the YAML file for a network will create
it n in domain “default-domain” and project “K8s-default”

image::https://user-images.githubusercontent.com/2038044/60268927-863d9580-98bb-11e9-965a-b50f91d811d1.png[]

Create two ubuntu Pods, one in each network using the annotation object

----
[root@cent11 ~]# cat left-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: left-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-left" }]'
spec:
  containers:
    - name: ubuntu-left-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN


[root@cent11 ~]# cat right-ubuntu-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: right-ubuntu-sc
  labels:
    app: webapp-sc
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
        { "name": "vn-right" }]'
spec:
  containers:
    - name: ubuntu-right-pod-sc
      image: virtualhops/ato-ubuntu:latest
      securityContext:
          privileged: true
          capabilities:
           add:
             - NET_ADMIN

[root@cent11 ~]# kubectl create -f right-ubuntu-sc.yaml
[root@cent11 ~]# kubectl create -f left-ubuntu-sc.yaml


[root@cent11 ~]# kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
left-ubuntu-sc    1/1     Running   0          25h
right-ubuntu-sc   1/1     Running   0          25h

----

create Juniper CSRX container that have one interface on the left network and
one interface on the right network using this YAML file 

----
[root@cent11 ~]# cat csrx1-sc.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: csrx1-sc
  labels:
    app: webapp-sc
  annotations:
   k8s.v1.cni.cncf.io/networks: '[
       { "name": "vn-left" },
       { "name": "vn-right" }
   ]'
spec:
  containers:
  - name: csrx1-sc
    image: csrx
    ports:
    - containerPort: 22
    imagePullPolicy: Never
    stdin: true
    tty: true
    securityContext:
      privileged: true

[root@cent11 ~]# kubectl create -f csrx1-sc.yaml


Confirm the interface placement in the correct network 

[root@cent11 ~]# kubectl describe pod 
Name:               csrx1-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:31 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.2",
                              "mac": "02:84:71:f4:f2:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.20.20.2",
                              "mac": "02:84:8b:4c:18:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.248",
                              "mac": "02:84:59:7e:54:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }, { "name": "vn-right" } ]
Status:             Running
IP:                 10.47.255.248
Containers:
  csrx1-sc:
    Container ID:   docker://82b7605172d937895269d76850d083b6dc6e278e41cb45b4cb8cee21283e4f17
    Image:          csrx
    Image ID:       docker://sha256:329e805012bdf081f4a15322f994e5e3116b31c90f108a19123cf52710c7617e

...<snipped>...

Name:               left-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 03:40:20 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.10.10.1",
                              "mac": "02:7d:b1:09:00:8d",
                              "name": "vn-left"
                          },
                          {
                              "ips": "10.47.255.249",
                              "mac": "02:7d:99:ff:62:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-left" }]
Status:             Running
IP:                 10.47.255.249
Containers:
  ubuntu-left-pod-sc:
    Container ID:   docker://2f9a22568d844c68a1c4a45de4a81478958233052e08d4473742827482b244cd
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
   
...<snipped>...

Name:               right-ubuntu-sc
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               cent22/10.85.188.17
Start Time:         Thu, 13 Jun 2019 04:09:18 -0400
Labels:             app=webapp-sc
Annotations:        k8s.v1.cni.cncf.io/network-status:
                      [
                          {
                              "ips": "10.20.20.1",
                              "mac": "02:89:cc:86:48:8d",
                              "name": "vn-right"
                          },
                          {
                              "ips": "10.47.255.252",
                              "mac": "02:89:b0:8e:98:8d",
                              "name": "cluster-wide-default"
                          }
                      ]
                    k8s.v1.cni.cncf.io/networks: [ { "name": "vn-right" }]
Status:             Running
IP:                 10.47.255.252
Containers:
  ubuntu-right-pod-sc:
    Container ID:   docker://4e0b6fa085905be984517a11c3774517d01f481fa43aadd76a633ef15c58cbfe
    Image:          virtualhops/ato-ubuntu:latest
    Image ID:       docker-pullable://virtualhops/ato-ubuntu@sha256:fa2930cb8f4b766e5b335dfa42de510ecd30af6433ceada14cdaae8de9065d2a
  
 ...<snipped>...
----

NOTE: each container has one interface belong to “cluster-wide-default” network
regardless the use of the annotations object because annotations object above
creates and put one extra interface in a specific network 


Login to the left, right Pods and the CSRX to confirm the IP/MAC address 
 
----
[root@cent11 ~]# kubectl exec -it left-ubuntu-sc bash
root@left-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:99:ff:62:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.249/12 scope global eth0
       valid_lft forever preferred_lft forever
15: eth1@if16: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:7d:b1:09:00:8d brd ff:ff:ff:ff:ff:ff
    inet 10.10.10.1/24 scope global eth1
       valid_lft forever preferred_lft forever



[root@cent11 ~]# kubectl exec -it right-ubuntu-sc bash
root@right-ubuntu-sc:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
23: eth0@if24: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:b0:8e:98:8d brd ff:ff:ff:ff:ff:ff
    inet 10.47.255.252/12 scope global eth0
       valid_lft forever preferred_lft forever
25: eth1@if26: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:89:cc:86:48:8d brd ff:ff:ff:ff:ff:ff
    inet 10.20.20.1/24 scope global eth1
       valid_lft forever preferred_lft forever


[root@cent11 ~]# kubectl exec -it csrx1-sc cli
root@csrx1-sc>
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d
----

NOTE: unlike other PODs the CSRX didn’t acquire IP with DHCP and it start with
factory default configuration hence it need to be configured. 

NOTE: By default, CSRX eth0 is visible only from shell and used for management.
And when attaching networks, the first attach network is mapped to eth1 which is
GE-0/0/1 And the second attach is mapped to eth2 which is GE-0/0/0

Configure this basic setup on the CSRX, to assign the correct IP address use the
MAC/IP address mapping from the “ kubectl describe pod” command show output as
well configure default security policy to allow everything for now 

----
set interfaces ge-0/0/1 unit 0 family inet address 10.10.10.2/24
set interfaces ge-0/0/0 unit 0 family inet address 10.20.20.2/24

set security zones security-zone trust interfaces ge-0/0/0
set security zones security-zone untrust interfaces ge-0/0/1 
set security policies default-policy permit-all 
commit
----

verify the IP address assigned on the CSRX

----
root@csrx1-sc> show interfaces 
Physical interface: ge-0/0/1, Enabled, Physical link is Up
  Interface index: 100
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:71:f4:f2:8d, Hardware address: 02:84:71:f4:f2:8d

  Logical interface ge-0/0/1.0 (Index 100)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.10.10.0/24, Local: 10.10.10.2

Physical interface: ge-0/0/0, Enabled, Physical link is Up
  Interface index: 200
  Link-level type: Ethernet, MTU: 1514
  Current address: 02:84:8b:4c:18:8d, Hardware address: 02:84:8b:4c:18:8d

  Logical interface ge-0/0/0.0 (Index 200)
    Flags: Encapsulation: ENET2
    Protocol inet
        Destination: 10.20.20.0/24, Local: 10.20.20.2
----

From the Left POD try to ping the left POD, ping would fail as there is no route 

----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 1999ms

root@left-ubuntu-sc:/# ip r
default via 10.47.255.254 dev eth0 
10.10.10.0/24 dev eth1  proto kernel  scope link  src 10.10.10.1 
10.32.0.0/12 dev eth0  proto kernel  scope link  src 10.47.255.249
----

Adding static route to the left and right PODs and try to ping again 

----
root@left-ubuntu-sc:/# ip r add 10.20.20.0/24 via 10.10.10.2

root@right-ubuntu-sc:/# ip r add 10.10.10.0/24 via 10.20.20.2

root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 2999ms
----


Still ping failed, as we didn’t create the service chaining which will also take care of the routing
let’s see what happen to our packets 

----
root@csrx1-sc# run show security flow session 
Total sessions: 0

No session on the CSRX.
Login to the compute node “cent22” that host this container to dump the traffic using tshark and check the routing 
To get the interface linking the containers 
[root@cent22 ~]# vif -l
Vrouter Interface Table

Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
       Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
       D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
       Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
       Uuf=Unknown Unicast Flood, Vof=VLAN insert/strip offload, Df=Drop New Flows, L=MAC Learning Enabled
       Proxy=MAC Requests Proxied Always, Er=Etree Root, Mn=Mirror without Vlan Tag, Ig=Igmp Trap Enabled

...<snipped>...

vif0/3      OS: tapeth0-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.252
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10760  bytes:452800 errors:0
            TX packets:14239  bytes:598366 errors:0
            Drops:10744

vif0/4      OS: tapeth1-89a4e2
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.1
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13002  bytes:867603 errors:0
            TX packets:16435  bytes:1046981 errors:0
            Drops:10805

vif0/5      OS: tapeth0-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.249
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:10933  bytes:459186 errors:0
            TX packets:14536  bytes:610512 errors:0
            Drops:10933

vif0/6      OS: tapeth1-7d8e06
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.1
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:12625  bytes:1102433 errors:0
            TX packets:15651  bytes:810689 errors:0
            Drops:10957

vif0/7      OS: tapeth0-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.47.255.248
            Vrf:3 Mcast Vrf:3 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:20996  bytes:1230688 errors:0
            TX packets:27205  bytes:1142610 errors:0
            Drops:21226

vif0/8      OS: tapeth1-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.10.10.2
            Vrf:6 Mcast Vrf:6 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:13908  bytes:742243 errors:0
            TX packets:29023  bytes:1790589 errors:0
            Drops:10514

vif0/9      OS: tapeth2-844f1c
            Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:10.20.20.2
            Vrf:5 Mcast Vrf:5 Flags:PL3DEr QOS:-1 Ref:6
            RX packets:16590  bytes:1053659 errors:0
            TX packets:31321  bytes:1635153 errors:0
            Drops:10421

...<snipped>...

----

Note that Vif0/3 and Vif0/4 are bounded with the right POD and both linked to
tapeth0-89a4e2 and tapeth1-89a4e2 respectively same goes for the left POD for
Vif0/5 and vif0/6 while vif0/7, vif 0/8 and vif0/9 are bound with CSRX1.| from
that you can also see the number of packets/bytes hits that interface as well
the VRF which is this interface belong in here VRF 3 is for the
default-cluster-network while VRF 6 for the left network and VRF 5 for the right
network in this figure you can see the interface mapping from the all
prospective (container, Linux , vr-agent) 

image::https://user-images.githubusercontent.com/2038044/60268930-863d9580-98bb-11e9-9dc3-b0c5598ff528.png[]

try to ping again from the left POD to the right POD and use tshark on the tap
interface for the right POD for further inspection 


----
[root@cent22 ~]# tshark -i tapeth1-89a4e2
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-89a4e2'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.254 (Request)
  2 0.000037656 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Gratuitous ARP for 10.20.20.253 (Request)
  3 1.379993896 IETF-VRRP-VRID_00 -> 02:89:cc:86:48:8d ARP 42 Who has 10.20.20.1?  Tell 10.20.20.253
----


Looks like the ping isn’t reaching the right POD at all , lets see on the CSRX
left network tap interface  

----
[root@cent22 ~]# tshark -i tapeth1-844f1c
Running as user "root" and group "root". This could be dangerous.
Capturing on 'tapeth1-844f1c'
  1 0.000000000 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Who has 0.255.255.252?  Tell 0.0.0.0
  2 0.201392098   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=410/39425, ttl=63
  3 0.201549430   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  4 1.201444156   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=411/39681, ttl=63
  5 1.201600074   10.10.10.2 -> 10.10.10.1   ICMP 70 Destination unreachable (Port unreachable)
  6 1.394074095 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.254 (Request)
  7 1.394108344 IETF-VRRP-VRID_00 -> 02:84:71:f4:f2:8d ARP 42 Gratuitous ARP for 10.10.10.253 (Request)
  8 2.201462515   10.10.10.1 -> 10.20.20.1   ICMP 98 Echo (ping) request  id=0x020a, seq=412/39937, ttl=63
----

We can see the packet but there is nothing in the CSRX security prospective to drop this packet

checking the routing table of the left network VRF by logging to the vrouter_vrouter-agent_1 in the compute node 

----
[root@cent22 ~]# docker ps | grep vrouter
9a737df53abe        ci-repo.englab.juniper.net:5000/contrail-vrouter-agent:master-latest   "/entrypoint.sh /usr…"   2 weeks ago         Up 47 hours                             vrouter_vrouter-agent_1
e25f1467403d        ci-repo.englab.juniper.net:5000/contrail-nodemgr:master-latest         "/entrypoint.sh /bin…"   2 weeks ago         Up 47 hours                             vrouter_nodemgr_1

[root@cent22 ~]# docker exec -it vrouter_vrouter-agent_1 bash
(vrouter-agent)[root@cent22 /]$ 
(vrouter-agent)[root@cent22 /]$ rt --dump 6 | grep 10.20.20.
(vrouter-agent)[root@cent22 /]$
----


Note that 6 is the routing table VRF of the left network, same would goes for
the right network VRF routing table there is missing route 

    (vrouter-agent)[root@cent22 /]$ rt --dump 5 | grep 10.10.10.
    (vrouter-agent)[root@cent22 /]$

So even if all the PODs are hosted on the same compute nodes, they can’t reach
each other. And if these PODs are hosted on different compute nodes then you
have a bigger problem to solve Service chaining isn’t about adjusting the routes
on the containers but mainly about exchange routes between the vrouter-agent
between the compute nodes regardless of the location of the POD as well adjust
that automatically if the POD moved to another compute node Before we build
service chaining lets address an important concerns for network administrator
who are not fan of this kind of CLI troubleshooting, can we do the same
troubleshooting using contrail controller GUI the answer is yes and lets do it 


From the Contrail Controller module control node (http://10.85.188.16:8143 in
oursetup), select monitor > infrastructure > virtual router then select the node
the that host the POD , in our case “Cent22.local” 

image::https://user-images.githubusercontent.com/2038044/60268931-863d9580-98bb-11e9-9682-d330878fa386.png[]

as shown in the figure from the interface tab which is equivalent to running “
vif -l” command on the vrouter_vrouter-agent-1 container and even showing more
information notice the mapping between the instance ID and tap interface naming
where the first 6 character of the instance ID are always reflected in the tap
interface naming

to check the routing tables of each VRF move to the “routes” tab and select the
VRF you want to see

image::https://user-images.githubusercontent.com/2038044/60268935-86d62c00-98bb-11e9-8eaa-820578b11127.png[]

If we select the left network ( the name is longer as it include the domain ,
project ) we can confirm there is not 10.20.20.0/24 prefix from the right
network We can also check the mac address learned in the left network by
selecting L2 ( which is equvilant to “rt --dump 6 --family bridge” command 

image::https://user-images.githubusercontent.com/2038044/60268936-86d62c00-98bb-11e9-9050-ca104b278a1a.png[]

Now lets utilize the CSRX to service chaining using contrail command GUI

creating Service chaining is 4 steps make sure to do them in this order 

1. create Service template 
2. creating service instance based on the service template you created before
3. creating network policy and select the service instance you created before
4. apply this network policy on network   


NOTE: since contrail command GUI is the solution to provide a single point of
management for all environments, we will use it to build service changing but
you still can use the normal contrail controller GUI to build service changing
 

Login to contrail command GUI ( in our setup https://10.85.188.16:9091/) then select service > catalog > create 

image::https://user-images.githubusercontent.com/2038044/60268937-86d62c00-98bb-11e9-8744-b8213b5246ed.png[]
image::https://user-images.githubusercontent.com/2038044/60268938-876ec280-98bb-11e9-991b-a54dedadfbcd.png[]
 
insret a name of services template “myweb-CSRX-CS” in here then chose v2 ,
virtual machine ( no other option available) for service mode we will work with
In-network and firewall as service type  
 

image::https://user-images.githubusercontent.com/2038044/60268941-876ec280-98bb-11e9-8f68-5c49af9b06d1.png[]

Select interfaces management, left and right then click create
 
image::https://user-images.githubusercontent.com/2038044/60268942-876ec280-98bb-11e9-8c7c-ac2a95da9ab0.png[]

Now select deployment and click create to create the service instances

image::https://user-images.githubusercontent.com/2038044/60268943-876ec280-98bb-11e9-9cf8-12b240de0286.png[]

Insert a name for this service instance then select from the drop down menu the
name of the template you created before then chose the proper network from the
prospective of the CSRX being the instance (container in that case) that will do
the service chaining and click on port tuples to expand it 

image::https://user-images.githubusercontent.com/2038044/60268945-88075900-98bb-11e9-87fa-375337170b12.png[]




 
then for each of the three interface bound one interface of the CSRX then click create

NOTE: the name of the virtual machine interface isn’t shown in the drop down
menu instead the instance ID, you can identify that from the tap interface name
as we showed before.  In other word all you have to know is most 6 left
character for any interface belong to that container as all the interface in a
given instance ( VM or container)  share the same first characters from the left 

 
Before you procced make sure the status of the three interfaces are up and they are showing the correct IP address of the CSRX instance 
 
image::https://user-images.githubusercontent.com/2038044/60268947-88075900-98bb-11e9-9b0a-ecf0c6a03e33.png[]

To create network policy go to overlay > network policies > create 

image::https://user-images.githubusercontent.com/2038044/60268948-88075900-98bb-11e9-88ba-f5f7a02161b0.png[]
 
Insert a name for your network policy then in the first rule add left network as source network and right network as destination with action pass 

image::https://user-images.githubusercontent.com/2038044/60268949-889fef80-98bb-11e9-844a-326b5d506038.png[]

Select advanced option to attached the service instance you create before and click create 
 


image::https://user-images.githubusercontent.com/2038044/60268951-889fef80-98bb-11e9-84c5-f354b3d8938e.png[]




To attach this network policy to network click virtual network and select the left network and edit 
 

image::https://user-images.githubusercontent.com/2038044/60268953-889fef80-98bb-11e9-8826-2626a76c3d4a.png[]

In network policies select the network policy you just created from the drop down menu then click save 
do the same for the right network

image::https://user-images.githubusercontent.com/2038044/60268955-89388600-98bb-11e9-9605-14fbc8d30fbe.png[]



Now lets check the effect of this service changing on routing 
From the Contrail Controller module control node (http://10.85.188.16:8143 in
oursetup), select monitor > infrastructure > virtual router then select the node
the that host the POD , in our case “Cent22.local” then select the “routes” tab
and select the left VRF 
 
image::https://user-images.githubusercontent.com/2038044/60268956-89388600-98bb-11e9-9e82-7d5fbddf38f8.png[]

Now we can the right networks host routes has been leaked to the left network (10.20.20.1/32 , 10.20.20.2/32 in this case) 

Now let’s try to ping the right pod from the left pod to see the session created on the CSRX 


----
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
64 bytes from 10.20.20.1: icmp_seq=1 ttl=61 time=0.863 ms
64 bytes from 10.20.20.1: icmp_seq=2 ttl=61 time=0.290 ms
^C
--- 10.20.20.1 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1000ms
rtt min/avg/max/mdev = 0.290/0.576/0.863/0.287 ms

root@csrx1-sc# run show security flow session 
Session ID: 5378, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/2 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/2;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 

Session ID: 5379, Policy name: default-policy-logical-system-00/2, Timeout: 2, Valid
  In: 10.10.10.1/3 --> 10.20.20.1/534;icmp, Conn Tag: 0x0, If: ge-0/0/1.0, Pkts: 1, Bytes: 84, 
  Out: 10.20.20.1/534 --> 10.10.10.1/3;icmp, Conn Tag: 0x0, If: ge-0/0/0.0, Pkts: 1, Bytes: 84, 
Total sessions: 2

----

Now let try to create security policy on the CSRX to allow only http and https

----
root@csrx1-sc# show security 
policies {
    traceoptions {
        file ayma;
        flag all;
    }
    from-zone trust to-zone untrust {
        policy only-http-s {
            match {
                source-address any;
                destination-address any;
                application [ junos-http junos-https ];
            }
            then {
                permit;
                log {
                    session-init;
                    session-close;
                }
            }
        }
        policy deny-ping {
            match {
                source-address any;
                destination-address any;
                application any;        
            }                           
            then {                      
                reject;                 
                log {                   
                    session-init;       
                    session-close;      
                }                       
            }                           
        }                               
    }                                   
    default-policy {                    
        deny-all;                       
    }                                   
}                                       
zones {                                 
    security-zone trust {               
        interfaces {                    
            ge-0/0/0.0;                 
        }                               
    }                                   
    security-zone untrust {             
        interfaces {                    
            ge-0/0/1.0;                 
        }                               
    }                                   
}
root@left-ubuntu-sc:/# ping 10.20.20.1
PING 10.20.20.1 (10.20.20.1) 56(84) bytes of data.
^C
--- 10.20.20.1 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2000ms
----

the ping failed as the policy on the CSRX drop it 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:04:01 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/8->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5394 N/A N/A -1
Jun 14 23:04:02 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_DENY: session denied 10.10.10.1/9->10.20.20.1/575 0x0 icmp 1(8) deny-ping trust untrust UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 No policy reject 5395 N/A N/A -1
Try to send http traffic from the left to the right POD and verify the session status on the CSRX
root@left-ubuntu-sc:/# wget 10.20.20.1
--2019-06-14 23:07:34--  http://10.20.20.1/
Connecting to 10.20.20.1:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 11510 (11K) [text/html]
Saving to: 'index.html.4'

100%[======================================>] 11,510      --.-K/s   in 0s      

2019-06-14 23:07:34 (278 MB/s) - 'index.html.4' saved [11510/11510]
----

And in the CSRX we can see the session creation 

----
root@csrx1-sc> show log syslog | last 20 
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Adding resolved unicast NH. dest: 10.20.20.1, proto v4 (peer initiated)
Jun 14 23:07:31 csrx1-sc flowd-0x2[374]: csrx_l3_add_new_resolved_unicast_nexthop: Sending resolve request for stale ARP entry (b). NH: 5507 dest: 10.20.20.1
Jun 14 23:07:34 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CREATE: session created 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 N/A(N/A) ge-0/0/1.0 UNKNOWN UNKNOWN UNKNOWN N/A N/A -1
Jun 14 23:07:35 csrx1-sc flowd-0x2[374]: RT_FLOW: RT_FLOW_SESSION_CLOSE: session closed TCP FIN: 10.10.10.1/47190->10.20.20.1/80 0x0 junos-http 10.10.10.1/47190->10.20.20.1/80 0x0 N/A N/A N/A N/A 6 only-http-s trust untrust 5434 14(940) 12(12452) 2 UNKNOWN UNKNOWN N/A(N/A) ge-0/0/1.0 UNKNOWN N/A N/A -1
----

